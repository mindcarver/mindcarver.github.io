# å­¦ä¹ æ£€æŸ¥æ¸…å•

## ğŸ“‹ å­¦ä¹ ç›®æ ‡æ£€æŸ¥

### âœ… Module 3.1: Gradient Boosting åŸç†

- [ ] ç†è§£ Boosting çš„è¿­ä»£çº é”™æœºåˆ¶
  - èƒ½è§£é‡Š Treeâ‚ â†’ Treeâ‚‚ â†’ Treeâ‚ƒ çš„è¿­ä»£è¿‡ç¨‹
  - ç†è§£æ®‹å·®å­¦ä¹  râ‚ = y - Å·â‚ çš„å«ä¹‰
  - çŸ¥é“æœ€ç»ˆé¢„æµ‹ = Å·â‚ + Å·â‚‚ + Å·â‚ƒ + ...

- [ ] çŸ¥é“ Bagging vs Boosting çš„åŒºåˆ«
  - å¹¶è¡Œ vs ä¸²è¡Œ
  - é™ä½æ–¹å·® vs é™ä½åå·®
  - ä¸å®¹æ˜“è¿‡æ‹Ÿåˆ vs éœ€è¦æ—©åœæ§åˆ¶

- [ ] äº†è§£ LightGBM çš„ä¸‰å¤§åˆ›æ–°
  - **GOSS (æ¢¯åº¦å•è¾¹é‡‡æ ·)**ï¼šä¿ç•™å¤§æ¢¯åº¦ï¼Œé‡‡æ ·å°æ¢¯åº¦
  - **EFB (äº’æ–¥ç‰¹å¾æ†ç»‘)**ï¼šåˆå¹¶ç¨€ç–ç‰¹å¾
  - **Leaf-wise ç”Ÿé•¿**ï¼šæ¯æ¬¡åˆ†è£‚å¢ç›Šæœ€å¤§çš„å¶å­

- [ ] ç†è§£ä¸ºä»€ä¹ˆé‡åŒ–åçˆ± Boosting
  - ä¿¡å·å¾ˆå¼±ï¼ˆIC åªæœ‰ 0.03~0.08ï¼‰
  - ç‰¹å¾ç¨€ç–ï¼ˆä¸æ˜¯æ‰€æœ‰ç‰¹å¾éƒ½æœ‰ç”¨ï¼‰
  - éœ€è¦å¿«é€Ÿè¿­ä»£ï¼ˆæ¯å¤©éƒ½æœ‰æ–°æ•°æ®ï¼‰

### âœ… Module 3.2: æ—¶åºæ•°æ®åˆ’åˆ†

- [ ] çŸ¥é“ä¸ºä»€ä¹ˆä¸èƒ½éšæœºåˆ’åˆ†æ—¶åºæ•°æ®
  - ç†è§£æ•°æ®æ³„éœ²çš„æ¦‚å¿µ
  - èƒ½ä¸¾ä¾‹è¯´æ˜éšæœºåˆ’åˆ†çš„é—®é¢˜
  - çŸ¥é“å› æœæ€§çº¦æŸçš„é‡è¦æ€§

- [ ] èƒ½æ­£ç¡®å®ç°æ—¶åºåˆ’åˆ†
  ```python
  dates = X.index.get_level_values(0)
  unique_dates = dates.unique().sort_values()
  train_mask = dates <= unique_dates[int(n_dates * 0.7)]
  valid_mask = (dates > unique_dates[int(n_dates * 0.7)]) & (dates <= unique_dates[int(n_dates * 0.85)])
  test_mask = dates > unique_dates[int(n_dates * 0.85)]
  ```

- [ ] äº†è§£ Purging å’Œ Embargo çš„ä½œç”¨
  - **Purging**: åˆ é™¤è®­ç»ƒé›†æœ«å°¾ N å¤©
  - **Embargo**: éªŒè¯é›†å¼€å¤´é¢å¤–ç©ºå‡ºå‡ å¤©
  - èƒ½å®ç°å®Œæ•´çš„ Purging + Embargo

- [ ] èƒ½å®ç° Walk-Forward éªŒè¯
  - ç†è§£æ»šåŠ¨çª—å£éªŒè¯çš„åŸç†
  - èƒ½å®ç° Walk-Forward éªŒè¯ä»£ç 
  - èƒ½åˆ†æå¤šä¸ªçª—å£çš„ IC å˜åŒ–

### âœ… Module 3.3: LightGBM è®­ç»ƒ

- [ ] äº†è§£å…³é”®å‚æ•°çš„å«ä¹‰

  | å‚æ•° | æ¨èå€¼ | ä½œç”¨ |
  |------|--------|------|
  | learning_rate | 0.01~0.1 | æ§åˆ¶æ”¶æ•›é€Ÿåº¦ |
  | num_leaves | 31~127 | æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ |
  | max_depth | 6~10 | é˜²æ­¢è¿‡æ‹Ÿåˆ |
  | feature_fraction | 0.8 | ç‰¹å¾é‡‡æ · |
  | bagging_fraction | 0.8 | æ ·æœ¬é‡‡æ · |
  | lambda_l1 | 0.1 | L1 æ­£åˆ™ |
  | lambda_l2 | 0.1 | L2 æ­£åˆ™ |

- [ ] èƒ½æ­£ç¡®åˆ›å»º Dataset
  ```python
  train_data = lgb.Dataset(X_train.values, label=y_train.values,
                           feature_name=list(FEATURES.keys()),
                           categorical_feature=[])
  valid_data = lgb.Dataset(X_valid.values, label=y_valid.values,
                           reference=train_data)
  ```

- [ ] èƒ½è®­ç»ƒæ¨¡å‹å¹¶ä½¿ç”¨æ—©åœ
  ```python
  model = lgb.train(
      params,
      train_data,
      num_boost_round=500,
      valid_sets=[train_data, valid_data],
      callbacks=[
          lgb.early_stopping(stopping_rounds=30),
          lgb.log_evaluation(period=50)
      ]
  )
  ```

- [ ] èƒ½ä¿å­˜å’ŒåŠ è½½æ¨¡å‹
  ```python
  model.save_model('model.txt')
  model = lgb.Booster(model_file='model.txt')
  ```

### âœ… Module 3.4: IC/ICIR è¯„ä¼°

- [ ] ç†è§£ä¸ºä»€ä¹ˆç”¨ IC è€Œä¸æ˜¯ MSE
  - é‡åŒ–åªå…³å¿ƒæ’åºï¼Œä¸å…³å¿ƒç»å¯¹å€¼
  - IC è¡¡é‡æ’åºèƒ½åŠ›ï¼ŒMSE è¡¡é‡ç²¾ç¡®åº¦
  - é¢„æµ‹ A=0.05, B=0.03 vs A=0.02, B=0.01
    â†’ MSE ä¸åŒï¼Œä½†æ’åºç›¸åŒï¼ŒIC ç›¸åŒ

- [ ] èƒ½è®¡ç®— IC å’Œ Rank IC
  ```python
  from scipy.stats import pearsonr, spearmanr

  ic, _ = pearsonr(y_pred, y_true)
  rank_ic, _ = spearmanr(y_pred, y_true)
  ```

- [ ] èƒ½è®¡ç®— ICIR å’Œ IC èƒœç‡
  ```python
  icir = ic_series.mean() / ic_series.std()
  win_rate = (ic_series > 0).mean()
  ```

- [ ] èƒ½è¯„ä¼°æ¨¡å‹è´¨é‡

  | IC å€¼ | æ¨¡å‹è´¨é‡ |
  |--------|----------|
  | > 0.10 | ğŸŒŸ é¡¶çº§ (éå¸¸ç½•è§) |
  | > 0.05 | âœ… ä¼˜ç§€ |
  | > 0.03 | âœ… æœ‰æ•ˆ |
  | > 0.02 | âš ï¸ ä¸€èˆ¬ |
  | < 0.02 | âŒ è¾ƒå¼± |

  | ICIR å€¼ | ç¨³å®šæ€§ |
  |----------|----------|
  | > 0.5 | ğŸŒŸ éå¸¸ç¨³å®š |
  | > 0.3 | âœ… è¾ƒç¨³å®š |
  | > 0.2 | âš ï¸ ä¸€èˆ¬ |
  | < 0.2 | âŒ ä¸ç¨³å®š |

### âœ… Module 3.5: ç‰¹å¾é‡è¦æ€§åˆ†æ

- [ ] çŸ¥é“ä¸‰ç§é‡è¦æ€§è®¡ç®—æ–¹å¼

  | æ–¹å¼ | é€Ÿåº¦ | å¯é æ€§ | æ¨èåº¦ |
  |------|------|--------|--------|
  | Split | å¿« | ä¸­ç­‰ | âš ï¸ ä¸€èˆ¬ |
  | Gain | å¿« | é«˜ | âœ… æ¨è |
  | Permutation | æ…¢ | æœ€é«˜ | âœ… æœ€å¯é  |

- [ ] èƒ½è·å–å’Œè§£è¯»ç‰¹å¾é‡è¦æ€§
  ```python
  importance = model.feature_importance(importance_type='gain')
  importance_df = pd.DataFrame({
      'feature': feature_names,
      'importance': importance
  }).sort_values('importance', ascending=False)
  ```

- [ ] èƒ½å®ç°ç‰¹å¾é€‰æ‹©ç­–ç•¥
  - é˜ˆå€¼é€‰æ‹©
  - Top-K é€‰æ‹©
  - é€’å½’ç‰¹å¾æ¶ˆé™¤ (RFE)

- [ ] èƒ½åˆ†æç‰¹å¾ç›¸å…³æ€§
  ```python
  corr_matrix = X[features].corr()
  high_corr_pairs = []
  for i, feat1 in enumerate(features):
      for feat2 in features[i+1:]:
          corr = corr_matrix.loc[feat1, feat2]
          if abs(corr) > threshold:
              high_corr_pairs.append((feat1, feat2, corr))
  ```

## ğŸ¯ å®è·µèƒ½åŠ›æ£€æŸ¥

### âœ… èƒ½ç‹¬ç«‹å®Œæˆçš„é¡¹ç›®

- [ ] **é¡¹ç›®1: åŸºç¡€æ¨¡å‹è®­ç»ƒ**
  - åŠ è½½æ•°æ®ï¼Œæ­£ç¡®åˆ’åˆ†è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†
  - è®­ç»ƒ LightGBM æ¨¡å‹
  - è¯„ä¼°æ¨¡å‹ IC å’Œ ICIR
  - æ‰“å°å®Œæ•´è¯„ä¼°æŠ¥å‘Š

- [ ] **é¡¹ç›®2: æ—¶åºæ•°æ®åˆ’åˆ†**
  - å®ç° Walk-Forward éªŒè¯
  - å®ç° Purging å’Œ Embargo
  - åˆ†æå¤šä¸ªçª—å£çš„ IC å˜åŒ–
  - è¯†åˆ«æ¨¡å‹æ˜¯å¦éœ€è¦é‡è®­ç»ƒ

- [ ] **é¡¹ç›®3: ç‰¹å¾å·¥ç¨‹**
  - è®¡ç®—å¤šç§ç‰¹å¾é‡è¦æ€§ï¼ˆSplit, Gain, Permutationï¼‰
  - åˆ†æç‰¹å¾ç›¸å…³æ€§
  - å®ç°ç‰¹å¾é€‰æ‹©
  - éªŒè¯ç‰¹å¾é€‰æ‹©çš„æ•ˆæœ

- [ ] **é¡¹ç›®4: æ¨¡å‹ä¼˜åŒ–**
  - è°ƒä¼˜å…³é”®å‚æ•°
  - å®ç°æ—©åœæœºåˆ¶
  - é˜²æ­¢è¿‡æ‹Ÿåˆ
  - æå‡æ¨¡å‹ IC å’Œ ICIR

### âœ… èƒ½å›ç­”çš„é—®é¢˜

1. **ä¸ºä»€ä¹ˆé‡åŒ–æŠ•èµ„åçˆ± Boosting è€Œä¸æ˜¯ Baggingï¼Ÿ**
   - Boosting æ›´é€‚åˆæ•æ‰å¾®å¼±ä¿¡å·
   - èƒ½é€æ­¥é™ä½åå·®
   - åœ¨é‡åŒ–åœºæ™¯ä¸­è¡¨ç°æ›´å¥½

2. **ä¸ºä»€ä¹ˆä¸èƒ½ç”¨éšæœºåˆ’åˆ†æ—¶åºæ•°æ®ï¼Ÿ**
   - ä¼šå¯¼è‡´æ•°æ®æ³„éœ²ï¼ˆæœªæ¥ä¿¡æ¯ï¼‰
   - ä¸ç¬¦åˆå®é™…æŠ•èµ„åœºæ™¯
   - æ¨¡å‹æ€§èƒ½è™šé«˜

3. **Purging å’Œ Embargo çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ**
   - Purging: åˆ é™¤è®­ç»ƒé›†æœ«å°¾ï¼Œé¿å…æ ‡ç­¾åŒ…å«æœªæ¥ä¿¡æ¯
   - Embargo: éªŒè¯é›†å¼€å¤´ç©ºå‡ºå‡ å¤©ï¼Œä½œä¸ºç¼“å†²
   - ä¸¤è€…ç»“åˆï¼Œæ›´ä¸¥æ ¼åœ°é˜²æ­¢ä¿¡æ¯æ³„éœ²

4. **IC å’Œ MSE å“ªä¸ªæ›´é‡è¦ï¼Ÿ**
   - IC æ›´é‡è¦ï¼Œå› ä¸ºé‡åŒ–åªå…³å¿ƒæ’åº
   - MSE è¡¡é‡ç²¾ç¡®åº¦ï¼Œä¸é€‚åˆé‡åŒ–åœºæ™¯
   - IC èƒ½ç›´æ¥åæ˜ é¢„æµ‹èƒ½åŠ›

5. **å¦‚ä½•åˆ¤æ–­æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆï¼Ÿ**
   - è®­ç»ƒé›† IC è¿œé«˜äºéªŒè¯é›† IC
   - ä½¿ç”¨æ—©åœæœºåˆ¶
   - å¢åŠ æ­£åˆ™åŒ–
   - å‡å°‘ç‰¹å¾æ•°é‡

6. **ç‰¹å¾é‡è¦æ€§åˆ†æçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ**
   - ç†è§£æ¨¡å‹å†³ç­–é€»è¾‘
   - è¯†åˆ«æœ‰æ•ˆå› å­
   - å‰”é™¤å†—ä½™ç‰¹å¾
   - æå‡æ¨¡å‹æ€§èƒ½

## ğŸ“š æ¨èå­¦ä¹ è·¯å¾„

### ğŸŸ¢ åˆå­¦è€…è·¯å¾„

```
1. ç†è§£ Gradient Boosting åŸç†
   â†“
2. å­¦ä¹ æ—¶åºæ•°æ®åˆ’åˆ†
   â†“
3. è®­ç»ƒç¬¬ä¸€ä¸ª LightGBM æ¨¡å‹
   â†“
4. è®¡ç®— IC å’Œ ICIR
   â†“
5. åˆ†æç‰¹å¾é‡è¦æ€§
```

### ğŸŸ¡ è¿›é˜¶è·¯å¾„

```
1. æŒæ¡ IC ä¼˜åŒ–è®­ç»ƒ
   â†“
2. å­¦ä¹ åœ¨çº¿å­¦ä¹ 
   â†“
3. å®ç° Walk-Forward éªŒè¯
   â†“
4. æŒæ¡é«˜çº§è¯„ä¼°æ–¹æ³•
   â†“
5. è¿›è¡Œç¨³å®šæ€§åˆ†æ
```

### ğŸ”´ å®æˆ˜è·¯å¾„

```
1. ä»å®é™…é¡¹ç›®å‡ºå‘
   â†“
2. é‡åˆ°é—®é¢˜æŸ¥æ–‡æ¡£
   â†“
3. ç†è®ºåŸç†å­¦ä¹ 
   â†“
4. å®è·µåº”ç”¨
   â†“
5. æŒç»­ä¼˜åŒ–è¿­ä»£
```

## ğŸ”§ å·¥å…·ç®±

### å¿…å¤‡ Python åº“

```python
import lightgbm as lgb  # LightGBM
import pandas as pd       # æ•°æ®å¤„ç†
import numpy as np        # æ•°å€¼è®¡ç®—
from scipy.stats import pearsonr, spearmanr  # IC è®¡ç®—
import matplotlib.pyplot as plt  # å¯è§†åŒ–
import shap  # SHAP è§£é‡Š
```

### å¸¸ç”¨ä»£ç ç‰‡æ®µ

#### 1. æ•°æ®åˆ’åˆ†

```python
def train_val_test_split(X, y, dates, train_ratio=0.7, val_ratio=0.15):
    unique_dates = np.unique(dates)
    n_dates = len(unique_dates)
    train_end_idx = int(n_dates * train_ratio)
    val_end_idx = int(n_dates * (train_ratio + val_ratio))

    train_mask = dates <= unique_dates[train_end_idx]
    valid_mask = (dates > unique_dates[train_end_idx]) & (dates <= unique_dates[val_end_idx])
    test_mask = dates > unique_dates[val_end_idx]

    return (X[train_mask], X[valid_mask], X[test_mask]), (y[train_mask], y[valid_mask], y[test_mask])
```

#### 2. æ¨¡å‹è®­ç»ƒ

```python
def train_lightgbm(X_train, y_train, X_valid, y_valid, params):
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)

    model = lgb.train(
        params,
        train_data,
        num_boost_round=500,
        valid_sets=[train_data, valid_data],
        callbacks=[
            lgb.early_stopping(stopping_rounds=30),
            lgb.log_evaluation(period=50)
        ]
    )

    return model
```

#### 3. IC è¯„ä¼°

```python
def evaluate_ic(y_pred, y_true):
    ic, _ = pearsonr(y_pred, y_true)
    rank_ic, _ = spearmanr(y_pred, y_true)
    return ic, rank_ic
```

#### 4. ç‰¹å¾é‡è¦æ€§

```python
def get_feature_importance(model, feature_names, importance_type='gain'):
    importance = model.feature_importance(importance_type=importance_type)
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importance
    }).sort_values('importance', ascending=False)
    return importance_df
```

## ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ 

å®Œæˆ Week 3 åï¼Œä½ å·²ç»æŒæ¡äº†ï¼š

âœ… Gradient Boosting åŸç†
âœ… æ—¶åºæ•°æ®æ­£ç¡®åˆ’åˆ†
âœ… LightGBM æ¨¡å‹è®­ç»ƒ
âœ… é‡åŒ–æ¨¡å‹è¯„ä¼° (IC/ICIR)
âœ… ç‰¹å¾é‡è¦æ€§åˆ†æä¸é€‰æ‹©

**ä¸‹ä¸€æ­¥ï¼šWeek 4 - ç­–ç•¥å›æµ‹**

ä½ å°†å­¦ä¹ ï¼š
- äº¤æ˜“ç­–ç•¥åŸç† (Top-Kã€ç­‰æƒé‡ã€ICåŠ æƒ)
- æŠ•èµ„ç»„åˆæ„å»ºæ–¹æ³•
- å›æµ‹æ¡†æ¶ä½¿ç”¨
- é£é™©æŒ‡æ ‡è®¡ç®— (å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ç­‰)

---

**ç¥å­¦ä¹ é¡ºåˆ©ï¼** ğŸ‰
