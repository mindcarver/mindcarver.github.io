# 基础理论系列 - RNN与LSTM原理

## 📚 系列概述
本系列文档涵盖深度学习基础、序列数据处理、RNN与LSTM的核心原理和数学基础。

---

## 📖 文档列表

1. [深度学习基础](#深度学习基础)
2. [序列数据与RNN](#序列数据与rnn)
3. [LSTM原理详解](#lstm原理详解)

---

## 深度学习基础

### 核心概念

#### 1. 深度学习定义
- **本质**: 基于多层神经网络的机器学习方法
- **特点**: 自动学习特征表达
- **优势**: 在非结构化数据（图像、语音、文本）上表现优异

#### 2. 神经元模型
```
输入层 → 加权求和 → 激活函数 → 输出
```

**数学表达式**:
```
y = f(Σ(w_i * x_i) + b)
```

#### 3. 常用激活函数

| 函数 | 公式 | 输出范围 | 特点 |
|------|------|---------|------|
| **Sigmoid** | 1/(1+e^(-x)) | (0, 1) | 输出概率，梯度消失 |
| **Tanh** | (e^x-e^(-x))/(e^x+e^(-x)) | (-1, 1) | 零中心，梯度消失 |
| **ReLU** | max(0, x) | [0, +∞) | 缓解梯度消失，计算快 |
| **Leaky ReLU** | max(αx, x), α<<1 | (-∞, +∞) | 解决ReLU死亡问题 |

### 深度学习 vs 传统机器学习

| 维度 | 传统机器学习 | 深度学习 |
|------|-------------|---------|
| **特征工程** | 手工设计 | 自动学习 |
| **数据需求** | 中等 | 大量 |
| **训练时间** | 短 | 长 |
| **可解释性** | 较好 | 较差 |

### 量化投资中的应用场景
- **时序预测**: 股价、收益率预测
- **特征学习**: 自动提取有效因子
- **模式识别**: 市场状态识别
- **风险预测**: 违约概率、波动率预测

---

## 序列数据与RNN

### 序列数据特征

#### 1. 定义
具有时间或顺序依赖的数据

#### 2. 特点
- **顺序重要**: 数据点的顺序有意义
- **长度可变**: 不同序列长度不同
- **上下文依赖**: 当前值依赖历史
- **模式重复**: 存在周期性模式

#### 3. 常见类型
- **时间序列**: 股价、天气、销量
- **文本**: 句子、文章
- **语音**: 音频波形
- **视频**: 帧序列

### RNN（循环神经网络）

#### 1. 核心思想
- **共享参数**: 同一神经网络处理每个时间步
- **隐藏状态**: 将历史信息传递到下一个时间步

#### 2. RNN架构

```
时间步 t:
  输入 x_t ──┐
              ├── 加权求和 → 激活 → 隐藏状态 h_t
  上一步 h_{t-1} ──┘
              ↓
          输出 y_t
```

#### 3. 数学公式

**隐藏状态更新**:
```
h_t = f(W_h * h_{t-1} + W_x * x_t + b)
```

**输出计算**:
```
y_t = g(W_y * h_t + c)
```

**变量说明**:
- `h_t`: 时间t的隐藏状态（包含历史信息）
- `x_t`: 时间t的输入
- `W_h`: 隐藏状态权重矩阵
- `W_x`: 输入权重矩阵
- `W_y`: 输出权重矩阵
- `b, c`: 偏置项
- `f, g`: 激活函数（通常为tanh）

#### 4. RNN计算示例

```python
# 参数
W_h = 0.5
W_x = 0.3
b = 0.1

# 输入序列
inputs = [0.1, 0.2, 0.3, 0.4]
h_prev = 0.0  # 初始隐藏状态

# 时间步计算
for t, x_t in enumerate(inputs, 1):
    h_t = tanh(W_h * h_prev + W_x * x_t + b)
    # h_t 包含了历史信息 (x_1, x_2, ..., x_t)
    h_prev = h_t
```

### RNN的局限性

#### 梯度消失问题

**原因分析**:

链式法则:
```
∂L/∂h_1 = ∂L/∂h_T * ∂h_T/∂h_{T-1} * ... * ∂h_2/∂h_1
```

如果 |∂h_t/∂h_{t-1}| < 1，乘积会快速趋近于0

**梯度消失示例**:

假设 |∂h_t/∂h_{t-1}| = 0.9:
- 时间步 10: 0.9^10 = 0.35
- 时间步 50: 0.9^50 = 0.005
- 时间步 100: 0.9^100 = 0.00003

**影响**:
- 早期信息的梯度接近0
- 无法学习长期依赖
- 只能记住短期信息（约10-20步）

#### 解决方案
- 使用LSTM（推荐）
- 使用GRU
- 梯度裁剪（解决梯度爆炸）

---

## LSTM原理详解

### LSTM的创新

#### 三大核心创新

**1. 细胞状态（Cell State, C_t）**
- 信息高速公路
- 可以长期保持信息
- 通过门控制信息流动

**2. 遗忘门（Forget Gate, f_t）**
- 决定丢弃什么信息
- 输出0-1的值（sigmoid）
- 0 = 完全遗忘，1 = 完全保留

**3. 输入门（Input Gate, i_t）**
- 决定添加什么信息
- 控制新信息的流入
- 选择性地更新细胞状态

**4. 输出门（Output Gate, o_t）**
- 决定输出什么信息
- 基于细胞状态和隐藏状态
- 生成最终的输出

### LSTM单元结构

#### 完整计算流程

**Step 1: 遗忘门**
```
f_t = σ(W_f * [h_{t-1}, x_t] + b_f)
```
- 决定从细胞状态中丢弃什么信息
- σ: sigmoid函数（输出0-1）

**Step 2: 输入门**
```
i_t = σ(W_i * [h_{t-1}, x_t] + b_i)
C̃_t = tanh(W_C * [h_{t-1}, x_t] + b_C)
```
- `i_t`：决定更新哪些值
- `C̃_t`：候选细胞状态

**Step 3: 细胞状态更新**
```
C_t = f_t * C_{t-1} + i_t * C̃_t
```
- `f_t * C_{t-1}`：遗忘部分信息
- `i_t * C̃_t`：添加部分新信息

**Step 4: 输出门**
```
o_t = σ(W_o * [h_{t-1}, x_t] + b_o)
```
- 决定输出什么信息

**Step 5: 隐藏状态更新**
```
h_t = o_t * tanh(C_t)
```

### LSTM门的作用示例

#### 股票价格预测场景

**场景1: 市场突然暴跌**
- 遗忘门：保留（记住暴跌事件）
- 输入门：添加（记录恐慌情绪）
- 输出门：输出（影响短期预测）

**场景2: 长期牛市趋势**
- 遗忘门：保留（长期趋势很重要）
- 输入门：添加（持续更新）
- 输出门：输出（影响预测）

**场景3: 短期噪音**
- 遗忘门：遗忘（忽略短期波动）
- 输入门：不添加（不记录噪音）
- 输出门：输出（基于长期信息）

### LSTM vs RNN vs GRU

#### 架构对比

| 特性 | RNN | LSTM | GRU |
|------|-----|------|-----|
| **长期依赖** | 差 | 优秀 | 良好 |
| **参数数量** | 少 | 多 | 中 |
| **训练速度** | 快 | 中 | 快 |
| **计算复杂度** | 低 | 高 | 中 |
| **门数量** | 0 | 3 | 2 |
| **细胞状态** | 无 | 有 | 无 |
| **记忆能力** | 短期 | 长 | 中长 |
| **量化使用** | 很少 | 常用 | 常用 |

#### GRU简介

GRU（Gated Recurrent Unit）是LSTM的简化版本:

**GRU计算**:
```
z_t = σ(W_z * [h_{t-1}, x_t])  # 更新门
r_t = σ(W_r * [h_{t-1}, x_t])  # 重置门
h̃_t = tanh(W * [r_t * h_{t-1}, x_t])
h_t = (1 - z_t) * h_{t-1} + z_t * h̃_t
```

**GRU特点**:
- 只有2个门（更新门、重置门）
- 参数更少，训练更快
- 性能与LSTM相近

#### 选择建议

**RNN**:
- 简单短期序列
- 快速原型
- 很少在量化中使用

**LSTM**:
- 长期依赖很重要
- 计算资源充足
- 量化投资常用

**GRU**:
- 平衡性能和速度
- 参数较少
- 也能用于量化

### LSTM的优势

#### 1. 解决梯度消失
- 细胞状态提供梯度通路
- 门机制控制梯度流动
- 能够学习长期依赖

#### 2. 灵活的信息控制
- 遗忘门：控制信息遗忘
- 输入门：控制信息添加
- 输出门：控制信息输出

#### 3. 应用广泛
- 时间序列预测
- 自然语言处理
- 语音识别
- 视频分析

---

## 核心知识点总结

### 深度学习基础
- ✅ 神经元模型和激活函数
- ✅ 多层感知机（MLP）
- ✅ 深度学习 vs 传统机器学习
- ✅ 在量化投资中的应用

### RNN原理
- ✅ 序列数据特征
- ✅ RNN架构和数学公式
- ✅ 梯度消失问题
- ✅ RNN的局限性

### LSTM原理
- ✅ LSTM的三大创新
- ✅ 细胞状态和门机制
- ✅ LSTM计算流程
- ✅ LSTM vs RNN vs GRU对比
- ✅ 选择建议

---

## 下一步

继续学习: [PyTorch框架系列](../02_PyTorch框架系列/README.md)
