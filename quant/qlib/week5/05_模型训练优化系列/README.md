# æ¨¡å‹è®­ç»ƒä¼˜åŒ–ç³»åˆ— - è®­ç»ƒä¸ä¼˜åŒ–ç­–ç•¥

## ğŸ“š ç³»åˆ—æ¦‚è¿°
æœ¬ç³»åˆ—æ–‡æ¡£æ¶µç›–æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ã€è®­ç»ƒå¾ªç¯ã€æ—©åœç­–ç•¥ã€æ­£åˆ™åŒ–æ–¹æ³•å’Œè¶…å‚æ•°ä¼˜åŒ–ã€‚

---

## ğŸ“– æ–‡æ¡£åˆ—è¡¨

1. [æŸå¤±å‡½æ•°](#æŸå¤±å‡½æ•°)
2. [ä¼˜åŒ–å™¨](#ä¼˜åŒ–å™¨)
3. [è®­ç»ƒå¾ªç¯](#è®­ç»ƒå¾ªç¯)
4. [æ—©åœç­–ç•¥](#æ—©åœç­–ç•¥)
5. [æ­£åˆ™åŒ–](#æ­£åˆ™åŒ–)
6. [å­¦ä¹ ç‡è°ƒåº¦](#å­¦ä¹ ç‡è°ƒåº¦)

---

## æŸå¤±å‡½æ•°

### å›å½’ä»»åŠ¡æŸå¤±å‡½æ•°

#### 1. MSEï¼ˆMean Squared Errorï¼‰

```python
import torch.nn as nn

criterion = nn.MSELoss()

predictions = torch.randn(10, 1)
targets = torch.randn(10, 1)

loss = criterion(predictions, targets)
```

**ç‰¹ç‚¹**:
- å¯¹å¤§è¯¯å·®æ•æ„Ÿ
- è®¡ç®—ç®€å•
- å¸¸ç”¨

**é€‚ç”¨åœºæ™¯**:
- å›å½’ä»»åŠ¡
- å¼‚å¸¸å€¼è¾ƒå°‘çš„æ•°æ®

#### 2. MAEï¼ˆMean Absolute Errorï¼‰

```python
criterion = nn.L1Loss()

loss = criterion(predictions, targets)
```

**ç‰¹ç‚¹**:
- å¯¹å¼‚å¸¸å€¼é²æ£’
- æ¢¯åº¦æ’å®š
- ä¸å¸¸ç”¨

**é€‚ç”¨åœºæ™¯**:
- å¼‚å¸¸å€¼è¾ƒå¤š
- éœ€è¦é²æ£’æ€§

#### 3. Smooth L1 Loss

```python
criterion = nn.SmoothL1Loss()

loss = criterion(predictions, targets)
```

**ç‰¹ç‚¹**:
- MSEå’ŒMAEçš„æŠ˜ä¸­
- å¯¹å¼‚å¸¸å€¼é²æ£’
- æ¨èä½¿ç”¨

**é€‚ç”¨åœºæ™¯**:
- é€šç”¨å›å½’ä»»åŠ¡
- å¹³è¡¡é²æ£’æ€§å’Œç¨³å®šæ€§

### è‡ªå®šä¹‰æŸå¤±å‡½æ•°

```python
def custom_loss(predictions, targets, model, lambda_l1=0.01, lambda_l2=0.01):
    """
    è‡ªå®šä¹‰æŸå¤±å‡½æ•°

    å‚æ•°:
        predictions: æ¨¡å‹é¢„æµ‹
        targets: çœŸå®å€¼
        model: æ¨¡å‹
        lambda_l1: L1æ­£åˆ™åŒ–ç³»æ•°
        lambda_l2: L2æ­£åˆ™åŒ–ç³»æ•°
    """
    # MSE
    mse = torch.mean((predictions - targets) ** 2)

    # L1æ­£åˆ™åŒ–
    l1_reg = sum(p.abs().sum() for p in model.parameters())

    # L2æ­£åˆ™åŒ–
    l2_reg = sum(p.pow(2).sum() for p in model.parameters())

    # æ€»æŸå¤±
    total_loss = mse + lambda_l1 * l1_reg + lambda_l2 * l2_reg

    return total_loss
```

### æŸå¤±å‡½æ•°é€‰æ‹©å»ºè®®

| åœºæ™¯ | æ¨èæŸå¤±å‡½æ•° |
|------|-------------|
| **é€šç”¨å›å½’** | MSE |
| **å¼‚å¸¸å€¼å¤š** | MAE |
| **å¹³è¡¡é€‰æ‹©** | Smooth L1 |
| **éœ€è¦æ­£åˆ™åŒ–** | è‡ªå®šä¹‰æŸå¤± |

---

## ä¼˜åŒ–å™¨

### å¸¸ç”¨ä¼˜åŒ–å™¨

#### 1. SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰

```python
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.01,
    momentum=0.9,
    weight_decay=1e-4  # L2æ­£åˆ™åŒ–
)
```

**ç‰¹ç‚¹**:
- ç®€å•ç¨³å®š
- æ”¶æ•›æ…¢
- éœ€è¦è°ƒå‚

**é€‚ç”¨åœºæ™¯**:
- æ•°æ®é‡å¤§
- éœ€è¦ç¨³å®šè®­ç»ƒ

#### 2. Adamï¼ˆæ¨èï¼‰

```python
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=0.001,
    betas=(0.9, 0.999),
    weight_decay=1e-4
)
```

**ç‰¹ç‚¹**:
- è‡ªé€‚åº”å­¦ä¹ ç‡
- æ”¶æ•›å¿«
- é»˜è®¤é€‰æ‹©

**é€‚ç”¨åœºæ™¯**:
- é€šç”¨åœºæ™¯
- å¿«é€Ÿè¿­ä»£

#### 3. RMSprop

```python
optimizer = torch.optim.RMSprop(
    model.parameters(),
    lr=0.001,
    alpha=0.99
)
```

**ç‰¹ç‚¹**:
- é€‚åˆRNN
- è‡ªé€‚åº”å­¦ä¹ ç‡

**é€‚ç”¨åœºæ™¯**:
- RNN/LSTM
- æ—¶åºé¢„æµ‹

### ä¼˜åŒ–å™¨å¯¹æ¯”

| ä¼˜åŒ–å™¨ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨èå­¦ä¹ ç‡ |
|--------|---------|------|------|-----------|
| **SGD** | æ•°æ®é‡å¤§ | ç®€å•ç¨³å®š | æ”¶æ•›æ…¢ | 0.01-0.1 |
| **Adam** | é€šç”¨ | æ”¶æ•›å¿« | å¯èƒ½è¿‡æ‹Ÿåˆ | 0.0001-0.001 |
| **RMSprop** | RNN | é€‚åˆåºåˆ— | å‚æ•°è°ƒæ•´ | 0.001-0.01 |

### ä¼˜åŒ–å™¨é€‰æ‹©å»ºè®®

**é€‰æ‹©Adam**:
- é»˜è®¤é€‰æ‹©
- å¿«é€Ÿè¿­ä»£
- é€šç”¨åœºæ™¯

**é€‰æ‹©SGD**:
- æ•°æ®é‡å¤§
- éœ€è¦ç¨³å®š
- ç ”ç©¶è®ºæ–‡

**é€‰æ‹©RMSprop**:
- RNN/LSTM
- æ—¶åºé¢„æµ‹

---

## è®­ç»ƒå¾ªç¯

### å®Œæ•´è®­ç»ƒå¾ªç¯

```python
import copy
from tqdm import tqdm

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50):
    """
    è®­ç»ƒæ¨¡å‹

    å‚æ•°:
        model: æ¨¡å‹
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        num_epochs: è®­ç»ƒè½®æ•°

    è¿”å›:
        model: æœ€ä½³æ¨¡å‹
        train_losses: è®­ç»ƒæŸå¤±
        val_losses: éªŒè¯æŸå¤±
    """

    # è®°å½•æŸå¤±
    train_losses = []
    val_losses = []

    # æœ€ä½³æ¨¡å‹
    best_model = None
    best_val_loss = float('inf')

    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0

        # ä½¿ç”¨è¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')

        for X_batch, y_batch in pbar:
            # å‰å‘ä¼ æ’­
            predictions = model(X_batch)
            loss = criterion(predictions.squeeze(), y_batch)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            # å‚æ•°æ›´æ–°
            optimizer.step()

            train_loss += loss.item()

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({'loss': loss.item()})

        train_loss /= len(train_loader)
        train_losses.append(train_loss)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0

        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                predictions = model(X_batch)
                loss = criterion(predictions.squeeze(), y_batch)
                val_loss += loss.item()

        val_loss /= len(val_loader)
        val_losses.append(val_loss)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model = copy.deepcopy(model.state_dict())

        # æ‰“å°è¿›åº¦
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"  Train Loss: {train_loss:.4f}")
        print(f"  Val Loss: {val_loss:.4f}")

    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_model)

    return model, train_losses, val_losses
```

### è®­ç»ƒå¾ªç¯ç¤ºä¾‹

```python
# å®šä¹‰æ¨¡å‹
model = LSTMModel(
    input_size=10,
    hidden_size=64,
    num_layers=2,
    output_size=1
)

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒ
model, train_losses, val_losses = train_model(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=criterion,
    optimizer=optimizer,
    num_epochs=50
)
```

---

## æ—©åœç­–ç•¥ï¼ˆEarly Stoppingï¼‰

### åŸç†
- ç›‘æ§éªŒè¯é›†æŸå¤±
- è¿ç»­Nä¸ªepochä¸æ”¹å–„åˆ™åœæ­¢
- é˜²æ­¢è¿‡æ‹Ÿåˆ

### å®ç°

```python
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0, verbose=True):
        """
        æ—©åœç­–ç•¥

        å‚æ•°:
            patience: å®¹å¿ä¸æ”¹å–„çš„epochæ•°
            min_delta: æœ€å°æ”¹å–„å¹…åº¦
            verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
        """
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.best_model = None

    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(val_loss, model)
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        if self.verbose:
            print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}). Saving model...')
        self.best_model = copy.deepcopy(model.state_dict())

    def load_best_model(self, model):
        """åŠ è½½æœ€ä½³æ¨¡å‹"""
        if self.best_model is not None:
            model.load_state_dict(self.best_model)
```

### ä½¿ç”¨æ—©åœ

```python
# åˆ›å»ºæ—©åœ
early_stopping = EarlyStopping(patience=10, min_delta=0.0001, verbose=True)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    # è®­ç»ƒ
    train_loss = train(model, train_loader, criterion, optimizer)

    # éªŒè¯
    val_loss = validate(model, val_loader, criterion)

    # æ£€æŸ¥æ—©åœ
    early_stopping(val_loss, model)
    if early_stopping.early_stop:
        print("Early stopping!")
        break

# åŠ è½½æœ€ä½³æ¨¡å‹
early_stopping.load_best_model(model)
```

---

## æ­£åˆ™åŒ–

### 1. L1/L2æ­£åˆ™åŒ–

```python
def l1_regularization(model, lambda_l1=0.01):
    """
    L1æ­£åˆ™åŒ–

    å‚æ•°:
        model: æ¨¡å‹
        lambda_l1: L1ç³»æ•°

    è¿”å›:
        L1æŸå¤±
    """
    l1_loss = 0
    for param in model.parameters():
        l1_loss += torch.sum(torch.abs(param))
    return lambda_l1 * l1_loss

def l2_regularization(model, lambda_l2=0.01):
    """
    L2æ­£åˆ™åŒ–

    å‚æ•°:
        model: æ¨¡å‹
        lambda_l2: L2ç³»æ•°

    è¿”å›:
        L2æŸå¤±
    """
    l2_loss = 0
    for param in model.parameters():
        l2_loss += torch.sum(param ** 2)
    return lambda_l2 * l2_loss

# ä½¿ç”¨
loss = criterion(predictions, targets)
loss += l1_regularization(model, lambda_l1=0.01)
loss += l2_regularization(model, lambda_l2=0.01)
```

### 2. Dropout

```python
class LSTMModelWithDropout(nn.Module):
    def __init__(self, input_size, hidden_size, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.dropout = nn.Dropout(dropout)  # Dropoutå±‚
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = self.dropout(lstm_out)
        output = self.fc(lstm_out[:, -1, :])
        return output
```

### 3. æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatchNormï¼‰

```python
class LSTMModelWithBN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.bn = nn.BatchNorm1d(hidden_size)  # BatchNorm
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = self.bn(lstm_out[:, -1, :])
        output = self.fc(lstm_out)
        return output
```

### 4. æ¢¯åº¦è£å‰ª

```python
# åå‘ä¼ æ’­å
loss.backward()

# æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# æˆ–æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
```

---

## å­¦ä¹ ç‡è°ƒåº¦

### å­¦ä¹ ç‡è¡°å‡

#### 1. StepLR

```python
scheduler = torch.optim.lr_scheduler.StepLR(
    optimizer,
    step_size=10,  # æ¯10ä¸ªepoch
    gamma=0.1      # å­¦ä¹ ç‡ä¹˜0.1
)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    train(model, train_loader, optimizer, criterion)
    scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡
```

#### 2. ReduceLROnPlateau

```python
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',      # ç›‘æ§æŒ‡æ ‡è¶Šå°è¶Šå¥½
    factor=0.1,      # å­¦ä¹ ç‡ä¹˜0.1
    patience=5,      # å®¹å¿5ä¸ªepochä¸æ”¹å–„
    verbose=True
)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    train_loss = train(model, train_loader, optimizer, criterion)
    val_loss = validate(model, val_loader, criterion)
    scheduler.step(val_loss)  # åŸºäºéªŒè¯æŸå¤±è°ƒæ•´
```

#### 3. CosineAnnealingLR

```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=50,        # æ€»epochæ•°
    eta_min=1e-6     # æœ€å°å­¦ä¹ ç‡
)

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    train(model, train_loader, optimizer, criterion)
    scheduler.step()
```

#### 4. Warmup

```python
from torch.optim.lr_scheduler import LambdaLR

def warmup_lambda(epoch, warmup_epochs=10):
    """Warmupå­¦ä¹ ç‡è°ƒåº¦"""
    if epoch < warmup_epochs:
        return epoch / warmup_epochs
    return 1.0

scheduler = LambdaLR(
    optimizer,
    lr_lambda=lambda epoch: warmup_lambda(epoch, warmup_epochs=10)
)
```

### å­¦ä¹ ç‡å¯è§†åŒ–

```python
import matplotlib.pyplot as plt

# è®°å½•å­¦ä¹ ç‡
learning_rates = []

for epoch in range(num_epochs):
    # è®­ç»ƒ
    train(model, train_loader, optimizer, criterion)

    # è®°å½•å­¦ä¹ ç‡
    learning_rates.append(optimizer.param_groups[0]['lr'])

    # æ›´æ–°å­¦ä¹ ç‡
    scheduler.step()

# ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿
plt.figure(figsize=(10, 6))
plt.plot(range(num_epochs), learning_rates)
plt.xlabel('Epoch')
plt.ylabel('Learning Rate')
plt.title('Learning Rate Schedule')
plt.show()
```

---

## è®­ç»ƒæŠ€å·§

### 1. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for X_batch, y_batch in train_loader:
    optimizer.zero_grad()

    with autocast():
        predictions = model(X_batch)
        loss = criterion(predictions.squeeze(), y_batch)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### 2. æ¢¯åº¦ç´¯ç§¯

```python
accumulation_steps = 4

for i, (X_batch, y_batch) in enumerate(train_loader):
    predictions = model(X_batch)
    loss = criterion(predictions.squeeze(), y_batch)

    loss = loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 3. æ¨¡å‹å¹¶è¡Œ

```python
# å¤šGPUè®­ç»ƒ
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)

model.to('cuda')
```

---

## æ ¸å¿ƒçŸ¥è¯†ç‚¹æ€»ç»“

### æŸå¤±å‡½æ•°
- âœ… MSEã€MAEã€Smooth L1
- âœ… è‡ªå®šä¹‰æŸå¤±å‡½æ•°
- âœ… é€‰æ‹©å»ºè®®

### ä¼˜åŒ–å™¨
- âœ… SGDã€Adamã€RMSprop
- âœ… ä¼˜åŒ–å™¨å¯¹æ¯”
- âœ… é€‰æ‹©å»ºè®®

### è®­ç»ƒå¾ªç¯
- âœ… å®Œæ•´è®­ç»ƒå¾ªç¯
- âœ… è¿›åº¦æ¡
- âœ… æ¢¯åº¦è£å‰ª

### æ—©åœç­–ç•¥
- âœ… EarlyStoppingå®ç°
- âœ… ä½¿ç”¨æ–¹æ³•
- âœ… æ¨¡å‹ä¿å­˜

### æ­£åˆ™åŒ–
- âœ… L1/L2æ­£åˆ™åŒ–
- âœ… Dropout
- âœ… BatchNorm
- âœ… æ¢¯åº¦è£å‰ª

### å­¦ä¹ ç‡è°ƒåº¦
- âœ… StepLR
- âœ… ReduceLROnPlateau
- âœ… CosineAnnealingLR
- âœ… Warmup

---

## ä¸‹ä¸€æ­¥

ç»§ç»­å­¦ä¹ : [æ¨¡å‹è¯„ä¼°ä¸å¯¹æ¯”ç³»åˆ—](../06_æ¨¡å‹è¯„ä¼°ä¸å¯¹æ¯”ç³»åˆ—/README.md)
