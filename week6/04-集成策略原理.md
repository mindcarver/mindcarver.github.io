---
layout: default
section: Week6
title: 04-é›†æˆç­–ç•¥åŸç†
---

# 04. é›†æˆç­–ç•¥åŸç† ğŸ”®

> "ä¸‰ä¸ªè‡­çš®åŒ ï¼Œé¡¶ä¸ªè¯¸è‘›äº®" ~ ä¸­å›½è°šè¯­

åœ¨é‡åŒ–äº¤æ˜“ä¸­ï¼Œ**é›†æˆå­¦ä¹ ** (Ensemble Learning) å°±æ˜¯ç”¨å¤šä¸ªæ¨¡å‹å…±åŒå†³ç­–ï¼Œå¾€å¾€èƒ½è·å¾—æ¯”å•ä¸ªæ¨¡å‹æ›´å¥½çš„æ•ˆæœï¼

---

## ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦é›†æˆï¼Ÿ

### å•ä¸ªæ¨¡å‹çš„å±€é™

```python
# ğŸ˜± å•ä¸ªæ¨¡å‹çš„é—®é¢˜
def single_model_problems():
    """
    å•ä¸ªæ¨¡å‹çš„å±€é™æ€§
    """
    
    model_limitations = {
        "è¿‡æ‹Ÿåˆ": "æ¨¡å‹åœ¨è®­ç»ƒé›†è¡¨ç°å¥½ï¼Œä½†æ³›åŒ–èƒ½åŠ›å·®",
        "æ¬ æ‹Ÿåˆ": "æ¨¡å‹å¤ªç®€å•ï¼Œå­¦ä¸åˆ°å¤æ‚æ¨¡å¼",
        "ä¸ç¨³å®šæ€§": "æ•°æ®çš„å¾®å°å˜åŒ–å¯¼è‡´é¢„æµ‹å¤§å˜",
        "ç›²åŒº": "æŸäº›æƒ…å†µæ¨¡å‹å®Œå…¨çœ‹ä¸æ‡‚",
        "åè§": "æ¨¡å‹å¯èƒ½å­¦åˆ°äº†é”™è¯¯çš„åè§"
    }
    
    print("ğŸ¤” å•ä¸ªæ¨¡å‹çš„é—®é¢˜:")
    for problem, description in model_limitations.items():
        print(f"  âŒ {problem}: {description}")
```

### é›†æˆçš„å¨åŠ›

```python
def ensemble_power():
    """é›†æˆçš„å¨åŠ›"""
    
    # ä¸‰ä¸ªæ¨¡å‹
    model_predictions = {
        "æ¨¡å‹A": [0.1, 0.9, 0.2, 0.8],
        "æ¨¡å‹B": [0.3, 0.7, 0.4, 0.6],
        "æ¨¡å‹C": [0.2, 0.8, 0.3, 0.7]
    }
    
    # ç®€å•å¹³å‡
    ensemble_pred = []
    for i in range(4):
        avg = sum([model_predictions[m][i] for m in model_predictions]) / 3
        ensemble_pred.append(avg)
    
    print("ğŸ”® é›†æˆçš„å¨åŠ›:")
    print(f"  æ¨¡å‹A: {model_predictions['æ¨¡å‹A']}")
    print(f"  æ¨¡å‹B: {model_predictions['æ¨¡å‹B']}")
    print(f"  æ¨¡å‹C: {model_predictions['æ¨¡å‹C']}")
    print(f"  é›†æˆ:   {ensemble_pred}")
    
    # å‡è®¾çœŸå®å€¼
    true_values = [0.2, 0.8, 0.3, 0.7]
    
    # è®¡ç®—è¯¯å·®
    errors = {}
    for model in model_predictions:
        mse = sum([(model_predictions[model][i] - true_values[i])**2 
                   for i in range(4)]) / 4
        errors[model] = mse
    
    ensemble_mse = sum([(ensemble_pred[i] - true_values[i])**2 
                       for i in range(4)]) / 4
    errors["é›†æˆ"] = ensemble_mse
    
    print(f"\n  è¯¯å·®å¯¹æ¯”:")
    for model, error in errors.items():
        print(f"    {model}: {error:.4f}")

ensemble_power()
```

---

## ğŸ—³ï¸ æŠ•ç¥¨é›†æˆ (Voting)

æŠ•ç¥¨é›†æˆæ˜¯æœ€ç®€å•çš„é›†æˆæ–¹æ³•ã€‚

### ç¡¬æŠ•ç¥¨ (Hard Voting)

```python
import numpy as np
from collections import Counter

def hard_voting(predictions_list):
    """
    ç¡¬æŠ•ç¥¨ - å¤šæ•°å†³
    
    æ¯ä¸ªæ¨¡å‹ç»™å‡ºä¸€ä¸ªç±»åˆ«æ ‡ç­¾ï¼Œé€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„æ ‡ç­¾
    
    é€‚ç”¨åœºæ™¯: åˆ†ç±»é—®é¢˜
    """
    # ç»Ÿè®¡æ¯ä¸ªæ ‡ç­¾çš„ç¥¨æ•°
    votes = Counter(predictions_list)
    
    # æ‰¾å‡ºç¥¨æ•°æœ€å¤šçš„æ ‡ç­¾
    most_common = votes.most_common(1)[0]
    winner = most_common[0]
    count = most_common[1]
    
    print(f"ğŸ—³ï¸ ç¡¬æŠ•ç¥¨ç»“æœ:")
    print(f"  ç¥¨æ•°ç»Ÿè®¡: {dict(votes)}")
    print(f"  è·èƒœæ ‡ç­¾: {winner} ({count}ç¥¨)")
    
    return winner

# ä½¿ç”¨ç¤ºä¾‹
predictions = ['ä¹°å…¥', 'ä¹°å…¥', 'å–å‡º', 'ä¹°å…¥', 'æŒæœ‰']
winner = hard_voting(predictions)
```

### è½¯æŠ•ç¥¨ (Soft Voting)

```python
def soft_voting(probabilities_list, labels=['ä¹°å…¥', 'æŒæœ‰', 'å–å‡º']):
    """
    è½¯æŠ•ç¥¨ - æ¦‚ç‡å¹³å‡
    
    æ¯ä¸ªæ¨¡å‹ç»™å‡ºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œå¹³å‡åé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„
    
    é€‚ç”¨åœºæ™¯: åˆ†ç±»é—®é¢˜ï¼ˆæœ‰æ¦‚ç‡è¾“å‡ºï¼‰
    """
    # è®¡ç®—å¹³å‡æ¦‚ç‡
    avg_probs = np.mean(probabilities_list, axis=0)
    
    # æ‰¾å‡ºæ¦‚ç‡æœ€é«˜çš„ç±»åˆ«
    winner_idx = np.argmax(avg_probs)
    winner = labels[winner_idx]
    
    print(f"ğŸ—³ï¸ è½¯æŠ•ç¥¨ç»“æœ:")
    print(f"  å¹³å‡æ¦‚ç‡: {dict(zip(labels, avg_probs))}")
    print(f"  è·èƒœæ ‡ç­¾: {winner} ({avg_probs[winner_idx]:.2%})")
    
    return winner, avg_probs

# ä½¿ç”¨ç¤ºä¾‹
# 3ä¸ªæ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹ç»™å‡º3ä¸ªç±»åˆ«çš„æ¦‚ç‡
probabilities = [
    [0.7, 0.2, 0.1],  # æ¨¡å‹1
    [0.6, 0.3, 0.1],  # æ¨¡å‹2
    [0.8, 0.1, 0.1]   # æ¨¡å‹3
]
winner, probs = soft_voting(probabilities)
```

### æŠ•ç¥¨é›†æˆå®ç°

```python
from sklearn.ensemble import VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

def voting_ensemble_example():
    """æŠ•ç¥¨é›†æˆç¤ºä¾‹"""
    
    # åˆ›å»ºå¤šä¸ªåŸºç¡€æ¨¡å‹
    model1 = DecisionTreeClassifier(max_depth=5)
    model2 = RandomForestClassifier(n_estimators=100)
    model3 = SVC(probability=True)
    
    # åˆ›å»ºæŠ•ç¥¨é›†æˆ
    voting_ensemble = VotingClassifier(
        estimators=[
            ('dt', model1),
            ('rf', model2),
            ('svm', model3)
        ],
        voting='soft'  # 'soft' æˆ– 'hard'
    )
    
    print("ğŸ—³ï¸ æŠ•ç¥¨é›†æˆ:")
    print("  åŒ…å«æ¨¡å‹:")
    print("    - å†³ç­–æ ‘")
    print("    - éšæœºæ£®æ—")
    print("    - SVM")
    print(f"  æŠ•ç¥¨æ–¹å¼: è½¯æŠ•ç¥¨")
    
    return voting_ensemble
```

---

## â— å¹³å‡é›†æˆ (Averaging)

å¹³å‡é›†æˆé€‚ç”¨äºå›å½’é—®é¢˜ã€‚

### ç®€å•å¹³å‡ (Simple Average)

```python
def simple_average(predictions_list):
    """
    ç®€å•å¹³å‡
    
    æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹å€¼å–å¹³å‡
    
    æ•°å­¦å…¬å¼:
        $$ \hat{y}_{ensemble} = \frac{1}{N} \sum_{i=1}^{N} \hat{y}_i $$
    """
    # è®¡ç®—å¹³å‡
    avg_prediction = np.mean(predictions_list, axis=0)
    
    # è®¡ç®—æ ‡å‡†å·®ï¼ˆè¡¡é‡æ¨¡å‹é—´çš„åˆ†æ­§ï¼‰
    std_prediction = np.std(predictions_list, axis=0)
    
    print("â— ç®€å•å¹³å‡ç»“æœ:")
    print(f"  é¢„æµ‹å€¼: {avg_prediction:.4f}")
    print(f"  æ ‡å‡†å·®: {std_prediction:.4f} (æ¨¡å‹åˆ†æ­§åº¦)")
    
    return avg_prediction, std_prediction

# ä½¿ç”¨ç¤ºä¾‹
predictions = [0.25, 0.30, 0.28, 0.27, 0.29]
avg, std = simple_average(predictions)
```

### åŠ æƒå¹³å‡ (Weighted Average)

```python
def weighted_average(predictions_list, weights):
    """
    åŠ æƒå¹³å‡
    
    æ ¹æ®æ¯ä¸ªæ¨¡å‹çš„é‡è¦æ€§åˆ†é…æƒé‡
    
    æ•°å­¦å…¬å¼:
        $$ \hat{y}_{ensemble} = \sum_{i=1}^{N} w_i \cdot \hat{y}_i $$
        
    å…¶ä¸­:
        - $w_i$: ç¬¬iä¸ªæ¨¡å‹çš„æƒé‡
        - $\sum w_i = 1$: æƒé‡å’Œä¸º1
    """
    # ç¡®ä¿æƒé‡å’Œä¸º1
    weights = np.array(weights) / sum(weights)
    
    # è®¡ç®—åŠ æƒå¹³å‡
    weighted_avg = np.average(predictions_list, weights=weights)
    
    print("â— åŠ æƒå¹³å‡ç»“æœ:")
    print(f"  é¢„æµ‹å€¼: {weighted_avg:.4f}")
    print(f"  æƒé‡åˆ†å¸ƒ: {weights}")
    print(f"  è´¡çŒ®åˆ†æ:")
    for i, (pred, weight) in enumerate(zip(predictions_list, weights)):
        contribution = pred * weight
        print(f"    æ¨¡å‹{i+1}: {contribution:.4f} (æƒé‡{weight:.2f})")
    
    return weighted_avg

# ä½¿ç”¨ç¤ºä¾‹
predictions = [0.25, 0.30, 0.28, 0.27, 0.29]
weights = [0.1, 0.3, 0.2, 0.2, 0.2]  # æ¨¡å‹2æƒé‡æœ€é«˜
weighted_avg = weighted_average(predictions, weights)
```

### å¦‚ä½•ç¡®å®šæƒé‡ï¼Ÿ

```python
def calculate_weights_by_ic(ics):
    """
    æ ¹æ®ICå€¼ç¡®å®šæƒé‡
    
    ICå€¼è¶Šé«˜çš„æ¨¡å‹ï¼Œæƒé‡è¶Šå¤§
    
    è®¡ç®—æ–¹æ³•:
        1. å°†ICè½¬æ¢ä¸ºæ­£æ•°ï¼ˆå–ç»å¯¹å€¼ï¼‰
        2. å½’ä¸€åŒ–
    """
    # å–ç»å¯¹å€¼
    abs_ics = [abs(ic) for ic in ics]
    
    # å½’ä¸€åŒ–
    weights = np.array(abs_ics) / sum(abs_ics)
    
    print("ğŸ“Š æƒé‡è®¡ç®—:")
    print(f"  ICå€¼: {ics}")
    print(f"  æƒé‡: {[f'{w:.2%}' for w in weights]}")
    
    return weights

# ä½¿ç”¨ç¤ºä¾‹
ics = [0.05, 0.07, 0.06, 0.04, 0.08]
weights = calculate_weights_by_ic(ics)
```

---

## ğŸ—ï¸ Stackingé›†æˆ

Stackingæ˜¯ä¸€ç§æ›´é«˜çº§çš„é›†æˆæ–¹æ³•ï¼Œä½¿ç”¨"å…ƒæ¨¡å‹"æ¥ç»„åˆåŸºç¡€æ¨¡å‹ã€‚

### StackingåŸç†

```
å±‚çº§ç»“æ„:

ç¬¬ä¸€å±‚ï¼ˆåŸºç¡€å±‚ï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ¨¡å‹A   â”‚  â”‚ æ¨¡å‹B   â”‚  â”‚ æ¨¡å‹C   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚
     â†“            â†“            â†“
 é¢„æµ‹A        é¢„æµ‹B        é¢„æµ‹C
     â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
          ç»„åˆç‰¹å¾ [A, B, C]
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ç¬¬äºŒå±‚ï¼ˆå…ƒæ¨¡å‹ï¼‰      â”‚
    â”‚      Linear Regression â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
          æœ€ç»ˆé¢„æµ‹ç»“æœ
```

### Stackingå®ç°

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import KFold
import numpy as np

class StackingEnsemble:
    """Stackingé›†æˆ"""
    
    def __init__(self, base_models, meta_model):
        """
        åˆå§‹åŒ–Stacking
        
        Args:
            base_models: åŸºç¡€æ¨¡å‹åˆ—è¡¨
            meta_model: å…ƒæ¨¡å‹
        """
        self.base_models = base_models
        self.meta_model = meta_model
        self.trained_base_models = []
        
    def fit(self, X_train, y_train, n_folds=5):
        """
        è®­ç»ƒStackingé›†æˆ
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            n_folds: äº¤å‰éªŒè¯æŠ˜æ•°
        """
        print("ğŸ—ï¸ è®­ç»ƒStackingé›†æˆ...")
        
        # 1. è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆä½¿ç”¨äº¤å‰éªŒè¯ç”Ÿæˆå…ƒç‰¹å¾ï¼‰
        print(f"\n[æ­¥éª¤1/3] è®­ç»ƒåŸºç¡€æ¨¡å‹ ({n_folds}æŠ˜äº¤å‰éªŒè¯)...")
        
        kf = KFold(n_splits=n_folds)
        meta_features = np.zeros((len(X_train), len(self.base_models)))
        
        for i, base_model in enumerate(self.base_models):
            print(f"  è®­ç»ƒæ¨¡å‹ {i+1}/{len(self.base_models)}...")
            
            # äº¤å‰éªŒè¯é¢„æµ‹
            fold_predictions = []
            
            for train_idx, val_idx in kf.split(X_train):
                X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
                y_fold_train = y_train.iloc[train_idx]
                
                # è®­ç»ƒæ¨¡å‹
                model = base_model.__class__(**base_model.get_params())
                model.fit(X_fold_train, y_fold_train)
                
                # é¢„æµ‹éªŒè¯é›†
                pred = model.predict(X_fold_val)
                fold_predictions.append((val_idx, pred))
            
            # åˆå¹¶æ‰€æœ‰æŠ˜çš„é¢„æµ‹
            fold_predictions.sort(key=lambda x: x[0][0])  # æŒ‰ç´¢å¼•æ’åº
            meta_features[:, i] = np.concatenate([p[1] for p in fold_predictions])
        
        print(f"  âœ… åŸºç¡€æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå…ƒç‰¹å¾å½¢çŠ¶: {meta_features.shape}")
        
        # 2. åœ¨å…¨éƒ¨æ•°æ®ä¸Šé‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹
        print(f"\n[æ­¥éª¤2/3] åœ¨å…¨éƒ¨æ•°æ®ä¸Šé‡æ–°è®­ç»ƒåŸºç¡€æ¨¡å‹...")
        self.trained_base_models = []
        
        for base_model in self.base_models:
            model = base_model.__class__(**base_model.get_params())
            model.fit(X_train, y_train)
            self.trained_base_models.append(model)
        
        print(f"  âœ… åŸºç¡€æ¨¡å‹é‡æ–°è®­ç»ƒå®Œæˆ")
        
        # 3. è®­ç»ƒå…ƒæ¨¡å‹
        print(f"\n[æ­¥éª¤3/3] è®­ç»ƒå…ƒæ¨¡å‹...")
        self.meta_model.fit(meta_features, y_train)
        
        # æ˜¾ç¤ºå…ƒæ¨¡å‹ç‰¹å¾é‡è¦æ€§
        if hasattr(self.meta_model, 'coef_'):
            print(f"  å…ƒæ¨¡å‹æƒé‡: {[f'{w:.3f}' for w in self.meta_model.coef_]}")
        elif hasattr(self.meta_model, 'feature_importances_'):
            print(f"  å…ƒæ¨¡å‹é‡è¦æ€§: {[f'{w:.3f}' for w in self.meta_model.feature_importances_]}")
        
        print(f"\nâœ… Stackingè®­ç»ƒå®Œæˆï¼")
        
    def predict(self, X):
        """
        é¢„æµ‹
        
        Args:
            X: æµ‹è¯•ç‰¹å¾
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        # 1. ä½¿ç”¨åŸºç¡€æ¨¡å‹é¢„æµ‹
        meta_features = np.zeros((len(X), len(self.trained_base_models)))
        
        for i, model in enumerate(self.trained_base_models):
            meta_features[:, i] = model.predict(X)
        
        # 2. ä½¿ç”¨å…ƒæ¨¡å‹é¢„æµ‹
        predictions = self.meta_model.predict(meta_features)
        
        return predictions
    
    def predict_proba(self, X):
        """
        é¢„æµ‹æ¦‚ç‡ï¼ˆå¦‚æœå…ƒæ¨¡å‹æ”¯æŒï¼‰
        
        Args:
            X: æµ‹è¯•ç‰¹å¾
            
        Returns:
            é¢„æµ‹æ¦‚ç‡
        """
        if not hasattr(self.meta_model, 'predict_proba'):
            raise ValueError("å…ƒæ¨¡å‹ä¸æ”¯æŒæ¦‚ç‡é¢„æµ‹")
        
        # 1. ä½¿ç”¨åŸºç¡€æ¨¡å‹é¢„æµ‹
        meta_features = np.zeros((len(X), len(self.trained_base_models)))
        
        for i, model in enumerate(self.trained_base_models):
            if hasattr(model, 'predict_proba'):
                meta_features[:, i] = model.predict_proba(X)[:, 1]
            else:
                meta_features[:, i] = model.predict(X)
        
        # 2. ä½¿ç”¨å…ƒæ¨¡å‹é¢„æµ‹æ¦‚ç‡
        proba = self.meta_model.predict_proba(meta_features)
        
        return proba
```

### Stackingä½¿ç”¨ç¤ºä¾‹

```python
import lightgbm as lgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR

# åˆ›å»ºåŸºç¡€æ¨¡å‹
base_models = [
    RandomForestRegressor(n_estimators=100, random_state=42),
    SVR(kernel='rbf'),
]

# åˆ›å»ºå…ƒæ¨¡å‹
meta_model = LinearRegression()

# åˆ›å»ºStackingé›†æˆ
stacking = StackingEnsemble(
    base_models=base_models,
    meta_model=meta_model
)

# è®­ç»ƒ
# stacking.fit(X_train, y_train)

# é¢„æµ‹
# predictions = stacking.predict(X_test)

print("âœ… Stackingé›†æˆç¤ºä¾‹")
```

---

## ğŸ¯ å®æˆ˜ï¼šé›†æˆç­–ç•¥é€‰æ‹©

### é›†æˆæ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ | å¤æ‚åº¦ |
|------|---------|------|------|--------|
| ç¡¬æŠ•ç¥¨ | åˆ†ç±»é—®é¢˜ | ç®€å•å¿«é€Ÿ | å¿½ç•¥æ¦‚ç‡ä¿¡æ¯ | â­ |
| è½¯æŠ•ç¥¨ | åˆ†ç±»é—®é¢˜ï¼ˆæœ‰æ¦‚ç‡ï¼‰ | åˆ©ç”¨æ¦‚ç‡ä¿¡æ¯ | éœ€è¦æ¦‚ç‡è¾“å‡º | â­â­ |
| ç®€å•å¹³å‡ | å›å½’é—®é¢˜ | ç®€å•ç¨³å®š | æœªè€ƒè™‘æ¨¡å‹å·®å¼‚ | â­ |
| åŠ æƒå¹³å‡ | å›å½’é—®é¢˜ | åŒºåˆ†æ¨¡å‹é‡è¦æ€§ | éœ€è¦ç¡®å®šæƒé‡ | â­â­ |
| Stacking | å¤æ‚é—®é¢˜ | å­¦ä¹ æœ€ä¼˜ç»„åˆ | è®¡ç®—æˆæœ¬é«˜ | â­â­â­â­ |

### é›†æˆç­–ç•¥ç®¡ç†å™¨

```python
class EnsembleManager:
    """é›†æˆç­–ç•¥ç®¡ç†å™¨"""
    
    def __init__(self, models, method='weighted_average'):
        """
        åˆå§‹åŒ–
        
        Args:
            models: æ¨¡å‹åˆ—è¡¨
            method: é›†æˆæ–¹æ³•
        """
        self.models = models
        self.method = method
        self.weights = None
        
    def fit(self, X_train, y_train, validation_data=None):
        """
        è®­ç»ƒï¼ˆæŸäº›æ–¹æ³•éœ€è¦è®­ç»ƒï¼‰
        """
        print(f"ğŸ”® è®­ç»ƒé›†æˆç­–ç•¥ ({self.method})...")
        
        # è®­ç»ƒæ‰€æœ‰åŸºç¡€æ¨¡å‹
        for i, model in enumerate(self.models):
            print(f"  è®­ç»ƒæ¨¡å‹ {i+1}/{len(self.models)}...")
            model.fit(X_train, y_train)
        
        # å¦‚æœæ˜¯åŠ æƒå¹³å‡ï¼Œæ ¹æ®éªŒè¯é›†è®¡ç®—æƒé‡
        if self.method == 'weighted_average' and validation_data is not None:
            X_val, y_val = validation_data
            
            # è®¡ç®—æ¯ä¸ªæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„IC
            ics = []
            for model in self.models:
                y_pred = model.predict(X_val)
                ic = np.corrcoef(y_pred, y_val)[0, 1]
                ics.append(ic)
            
            # è®¡ç®—æƒé‡
            abs_ics = [abs(ic) for ic in ics]
            self.weights = np.array(abs_ics) / sum(abs_ics)
            
            print(f"\n  æ¨¡å‹ICå€¼: {[f'{ic:.3f}' for ic in ics]}")
            print(f"  é›†æˆæƒé‡: {[f'{w:.2%}' for w in self.weights]}")
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
        
    def predict(self, X):
        """
        é¢„æµ‹
        """
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        predictions = [model.predict(X) for model in self.models]
        
        # æ ¹æ®æ–¹æ³•é›†æˆ
        if self.method == 'simple_average':
            return np.mean(predictions, axis=0)
        
        elif self.method == 'weighted_average':
            if self.weights is None:
                raise ValueError("éœ€è¦å…ˆè®­ç»ƒç¡®å®šæƒé‡")
            return np.average(predictions, axis=0, weights=self.weights)
        
        else:
            raise ValueError(f"æœªçŸ¥çš„é›†æˆæ–¹æ³•: {self.method}")
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ¨¡å‹å¤šæ ·æ€§

```python
# âœ… å¥½çš„é›†æˆ - æ¨¡å‹å¤šæ ·æ€§é«˜
good_ensemble = [
    "å†³ç­–æ ‘",      # éçº¿æ€§
    "çº¿æ€§å›å½’",    # çº¿æ€§
    "SVM",         # æ ¸æ–¹æ³•
    "ç¥ç»ç½‘ç»œ",    # æ·±åº¦å­¦ä¹ 
    "LightGBM"     # æ¢¯åº¦æå‡
]

# âŒ åçš„é›†æˆ - æ¨¡å‹ç›¸ä¼¼åº¦é«˜
bad_ensemble = [
    "LightGBM v1",
    "LightGBM v2",
    "LightGBM v3",
    "LightGBM v4",
    "LightGBM v5"
]
```

### 2. æ•°æ®å¤šæ ·æ€§

```python
# ä½¿ç”¨ä¸åŒçš„æ•°æ®å­é›†è®­ç»ƒ
def bagging_models(X, y, n_models=5):
    """Baggingæ–¹æ³•"""
    models = []
    
    for i in range(n_models):
        # æœ‰æ”¾å›é‡‡æ ·
        idx = np.random.choice(len(X), size=len(X), replace=True)
        X_sample, y_sample = X.iloc[idx], y.iloc[idx]
        
        # è®­ç»ƒæ¨¡å‹
        model = train_model(X_sample, y_sample)
        models.append(model)
    
    return models
```

### 3. ç‰¹å¾å¤šæ ·æ€§

```python
# ä½¿ç”¨ä¸åŒçš„ç‰¹å¾å­é›†
def feature_subsets(models, features):
    """ç‰¹å¾å­é›†é›†æˆ"""
    results = []
    
    for model, feature_set in zip(models, features):
        # ä½¿ç”¨ç‰¹å®šç‰¹å¾è®­ç»ƒ
        X_subset = X[feature_set]
        model.fit(X_subset, y)
        results.append(model)
    
    return results
```

### 4. é¿å…è¿‡æ‹Ÿåˆ

```python
# ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°é›†æˆæ€§èƒ½
def evaluate_ensemble(ensemble, X, y, cv=5):
    """è¯„ä¼°é›†æˆ"""
    from sklearn.model_selection import cross_val_score
    
    scores = cross_val_score(ensemble, X, y, cv=cv)
    
    print(f"ğŸ“Š é›†æˆæ€§èƒ½:")
    print(f"  å¹³å‡IC: {scores.mean():.3f}")
    print(f"  æ ‡å‡†å·®: {scores.std():.3f}")
    
    return scores
```

---

## ğŸ“ æœ¬èŠ‚å°ç»“

**æ ¸å¿ƒè¦ç‚¹**ï¼š

1. âœ… é›†æˆå­¦ä¹ èƒ½æå‡æ¨¡å‹çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§
2. âœ… æŠ•ç¥¨é›†æˆé€‚ç”¨äºåˆ†ç±»é—®é¢˜
3. âœ… å¹³å‡é›†æˆé€‚ç”¨äºå›å½’é—®é¢˜
4. âœ… Stackingæ˜¯æ›´é«˜çº§çš„é›†æˆæ–¹æ³•
5. âœ… æ¨¡å‹å¤šæ ·æ€§æ˜¯é›†æˆæˆåŠŸçš„å…³é”®

**é›†æˆä¸‰è¦ç´ **ï¼š

```python
ensemble_success = {
    "è¦ç´ 1": "æ¨¡å‹å¤šæ ·æ€§",  # ä¸åŒç±»å‹çš„æ¨¡å‹
    "è¦ç´ 2": "æ•°æ®å¤šæ ·æ€§",  # ä¸åŒçš„æ•°æ®å­é›†
    "è¦ç´ 3": "ç‰¹å¾å¤šæ ·æ€§"   # ä¸åŒçš„ç‰¹å¾ç»„åˆ
}
```

**ä¸‹ä¸€æ­¥**ï¼šé›†æˆç­–ç•¥å­¦ä¼šäº†ï¼Œå¦‚ä½•**è®¾è®¡æ•´ä¸ªç³»ç»Ÿæ¶æ„**å‘¢ï¼Ÿ

[â†’ å‰å¾€ 05-ç³»ç»Ÿæ¶æ„è®¾è®¡](05-ç³»ç»Ÿæ¶æ„è®¾è®¡.md)

---

**ğŸ”® é›†æˆå­¦ä¹ æ˜¯æå‡æ•ˆæœçš„ç¥å™¨ï¼Œä¸‹èŠ‚è¯¾æˆ‘ä»¬å°†è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„ç³»ç»Ÿæ¶æ„ï¼**
