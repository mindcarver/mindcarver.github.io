{"Week5_LSTM/README":{"slug":"Week5_LSTM/README","filePath":"Week5_LSTM/README.md","title":"README","links":["01_基础理论系列/README","02_PyTorch框架系列/README","03_LSTM模型构建系列/README","04_时序数据处理系列/README","05_模型训练优化系列/README","06_实战应用系列/README"],"tags":[],"content":"Week 5 LSTM深度学习模型 - 知识点体系\n📚 文档说明\n\n文档版本: v1.0\n创建日期: 2025-01-09\n学习主题: LSTM深度学习模型\n适用对象: Qlib量化投资学习者\n建议学习时间: 5-6小时（4-5天）\n\n\n📖 文档体系\n本文档采用分系列组织方式，将Week 5的知识点分为6个系列，每个系列聚焦一个核心主题。\n系列列表\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n系列说明文档路径1. 基础理论系列深度学习基础、RNN与LSTM原理README.md2. PyTorch框架系列Tensor、Autograd、nn.Module、常用层README.md3. LSTM模型构建系列单层、多层、双向LSTM架构README.md4. 时序数据处理系列滑动窗口、标准化、DatasetREADME.md5. 模型训练优化系列损失函数、优化器、训练循环README.md6. 实战应用系列完整流程、调优、评估README.md\n\n🎯 学习路径\n推荐学习顺序\nStep 1: 基础理论系列\n   ↓\nStep 2: PyTorch框架系列\n   ↓\nStep 3: LSTM模型构建系列\n   ↓\nStep 4: 时序数据处理系列\n   ↓\nStep 5: 模型训练优化系列\n   ↓\nStep 6: 实战应用系列\n\n每日学习计划\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n天数学习内容预计时间Day 1基础理论系列 + PyTorch框架系列1.5-2小时Day 2LSTM模型构建系列1小时Day 3时序数据处理系列1小时Day 4模型训练优化系列1-1.5小时Day 5实战应用系列1.5小时\n\n📊 知识点概览\n1. 基础理论系列\n核心知识点:\n\n✅ 深度学习基础（神经元、激活函数、神经网络）\n✅ 序列数据特点\n✅ RNN原理与局限性（梯度消失）\n✅ LSTM原理（细胞状态、门机制）\n✅ LSTM vs RNN vs GRU对比\n\n关键概念:\n\n梯度消失问题\n细胞状态\n遗忘门、输入门、输出门\n长期依赖\n\n2. PyTorch框架系列\n核心知识点:\n\n✅ Tensor创建与操作\n✅ Autograd自动微分\n✅ nn.Module模型定义\n✅ 常用层（LSTM、Linear、Dropout）\n\n关键概念:\n\nrequires_grad\nbackward()\n计算图\nGPU加速\n\n3. LSTM模型构建系列\n核心知识点:\n\n✅ 单层LSTM\n✅ 多层LSTM\n✅ 双向LSTM\n✅ LSTM变体（堆叠、编码器-解码器、注意力）\n✅ 超参数选择\n\n关键概念:\n\nhidden_size\nnum_layers\ndropout\nbatch_first\nbidirectional\n\n4. 时序数据处理系列\n核心知识点:\n\n✅ 滑动窗口方法\n✅ 时间序列划分\n✅ 特征标准化（Z-score、Min-Max、RobustScaler）\n✅ PyTorch Dataset\n✅ DataLoader\n\n关键概念:\n\n序列长度（seq_len）\n训练/验证/测试集\n数据泄漏\n批处理\n\n5. 模型训练优化系列\n核心知识点:\n\n✅ 损失函数（MSE、MAE、Smooth L1）\n✅ 优化器（SGD、Adam、RMSprop）\n✅ 训练循环\n✅ 早停策略\n✅ 正则化（L1/L2、Dropout、BatchNorm）\n✅ 学习率调度\n\n关键概念:\n\n前向传播\n反向传播\n梯度裁剪\n过拟合\n学习率衰减\n\n6. 实战应用系列\n核心知识点:\n\n✅ 完整预测流程\n✅ 超参数调优（网格搜索、随机搜索）\n✅ 模型保存与加载\n✅ 评估指标（MSE、MAE、IC、ICIR）\n✅ LSTM vs LightGBM对比\n✅ 最佳实践\n\n关键概念:\n\n端到端流程\n模型持久化\n泛化能力\n样本外验证\n鲁棒性\n\n\n💡 学习建议\n1. 理论学习\n\n先理解RNN的局限性\n再学习LSTM的创新点\n掌握门机制的作用\n理解细胞状态的意义\n\n2. 实践练习\n\n从简单的单层LSTM开始\n逐步增加复杂度\n使用真实数据训练\n对比不同架构的性能\n\n3. 代码实现\n\n跟着文档实现代码\n理解每一步的作用\n尝试修改参数\n观察效果变化\n\n4. 问题解决\n\n遇到问题时先查看文档\n利用PyTorch官方文档\n搜索Stack Overflow\n在社区提问\n\n\n🔧 技能检查清单\n基础理论\n\n 理解深度学习基本概念\n 掌握序列数据特点\n 理解RNN的工作原理\n 知道RNN的局限性\n 掌握LSTM的架构\n 理解门机制的作用\n 知道LSTM vs GRU的区别\n\nPyTorch框架\n\n 能够创建和操作Tensor\n 理解自动微分原理\n 能够定义自定义模型\n 掌握常用层的使用\n 能够使用GPU加速\n\n模型构建\n\n 能够定义单层LSTM\n 能够定义多层LSTM\n 理解双向LSTM\n 知道如何选择超参数\n 了解LSTM变体\n\n数据处理\n\n 能够实现滑动窗口\n 能够划分时序数据\n 掌握特征标准化\n 能够创建Dataset\n 能够使用DataLoader\n\n训练优化\n\n 能够选择合适的损失函数\n 能够选择合适的优化器\n 能够实现完整的训练循环\n 能够使用早停策略\n 理解正则化方法\n 能够调整学习率\n\n实战应用\n\n 能够完成完整的预测流程\n 能够进行超参数调优\n 能够保存和加载模型\n 能够计算评估指标\n 理解LSTM vs LightGBM\n 掌握最佳实践\n\n\n📚 扩展阅读\n推荐书籍\n\n《深度学习》（Ian Goodfellow等）\n《动手学深度学习》（Dive into Deep Learning）\n《Python深度学习》（François Chollet）\n\n在线课程\n\nCoursera: Deep Learning Specialization\nFast.ai: Practical Deep Learning for Coders\nUdacity: Deep Learning Nanodegree\n\n论文\n\nHochreiter &amp; Schmidhuber (1997). Long Short-Term Memory\nCho et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder\n\n官方文档\n\nPyTorch: pytorch.org/docs/\nPyTorch Tutorials: pytorch.org/tutorials/\n\n\n🎓 学习成果\n完成本系列学习后，您将能够：\n理论层面\n\n✅ 深入理解LSTM的工作原理\n✅ 掌握时序数据的处理方法\n✅ 理解深度学习的核心概念\n\n实践层面\n\n✅ 使用PyTorch构建LSTM模型\n✅ 训练和优化LSTM模型\n✅ 评估和对比模型性能\n\n应用层面\n\n✅ 将LSTM应用于量化投资\n✅ 进行股票价格预测\n✅ 提取时序特征\n\n\n📞 支持与反馈\n如有问题或建议，请通过以下方式联系：\n\nGitHub Issues: github.com/anomalyco/opencode/issues\n学习交流群: [待添加]\n\n\n祝学习顺利！🎓"},"blockchain/index":{"slug":"blockchain/index","filePath":"blockchain/index.md","title":"index","links":[],"tags":[],"content":"区块链技术\n\n探索去中心化世界，从智能合约到DeFi应用\n\n\n🚧 建设中\n区块链技术相关内容正在整理中，敬请期待！\n\n📚 即将上线\n智能合约开发\n\nSolidity语言基础\n智能合约安全\nGas优化技巧\n常见漏洞与防范\n\nDeFi应用\n\n去中心化交易所\n借贷协议\n稳定币\n收益聚合器\n\nWeb3开发\n\n前端与区块链交互\n钱包集成\nDApp开发实践\n用户身份认证\n\nNFT市场\n\nNFT标准（ERC-721, ERC-1155）\nNFT铸造与交易\nNFT市场开发\n元数据管理\n\n\n🛠️ 技术栈\n\nSolidity: 智能合约开发语言\nEthers.js: 以太坊JavaScript库\nHardhat: 以太坊开发环境\nWeb3.js: Web3 JavaScript API\nReact: 前端框架\nNext.js: React框架\n\n\n📖 推荐资源\n📚 书籍\n\n《精通比特币》\n《以太坊技术详解与实战》\n《智能合约开发实战》\n《DeFi与未来金融》\n《区块链技术指南》\n\n🎓 在线课程\n\nCryptoZombies: Solidity游戏开发\nSolidity官方文档\nEthereum.org开发者文档\nLearnWeb3 DAO\n\n🌐 网站\n\nEthereum.org\nOpenZeppelin Docs\nEthers.js Docs\nHardhat Docs\n\n\n📝 写作计划\n2025年计划\n基础篇\n\n 区块链基础原理\n 智能合约入门\n Solidity语言详解\n 开发环境搭建\n\n进阶篇\n\n 智能合约安全\n Gas优化技巧\n 链下数据获取（Oracles）\n 多签钱包\n\n应用篇\n\n ERC-20代币开发\n NFT开发实战\n 去中心化交易所\n 借贷协议实现\n\nDApp篇\n\n Web3前端开发\n 钱包集成（MetaMask）\n 钱包连接与管理\n 交易签名与发送\n\n\n💡 学习建议\n循序渐进\n从基础概念开始，逐步深入到实际应用。\n动手实践\n多写代码，多部署合约，在实践中学习。\n关注安全\n智能合约一旦部署很难修改，安全第一。\n参与社区\n加入开发者社区，与其他开发者交流学习。\n保持更新\n区块链技术发展迅速，持续学习新技术。\n\n🤝 交流与合作\n如有问题或建议，欢迎通过以下方式联系：\n\n📧 Email: [待添加]\n🐦 Twitter: [待添加]\n💻 GitHub: [待添加]\n💬 微信: [待添加]\n\n\n🔗 相关链接\n\nEthereum.org\nSolidity文档\nOpenZeppelin\nEthers.js\n\n\n\n  🚧 内容建设中\n  区块链技术相关内容正在整理中，敬请期待！\n  预计上线时间：2025年Q1\n\n\n\n  持续更新中，欢迎收藏和分享！\n"},"index":{"slug":"index","filePath":"index.md","title":"欢迎","links":["quant/qlib/week1/","quant/qlib/week2/","quant/qlib/week3/","quant/qlib/week5/","blockchain/","quant/qlib/week5/01_基础理论系列/","quant/qlib/week5/02_PyTorch框架系列/","quant/qlib/week5/03_LSTM模型构建系列/","quant/qlib/week5/04_时序数据处理系列/","quant/qlib/week5/05_模型训练优化系列/","quant/qlib/week5/06_实战应用系列/","quant/qlib/"],"tags":[],"content":"欢迎来到我的技术博客\n\n探索技术、分享知识、记录成长\n\n\n👋 关于我\n我是一名热爱技术的开发者，专注于以下领域：\n\n量化投资：使用机器学习和深度学习进行量化分析和交易策略研究\n区块链技术：智能合约开发、去中心化应用（DApp）构建\n深度学习：PyTorch、TensorFlow等框架的实践与应用\n\n\n📚 博客内容\n量化投资\n探索量化世界的奥秘，从特征工程到深度学习模型应用。\n\n  \n    \n      📊 特征工程\n      Qlib特征工程的系统讲解\n      进入模块 →\n    \n    \n      ⚡ LightGBM\n      机器学习在量化中的应用\n      进入模块 →\n    \n    \n      📊 回测引擎\n      策略回测与绩效评估\n      进入模块 →\n    \n     \n       🧠 LSTM深度学习\n       时序数据的深度学习方法\n       进入模块 →\n     \n   \n \nQlib 学习路径\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n周主题文档状态Week 1特征工程查看文档✅ 完成Week 2LightGBM查看文档✅ 完成Week 3回测系统查看文档✅ 完成Week 4强化学习敬请期待🚧 计划中Week 5LSTM深度学习查看文档✅ 完成\n\n区块链技术\n\n  \n    🔗 区块链技术\n    智能合约开发、DeFi应用、Web3开发\n    查看文档\n  \n\n\n🚧 建设中 - 区块链相关内容正在整理中，敬请期待！\n\n\n🔥 最新更新\nWeek 5 - LSTM深度学习模型 ✨\n深入学习LSTM神经网络，掌握时序数据的深度学习方法。\n\n  \n    \n      📚 基础理论\n      RNN与LSTM原理\n      查看 →\n    \n    \n      🔧 PyTorch框架\n      Tensor与模型构建\n      查看 →\n    \n    \n      🏗️ 模型构建\n      架构与实现\n      查看 →\n    \n    \n      💾 数据处理\n      预处理与Dataset\n      查看 →\n    \n    \n      ⚙️ 训练优化\n      训练与优化策略\n      查看 →\n    \n    \n      🎯 实战应用\n      案例与最佳实践\n      查看 →\n    \n  \n\n\n🛠️ 技术栈\n量化投资\n\n  Python\n  Qlib\n  PyTorch\n  LightGBM\n  Pandas\n  NumPy\n\n区块链\n\n  Solidity\n  Ethers.js\n  Hardhat\n  Web3.js\n  React\n\n\n📖 推荐资源\n📚 书籍\n量化投资\n\n《量化投资：策略与技术》\n《Python金融大数据分析》\n《统计套利》\n\n深度学习\n\n《深度学习》（Ian Goodfellow）\n《动手学深度学习》\n《Python深度学习》（François Chollet）\n\n区块链\n\n《精通比特币》\n《以太坊技术详解与实战》\n《智能合约开发实战》\n\n🎓 在线课程\n\nCoursera: Deep Learning Specialization\nFast.ai: Practical Deep Learning for Coders\nedX: Blockchain Basics\n\n\n📝 写作计划\n2025年计划\n量化投资\n\n Week 1 - 特征工程\n Week 2 - LightGBM模型\n Week 3 - 回测系统\n Week 4 - 强化学习\n Week 5 - LSTM深度学习\n Week 6 - Transformer模型\n 高级回测技巧\n 多因子模型\n\n区块链\n\n 区块链基础\n 智能合约开发\n DeFi项目实战\n NFT市场开发\n DAO治理机制\n\n\n🤝 交流与合作\n欢迎与我交流技术问题、分享学习心得！\n\n📧 Email: [待添加]\n🐦 Twitter: [待添加]\n💻 GitHub: [待添加]\n💬 微信: [待添加]\n\n\n\n  💡 开始学习\n  从量化投资到区块链，探索技术的无限可能\n  \n    量化学习\n    区块链\n  \n\n\n\n  持续更新中，欢迎收藏和分享！\n  Made with ❤️ by [你的名字]\n"},"quant/qlib/index":{"slug":"quant/qlib/index","filePath":"quant/qlib/index.md","title":"index","links":["week1/01-qlib特征工程全景概览","week1/02-horizon对齐详解","week1/03-横截面标准化与中性化","week1/04-相对强弱预测的量化思维","week1/05-qlib特征工程实践指南","week2/01-Gradient-Boosting原理","week2/02-时序数据划分","week2/03-模型训练","week2/04-IC-Rank-IC评估指标","week2/05-特征重要性分析","week2/06-学习检查清单","week3/01-交易策略理论","week3/02-投资组合构建方法","week3/03-Executor与成本模型","week3/04-绩效评估指标","week3/05-实验分析方法","week3/06-回测流程与实践","week3/07-学习检查清单","week5/01_基础理论系列/","week5/02_PyTorch框架系列/","week5/03_LSTM模型构建系列/","week5/04_时序数据处理系列/","week5/05_模型训练优化系列/","week5/06_实战应用系列/","week1/"],"tags":[],"content":"Qlib 量化投资学习路径\n\n系统学习量化投资，从特征工程到深度学习模型\n\n\n📚 学习概览\n本模块提供系统化的Qlib量化投资学习路径，涵盖从基础特征工程到高级深度学习模型的完整知识体系。\n\n🎯 学习路径\n推荐学习顺序\nWeek 1: 特征工程基础\n    ↓\nWeek 2: LightGBM模型\n    ↓\nWeek 3: 回测系统\n    ↓\nWeek 5: LSTM深度学习\n\n\n📖 课程内容\nWeek 1 - 特征工程 📊\n系统讲解Qlib特征工程的核心概念与实践方法。\n文档列表:\n\n01-qlib特征工程全景概览\n02-horizon对齐详解\n03-横截面标准化与中性化\n04-相对强弱预测的量化思维\n05-qlib特征工程实践指南\n\n学习目标:\n\n✅ 理解Qlib特征工程的核心概念\n✅ 掌握horizon对齐方法\n✅ 学会横截面标准化与中性化\n✅ 培养相对强弱预测的量化思维\n✅ 能够独立完成特征工程实践\n\n预计时间: 5-6小时（4-5天）\n\nWeek 2 - LightGBM ⚡\n深入学习LightGBM在量化投资中的应用。\n文档列表:\n\n01-Gradient Boosting原理 - GOSS、EFB、Leaf-wise三大创新\n02-时序数据划分 - 因果性约束、交叉验证、滚动窗口\n03-模型训练 - IC优化、在线学习、分布式训练\n04-IC-Rank-IC评估指标 - 统计检验、时序分析、IR指标\n05-特征重要性分析 - Permutation、SHAP、稳定性分析\n06-学习检查清单 - 学习目标与实践建议\n\n学习目标:\n\n✅ 理解Gradient Boosting原理\n✅ 掌握时序数据划分方法\n✅ 学会模型训练与优化\n✅ 能够使用IC/Rank-IC评估模型\n✅ 掌握特征重要性分析方法\n\n预计时间: 6-8小时（5-7天）\n\nWeek 3 - 回测系统 📈\n完整讲解策略回测、投资组合构建、绩效评估。\n文档列表:\n\n01-交易策略理论 - Top-K、IC权重、MV优化\n02-投资组合构建方法 - 三种组合构建方法对比\n03-Executor与成本模型 - 交易成本与执行机制\n04-绩效评估指标 - 收益、风险、绩效指标\n05-实验分析方法 - 参数敏感性、样本外验证\n06-回测流程与实践 - Qlib回测框架\n07-学习检查清单 - 学习目标与实践建议\n\n学习目标:\n\n✅ 理解交易策略理论\n✅ 掌握投资组合构建方法\n✅ 了解交易成本模型\n✅ 能够计算绩效评估指标\n✅ 学会实验分析方法\n✅ 能够完成完整的回测流程\n\n预计时间: 7-9小时（6-8天）\n\nWeek 5 - LSTM深度学习 🧠\n深入学习LSTM神经网络，掌握时序数据的深度学习方法。\n文档系列:\n1. 基础理论系列\n\n基础理论系列\n\n深度学习基础、RNN原理、LSTM原理\nLSTM vs RNN vs GRU对比\n\n\n\n2. PyTorch框架系列\n\nPyTorch框架系列\n\nTensor操作、Autograd、nn.Module\n常用层（LSTM、Linear、Dropout）\n\n\n\n3. LSTM模型构建系列\n\nLSTM模型构建系列\n\n单层、多层、双向LSTM\nLSTM变体与超参数选择\n\n\n\n4. 时序数据处理系列\n\n时序数据处理系列\n\n滑动窗口、数据划分、特征标准化\nPyTorch Dataset与DataLoader\n\n\n\n5. 模型训练优化系列\n\n模型训练优化系列\n\n损失函数、优化器、训练循环\n早停策略、正则化、学习率调度\n\n\n\n6. 实战应用系列\n\n实战应用系列\n\n完整预测流程、超参数调优\n模型保存加载、评估指标\nLSTM vs LightGBM对比、最佳实践\n\n\n\n学习目标:\n\n✅ 理解深度学习和LSTM原理\n✅ 掌握PyTorch框架\n✅ 能够构建LSTM模型\n✅ 学会时序数据处理\n✅ 掌握模型训练与优化\n✅ 能够完成完整的实战应用\n\n预计时间: 8-10小时（5-6天）\n\n🛠️ 技术栈\n\nPython: 主要编程语言\nQlib: 量化投资平台\nPyTorch: 深度学习框架\nLightGBM: 梯度提升树\nPandas/NumPy: 数据处理\nMatplotlib: 数据可视化\n\n\n💡 学习建议\n循序渐进\n按照推荐的学习顺序逐步学习，不要跳过基础概念。\n动手实践\n每个模块都包含代码示例，建议动手运行和修改。\n理解原理\n不仅要会用，还要理解背后的原理。\n多维度思考\n从不同角度理解问题，如风险、收益、成本等。\n持续优化\n量化投资是一个持续优化的过程，不要满足于一次性结果。\n\n📚 扩展阅读\n推荐书籍\n\n《量化投资：策略与技术》\n《Python金融大数据分析》\n《统计套利》\n《深度学习》（Ian Goodfellow）\n《动手学深度学习》\n\n在线课程\n\nCoursera: Deep Learning Specialization\nFast.ai: Practical Deep Learning for Coders\nedX: Machine Learning\n\n\n🤝 交流与反馈\n如有问题或建议，欢迎通过以下方式联系：\n\n📧 Email: [待添加]\n💻 GitHub: [待添加]\n💬 微信: [待添加]\n\n\n\n  开始学习之旅\n  从特征工程开始，系统学习量化投资\n  开始学习 Week 1 →\n\n\n\n  持续更新中，欢迎收藏和分享！\n"},"quant/qlib/week1/01-qlib特征工程全景概览":{"slug":"quant/qlib/week1/01-qlib特征工程全景概览","filePath":"quant/qlib/week1/01-qlib特征工程全景概览.md","title":"01-qlib特征工程全景概览","links":[],"tags":[],"content":"Qlib特征工程全景概览\n1. 引言：量化特征工程的特殊性\n1.1 金融时序数据的三大挑战\n在量化投资领域，特征工程面临着传统机器学习领域所未见的三大核心挑战，这些挑战直接决定了Qlib的设计哲学和技术路线。\n挑战一：非平稳性（Non-stationarity）\n金融时间序列的统计特性随时间漂移，这是量化特征工程最根本的难题。设时间序列 X_t，其分布 P(X_t) 在不同时间窗口 \\Delta t 上不满足平稳性条件：\nP(X_t) \\neq P(X_{t+\\Delta t})\n这意味着：\n\n历史有效的因子在未来可能失效\n训练集和测试集分布存在差异\n模型需要持续更新和适应\n\n举例来说，动量因子在牛市中IC可能达到0.08，但在熊市中可能降至0.02甚至变为负值。这种分布漂移要求特征工程系统具备：\n\n时间窗口敏感性：自动检测因子衰减周期\n动态因子选择：根据市场状态调整因子权重\n回测一致性：确保历史表现具有外推性\n\n挑战二：自相关性（Autocorrelation）\n金融价格序列存在显著的自相关结构，这对特征工程提出了双重约束：\n\\rho_k = \\frac{E[(X_t - \\mu)(X_{t+k} - \\mu)]}{\\sigma^2}\n其中 \\rho_k 是滞后 k 阶的自相关系数。\n自相关性的影响：\n\n特征独立性假设失效：传统ML假设样本独立，但金融数据样本在时间上强相关\n交叉验证方法受限：不能使用随机K-Fold，必须采用时间序列交叉验证\n信息泄露风险增加：不小心引入未来函数会导致模型表现虚高\n\n挑战三：信噪比低（Low Signal-to-Noise Ratio）\n金融数据的信噪比极低，估计在 10^{-3} 到 10^{-2} 量级。这意味着：\n\\frac{Var(\\text{Signal})}{Var(\\text{Noise})} \\approx 10^{-3}\n低信噪比的后果：\n\n特征工程必须极其谨慎，每一步操作都可能放大噪声\n过拟合风险极高，需要严格的样本外验证\n因子组合需要通过大数定律分散风险\n\n1.2 传统ML方法在量化中的失效案例\n让我们通过一个典型失效案例，理解传统ML特征工程在量化中的陷阱。\n案例：PCA在量化中的数据泄漏\n假设你对50个技术指标进行PCA降维，使用sklearn标准流程：\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=5)\nfactors_pca = pca.fit_transform(factor_matrix)  # fit使用全部数据\n这个操作存在两个致命问题：\n问题1：未来信息泄露\nPCA在计算主成分时，使用了整个时间窗口的数据。对于时刻 t 的因子值，PCA的计算实际上包含了 t+1, t+2, \\dots, t+T 的信息：\nX_{\\text{PCA}}(t) = f(X_1, X_2, \\dots, X_t, X_{t+1}, \\dots, X_T)\n这违反了因果性约束，导致回测结果虚高。\n问题2：时序平稳性假设\nPCA假设数据的协方差矩阵在整个时间窗口内稳定：\n\\Sigma = E[X X^T] \\approx \\frac{1}{T}\\sum_{t=1}^T X_t X_t^T\n但在金融时序中，\\Sigma 随时间剧烈变化，导致历史训练的主成分在未来失效。\nQlib的解决方案\nQlib通过以下机制避免了上述问题：\n\n表达式系统强制因果性：所有算子只能使用历史数据\n增量计算：Rolling操作避免跨时间窗口的信息泄露\n分步训练：每个时间窗口独立计算统计量，不使用未来信息\n\n1.3 Qlib的诞生背景与设计哲学\n历史背景\nQlib由微软亚洲研究院于2020年开源，旨在解决量化投资中特征工程的标准化和可复现性问题。在此之前，量化团队各自实现特征计算，导致：\n\n因子定义不统一，难以复现\n回测结果缺乏一致性\n因子研究效率低下\n\n核心设计哲学\nQlib的设计基于三个核心原则：\n原则1：可审计的计算图（Auditable Computation Graph）\n特征工程不是黑盒，而是显式的DAG（有向无环图）：\n\\Phi(X_t) = \\text{Compose}(Op_N \\circ \\cdots \\circ Op_1)(X_t)\n其中每个操作 Op_i 的语义清晰、可追溯、可审计。\n原则2：因果性前置约束（Causality-First Constraint）\n在系统层面强制因果性，而不是依赖开发者自律：\n\\forall t, \\text{Factor}(t) = f(X_{&lt;t})\n所有算子在定义时就必须遵守”只能使用历史数据”的约束。\n原则3：回测一致性优先（Backtest-Consistency Priority）\n宁可牺牲部分灵活性，也要保证回测结果的可信度：\n\n限制某些看似强大但可能导致泄漏的操作\n提供严格的时序验证机制\n输出详细的因子构建日志\n\n\n2. Qlib特征工程的核心定义\n2.1 数学定义：Factor Tensor Transformation\nQlib特征工程的核心是一个从原始数据到因子张量的映射函数 \\Phi：\n\\Phi: \\mathcal{D}_{\\text{raw}} \\to \\mathbb{R}^{[T, N, F]}\n其中：\n\n\\mathcal{D}_{\\text{raw}}：原始数据空间，包含行情、行为等基础字段\nT：时间维度（Time）\nN：资产维度（Number of instruments）\nF：因子维度（Factors）\n\n原始数据的形式化定义\n设原始数据为三维张量 \\mathcal{D}_{\\text{raw}} \\in \\mathbb{R}^{[T, N, V]}，其中 V 是原始字段数（如open, high, low, close, volume）：\n\\mathcal{D}_{\\text{raw}}[t, i, v] = \\text{value}_{i, v}^{(t)}\n表示资产 i 在时刻 t 的字段 v 的值。\n特征工程的目标\nQlib的目标是将 \\mathcal{D}_{\\text{raw}} 转换为可被模型学习的因子张量 \\mathcal{F}：\n\\mathcal{F}[t, i, f] = \\Phi_f(\\mathcal{D}_{\\text{raw}}[:, i, :])[t]\n其中：\n\nf：第 f 个因子\n\\Phi_f：第 f 个因子的计算表达式\n\n2.2 Pipeline形式化表示\nQlib的特征工程Pipeline可以形式化为复合函数：\n\\mathcal{F} = \\mathcal{N} \\circ \\mathcal{C} \\circ \\mathcal{T} \\circ \\mathcal{E}(\\mathcal{D}_{\\text{raw}})\n其中四个层次的操作分别为：\n1. 表达式层（Expression Layer）\\mathcal{E}\n将原始字段通过算子组合成基础因子：\n\\mathcal{E}(\\mathcal{D}_{\\text{raw}}) = \\{ \\text{Ref}(\\mathcal{D}_{\\text{raw}}[:, :, \\text{close}], 1), \\text{Mean}(\\mathcal{D}_{\\text{raw}}[:, :, \\text{close}], 20), \\dots \\}\n2. 时间结构层（Temporal Layer）\\mathcal{T}\n应用滚动、滞后、衰减等时间操作：\n\\mathcal{T}(X) = \\{ \\text{RollingMean}(X, 20), \\text{Lag}(X, 5), \\dots \\}\n3. 横截面层（Cross-Sectional Layer）\\mathcal{C}\n在同一时刻对多个资产进行标准化、排序、中性化：\n\\mathcal{C}(X)[t, i, :] = \\text{Standardize}(X[t, :, :])[i]\n标准化定义为：\nz_{i,t} = \\frac{x_{i,t} - \\mu_t}{\\sigma_t}\n其中 \\mu_t = \\frac{1}{N}\\sum_{i=1}^N x_{i,t} 是横截面均值，\\sigma_t 是标准差。\n4. 标准化层（Normalization Layer）\\mathcal{N}\n最终的标准化处理：\n\\mathcal{N}(X) = \\text{Scale}(X)\nPipeline的图示\n原始数据 [T, N, V]\n    ↓ Expression层\n中间因子 [T, N, K]\n    ↓ Temporal层\n时间特征 [T, N, K]\n    ↓ Cross-section层\n横截面特征 [T, N, K]\n    ↓ Normalization层\n最终因子 [T, N, F]\n\n2.3 与sklearn特征工程的对比表格\nQlib特征工程与传统机器学习（sklearn）特征工程在多个维度上存在根本差异：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n维度sklearn特征工程Qlib特征工程数据结构[N, F] 二维矩阵[T, N, F] 三维张量时序假设样本独立（IID）样本时序相关时间处理手动shift，易出错内置Ref/Lag，语义清晰因果性保证无保证，依赖开发者系统强制约束特征计算静态预处理动态计算图横截面操作需手动实现内置Z-score/Rank/Neutralize可回测性低，易引入未来函数高，时序验证机制可解释性中等强，每步可追溯适用场景图像、文本等静态数据金融时序、推荐系统等序列数据\n关键差异解析\n差异1：数据结构\nsklearn处理的是静态特征矩阵，假设样本独立：\nX \\in \\mathbb{R}^{N \\times F}\n其中 N 是样本数，F 是特征数。\nQlib处理的是动态特征张量，包含时间维度：\n\\mathcal{F} \\in \\mathbb{R}^{T \\times N \\times F}\n这个差异导致了两个系统在设计哲学上的根本不同：\n\nsklearn适合静态分类/回归任务\nQlib适合时序预测任务\n\n差异2：因果性保证\nsklearn中的StandardScaler没有时序概念：\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)  # 使用全部数据计算μ和σ\n这会导致数据泄漏：在计算 t 时刻的标准化值时，使用了 t 之后的数据。\nQlib中的标准化是时序感知的：\n# Qlib伪代码\nz[t, i] = (x[t, i] - μ[t]) / σ[t]\n# μ[t]只使用t时刻的N个资产计算，不包含t+1, t+2...\n差异3：可回测性\nsklearn特征工程的回测结果往往不可信，因为：\n\n容易引入Look-ahead Bias\n没有时序验证机制\n缺乏详细的因子构建日志\n\nQlib通过以下机制保证回测一致性：\n\n强制因果性：所有算子只能使用历史数据\n增量计算：Rolling操作避免跨时间窗口的信息泄露\n审计日志：记录每个因子的计算过程\n\n\n3. 四层架构深度解析\nQlib特征工程的四层架构是其设计的核心，每一层都有明确的职责和约束。让我们从底层向上逐层解析。\n3.1 Layer 1：原始字段层（Raw Feature Layer）\n定义\n原始字段层是最基础的数据层，直接来自数据源的原始字段。这些字段通常来自：\n\n行情数据：open, high, low, close, volume, amount\n财务数据：market_cap, pe_ratio, pb_ratio, roe, $debt_ratio\n衍生数据：turnover_rate, amplitude, $change_pct\n\n数据特性\n原始字段具有以下特性：\n\n低信噪比：单字段预测能力极弱\n多源异构：不同字段量纲、频率、覆盖率不同\n存在缺失：某些资产在某些时刻可能缺失某些字段\n包含噪声：数据清洗和去噪是预处理重点\n\n代码示例\n# 原始字段示例\n$close          # 收盘价\n$volume         # 成交量\n$market_cap     # 市值\n$turnover_rate  # 换手率\n使用建议\n\n直接使用原始字段作为特征效果很差\n需要通过Layer 2的表达式层进行组合和变换\n缺失值处理需要在原始字段层完成\n\n\n3.2 Layer 2：表达式层（Expression Layer）⭐\n定义\n表达式层是Qlib的灵魂，通过算子组合将原始字段转换为基础因子。表达式可以看作是一个有向无环图（DAG），每个节点是一个算子，边是数据流。\n核心思想\n\\text{Factor} = \\text{Expression}(\\text{Raw Fields})\n表达式 = 算子 + 原始字段 + 参数\n语法示例\n# 基础算子\nRef($close, 1)           # 昨收盘（滞后1期）\nMean($close, 20)         # 20日均线\nStd($close, 20)          # 20日标准差\nMax($high, 5)            # 5日最高价\nMin($low, 5)             # 5日最低价\n \n# 组合算子\n($close - Mean($close, 20)) / Std($close, 20)  # BOLL（布林带）\nMean($close, 5) / Mean($close, 20) - 1        # MA乖离率\n($close - Ref($close, 1)) / Ref($close, 1)     # 日收益率\n实用表达式库\n以下提供10+个常用的Qlib表达式，涵盖技术分析的主要类别：\n技术指标类\n\n\n移动平均线（MA）\nma_5 = Mean($close, 5)\nma_20 = Mean($close, 20)\nma_60 = Mean($close, 60)\n\n\n指数移动平均（EMA）\nema_12 = EMA($close, 12)\nema_26 = EMA($close, 26)\n\n\n相对强弱指数（RSI）\n# RSI = 100 - (100 / (1 + RS))\n# RS = 平均涨幅 / 平均跌幅\nrsi_14 = RSI($close, 14)\n\n\n布林带（BOLL）\nma_20 = Mean($close, 20)\nstd_20 = Std($close, 20)\nboll_upper = ma_20 + 2 * std_20\nboll_lower = ma_20 - 2 * std_20\nboll_width = (boll_upper - boll_lower) / ma_20\n\n\n动量类\n\n\n变动率（ROC）\nroc_5 = ($close / Ref($close, 5)) - 1\nroc_20 = ($close / Ref($close, 20)) - 1\n\n\n动量因子\nmomentum_10 = $close - Ref($close, 10)\nmomentum_20 = $close - Ref($close, 20)\n\n\n波动类\n\n\n平均真实波幅（ATR）\n# ATR = MA(True Range, N)\n# True Range = max(high-low, abs(high-ref(close,1)), abs(low-ref(close,1)))\natr_14 = ATR($high, $low, $close, 14)\n\n\n历史波动率（HV）\n# HV = Std(收益率, N)\nreturns = ($close / Ref($close, 1)) - 1\nhv_20 = Std(returns, 20)\n\n\n成交量类\n\n\n能量潮（OBV）\n# OBV = Σ sign(close变化) * volume\nobv = OBV($close, $volume)\n\n\n量价关系\nvol_price_ratio = $volume / $close\nvol_ma_5 = Mean($volume, 5)\nvol_ratio = $volume / vol_ma_5\n\n\n复合因子\n\n\nMACD\ndif = ema_12 - ema_26\ndea = EMA(dif, 9)\nmacd = (dif - dea) * 2\n\n\nKDJ\n# K值、D值、J值的计算\nrsv = ($close - Min($low, 9)) / (Max($high, 9) - Min($low, 9))\nk = SMA(rsv, 3)\nd = SMA(k, 3)\nj = 3 * k - 2 * d\n\n\n特点与约束\nQlib表达式层有以下特点：\n特点1：可组合性（Composability）\n表达式可以任意组合，形成复杂的计算图：\n\\Phi = \\text{Op}_N \\circ \\cdots \\circ \\text{Op}_2 \\circ \\text{Op}_1\n例如：\nfactor = Std(\n    ($close - Mean($close, 20)) / Std($close, 20),\n    10\n)\n特点2：语义清晰（Semantic Clarity）\n每个算子都有明确的金融语义，不是黑盒：\n\nMean($close, 20)：20日均线（平滑价格）\nRef($close, 1)：滞后1期（避免未来函数）\nStd($close, 20)：20日波动率（风险度量）\n\n特点3：强制因果性（Causality Constraint）\n所有算子只能使用历史数据，这是系统级约束：\n\\forall \\text{Op}, \\text{Op}(X_t) = f(X_{&lt;t})\n例如，Ref($close, 1) 是正确的，因为使用的是 t-1 时刻的数据。\n错误的写法：Ref($close, -1)，因为使用了 t+1 时刻的数据。\n特点4：天然支持Rolling/Lag\n表达式系统天然支持滚动窗口和滞后操作：\nRollingMean($close, 20)  # 20日滚动均值\nLag($close, 5)            # 滞后5期\n这些操作在计算时会自动处理时间对齐，避免数据泄漏。\n\n3.3 Layer 3：时间结构层（Temporal Feature Engineering）\n定义\n时间结构层关注如何在时间维度上提取特征，这是金融时序数据的本质难点。核心问题是：\n\n模型学到的是”时间结构”还是”噪声”？\n\n常见操作\nQlib在时间结构层提供了以下操作：\n1. Lag（滞后）\n将历史数据取到当前时刻：\n\\text{Lag}(X, k)[t] = X[t-k]\n应用场景：\n\n获取历史价格：Lag($close, 1) 获取昨收盘\n构建时滞特征：Lag($volume, 5) 获取5日前的成交量\n避免未来函数：使用滞后数据计算当前指标\n\n2. Rolling（滚动窗口）\n在滚动时间窗口上计算统计量：\n\\text{RollingMean}(X, w)[t] = \\frac{1}{w} \\sum_{i=0}^{w-1} X[t-i]\n常见Rolling算子：\n\nRollingMean(X, w)：滚动均值\nRollingStd(X, w)：滚动标准差\nRollingMax(X, w)：滚动最大值\nRollingMin(X, w)：滚动最小值\nRollingCorr(X, Y, w)：滚动相关系数\nRollingRegression(X, Y, w)：滚动回归\n\n应用场景：\n\n技术指标计算：RollingMean($close, 20) 是20日均线\n波动率度量：RollingStd($return, 20) 是20日波动率\n动量识别：RollingCorr($close, $market, 20) 衡量与市场相关性\n\n3. Decay（衰减）\n对历史数据应用指数衰减，赋予近期数据更高权重：\n\\text{Decay}(X, \\alpha)[t] = \\frac{\\sum_{i=0}^{\\infty} \\alpha^i X[t-i]}{\\sum_{i=0}^{\\infty} \\alpha^i} = \\frac{\\sum_{i=0}^{\\infty} \\alpha^i X[t-i]}{1/(1-\\alpha)}\n其中 \\alpha \\in (0, 1) 是衰减因子。\n应用场景：\n\nEMA（指数移动平均）：EMA($close, 20) 本质是衰减均值\n动量衰减：Decay($volume, 0.95) 赋予近期成交量更高权重\n信号平滑：衰减操作比简单Moving Average更平滑\n\n4. Horizon对齐（Label Shift）\n将未来的Label对齐到当前时刻，这是量化特征工程的核心操作：\n\\text{Label}[t] = Y_{t \\to t+h} = \\frac{P_{t+h}}{P_t} - 1\n详细原理将在文档2中展开。\n时间因果关系\nQlib强制你把”时间因果关系”写进特征定义，而不是交给模型猜。\n错误示例（交给模型猜）：\n# 直接把原始价格扔给模型，让模型自己学时间关系\nfeature = $close\nmodel.fit(feature, label)\n正确示例（显式时间结构）：\n# 显式定义时间结构：20日均线、5日动量\nfeature1 = Mean($close, 20)       # 20日均线\nfeature2 = ($close / Ref($close, 5)) - 1  # 5日动量\nmodel.fit([feature1, feature2], label)\n在链上高频/套利中的重要性\n在链上高频交易和套利中，时间结构工程尤为重要：\n场景1：MEV套利\n链上交易存在时序依赖关系：\n\n用户提交交易 → 矿工看到交易 → 矿工插入MEV交易\n\n特征工程需要显式建模这个时序：\n# 链上时间结构特征\nmempool_latency = block_timestamp - tx_timestamp  # 内存池等待时间\ngas_price_momentum = (gas_price - Ref(gas_price, 1)) / Ref(gas_price, 1)\n场景2：套利机会识别\nDEX间套利需要识别价格时序模式：\n# DEX A和B的价格时序关系\nprice_diff_momentum = (diff_A_t - diff_A_{t-1}) / diff_A_{t-1}\n这些时间结构特征必须显式定义，而不是让模型从原始价格序列中自动学习。\n\n3.4 Layer 4：横截面层（Cross-sectional Engineering）\n定义\n横截面层关注同一时刻不同资产之间的关系。这是很多非量化出身的人最容易忽略的层次，但却是量化投资的核心差异所在。\n核心思想\n\n你不是在预测价格，而是在预测”同一时刻资产之间的相对强弱”。\n\n常见操作\nQlib在横截面层提供了以下操作：\n1. Z-score标准化（去量纲化）\n将同一时刻的所有资产值标准化到标准正态分布：\nz_{i,t} = \\frac{x_{i,t} - \\mu_t}{\\sigma_t}\n其中：\n\n\\mu_t = \\frac{1}{N}\\sum_{j=1}^N x_{j,t} 是 t 时刻的横截面均值\n\\sigma_t = \\sqrt{\\frac{1}{N}\\sum_{j=1}^N (x_{j,t} - \\mu_t)^2} 是标准差\n\n应用场景\n将不同单位的因子拉到同一量纲：\n\nPE（倍数）和ROE（百分比）无法直接相加\n标准化后，两者都变成”偏离均值的标准差倍数”\n可以线性组合：factor = 0.6 * z_pe + 0.4 * z_roe\n\n示例\n假设 t 时刻有3只股票的PE值：\n\n股票A：PE = 30\n股票B：PE = 20\n股票C：PE = 10\n\n计算：\n\n\\mu = (30 + 20 + 10) / 3 = 20\n\\sigma = \\sqrt{((30-20)^2 + (20-20)^2 + (10-20)^2) / 3} = 8.16\nz_A = (30-20)/8.16 = 1.22\nz_B = (20-20)/8.16 = 0\nz_C = (10-20)/8.16 = -1.22\n\n2. Rank（排序标准化）\n将因子值换成排序位置，再缩放到 [0, 1]：\n\\text{Rank}(x_i) = \\frac{\\text{rank}(x_i) - 1}{N - 1}\n其中 \\text{rank}(x_i) 是 x_i 在 \\{x_1, x_2, \\dots, x_N\\} 中的排序位置（从1开始）。\n应用场景\n彻底杀掉异常值：\n\n某股票的PE可能是10000倍（数据噪声）\n在Rank眼里，它仅仅是第一名\n只在乎”谁比谁强”，不在乎强多少\n\n示例\n假设3只股票的PE值：[30, 20, 10000]\nZ-score标准化：\n\n\\mu = 3350\n\\sigma = 5756\nz = [-0.58, -0.58, 1.16]\n10000的股票对z-score影响很大\n\nRank标准化：\n\n排序：[2, 1, 3]\n标准化：[0.5, 0, 1]\n10000的股票只是第一名，对其他股票无影响\n\n3. Neutralization（中性化/去相关）\n做回归，取残差，剔除行业、市值等”作弊因素”：\nx_i = \\beta_0 + \\sum_{j=1}^K \\beta_j \\cdot \\text{feature}_{i,j} + \\varepsilon_i\n残差 \\varepsilon_i 是中性化后的因子，表示”剔除风格因子后的纯Alpha”。\n应用场景\n行业中性化：\n\n如果一个股票涨是因为整个白酒板块都在涨，这不叫你的因子厉害\n中性化后，剩下的才是这只股票超出所属行业的”纯粹特质”\n\n市值中性化：\n\n小盘股弹性大，天然涨得多\n中性化后，剔除市值效应\n比较的是”同等市值下的相对强弱”\n\n示例\n假设有3只股票：\n\n股票A：行业=白酒，市值=100亿，因子值=10\n股票B：行业=白酒，市值=100亿，因子值=8\n股票C：行业=科技，市值=50亿，因子值=9\n\n如果不中性化：\n\n股票A表现最好（因子值=10）\n\n行业中性化后（剔除行业效应）：\n\n在白酒行业内部，A比B强（10 &gt; 8）\n残差：A=+1, B=-1, C=0\n\n市值中性化后（再剔除市值效应）：\n\nC是小盘股（50亿），有市值溢价\n中性化后，C的因子值会下降\n\n横截面处理的意义\n通过横截面标准化和中性化，你的思维模型发生了进化：\n\n承认无知：承认自己无法准确预测宏观经济和大盘波动（Beta）\n寻找秩序：相信即便在乱世或盛世，资产之间总有”好坏之分”\n纯化信号：把那些”搭便车”的收益（行业、市值、大盘涨跌）全部扔掉，只捕捉那一点点代表公司真正竞争力的纯Alpha\n\n在DeFi世界中的类比\n在DeFi世界中，横截面处理同样重要：\n同一区块里：哪个池子更”异常”\n\n比较同一区块内所有DEX池子的交易量、价格变化\n找出显著偏离平均水平的池子\n\n同一时间窗内：哪个Token行为偏离更多\n\n比较同一小时内所有Token的价格波动、成交量变化\n找出表现异常的Token\n\n横截面不是数学技巧，而是一种投资策略：\n\n我们不赌国运涨跌，我们只赌”优胜劣汰”。\n\n\n4. 解决的三个核心问题\nQlib特征工程的设计旨在解决量化投资中的三个核心问题。让我们逐一深入分析。\n4.1 问题一：如何定义因子（表达式层）\n问题描述\n在量化投资中，“因子”是一个高度抽象的概念。传统做法是：\n\n用Python/Pandas手动计算因子\n因子定义散落在不同脚本中\n难以复现和验证\n\n这种做法存在三个问题：\n\n可读性差：复杂的因子计算逻辑难以理解\n可维护性差：修改因子需要改动多处代码\n可复现性差：不同人对因子的理解可能不一致\n\nQlib的解决方案\nQlib引入表达式系统，将因子定义标准化：\n核心思想\n\\text{Factor} = \\text{Expression} = \\text{Composition of Operators}\n因子 = 表达式 = 算子的组合\n表达式系统的优势\n优势1：声明式定义\n# 声明式：清晰表达意图\nfactor = ($close - Mean($close, 20)) / Std($close, 20)\n \n# vs 命令式：实现细节混乱\ndef compute_factor(close):\n    ma20 = close.rolling(20).mean()\n    std20 = close.rolling(20).std()\n    return (close - ma20) / std20\n优势2：可组合性\n表达式可以任意组合，形成复杂的因子：\n# 基础因子\nma20 = Mean($close, 20)\nvol20 = Std($close, 20)\nboll = ($close - ma20) / vol20\n \n# 组合因子\nmomentum_boll = boll * (($close / Ref($close, 5)) - 1)\n优势3：自动优化\n表达式系统可以自动优化计算：\n\n公共子表达式消除\n增量计算（避免重复计算Rolling）\n并行化执行\n\n算子语义约束\nQlib的算子有严格的语义约束，确保：\n约束1：因果性（Causality）\n所有算子只能使用历史数据：\n\\forall \\text{Op}, \\text{Op}(X_t) = f(X_{&lt;t})\n例如：\n\nRef($close, 1) ✓ 使用 t-1 时刻数据\nRef($close, -1) ✗ 使用 t+1 时刻数据（未来函数）\n\n约束2：时间一致性（Temporal Consistency）\n算子的输出时间戳与输入一致：\n\\text{Timestamp}(\\text{Op}(X_t)) = \\text{Timestamp}(X_t)\n例如：\n\nMean($close, 20) 输出的是 t 时刻的均值（使用 t-19 到 t 的数据）\n输出时间戳仍然是 t，不是 t-10 或 t+1\n\n约束3：可审计性（Auditability）\n每个算子的计算过程可以追溯：\n\n输入数据：明确指定\n计算过程：语义清晰\n输出结果：可验证\n\n\n4.2 问题二：如何对齐时间 &amp; 横截面（金融时序的本质难点）\n问题描述\n金融时序数据的对齐是特征工程中最本质、最困难的问题，包含两个维度：\n维度1：时间对齐（Temporal Alignment）\n如何将”当前的因子”与”未来的收益”对齐？\n场景示例\n你有：\n\nt 时刻的因子：Factor[t] = 1.5\nt 到 t+5 时刻的收益：Return[t→t+5] = 5%\n\n问题是：如何把它们放在同一行数据中训练模型？\n错误对齐：\nRow_t: {Factor[t], Return[t]}  # Return[t]是t时刻收益，已实现\n这会导致Look-ahead Bias，因为Return[t]在t时刻还未发生。\n正确对齐：\nRow_t: {Factor[t], Return[t→t+5]}  # Return[t→t+5]是未来收益\n维度2：横截面对齐（Cross-Sectional Alignment）\n如何将不同时间频率、不同覆盖范围的数据对齐到同一时刻？\n场景示例\n\n行情数据：每日（频率=1天，覆盖=全市场5000只股票）\n财务数据：季度（频率=1季度，覆盖=部分股票）\n行业数据：月度（频率=1月，覆盖=全市场）\n\n如何把它们对齐到 t 时刻？\nQlib的解决方案\n时间对齐：Horizon Shift\nQlib使用Label Shift操作将未来的Label对齐到当前时刻：\n\\text{Label}_t = Y_{t \\to t+h} = \\frac{P_{t+h}}{P_t} - 1\n详细原理见文档2。\n横截面对齐：统一时间网格\nQlib将所有数据对齐到统一的时间网格：\ntime_grid = [t_1, t_2, ..., t_T]\n \n# 行情数据：已有，直接映射\nprice[t] = get_price(t)\n \n# 财务数据：前向填充\nfinancial[t] = latest_financial_data_before(t)\n \n# 行业数据：对齐到最近月份\nindustry[t] = industry_data(round_down_to_month(t))\n横截面对齐的挑战\n挑战1：频率不一致\n不同数据源的频率不同：\n\n行情数据：日频\n财务数据：季频\n宏观数据：月频\n\n解决方案：降采样/升采样\n\n高频 → 低频：取最后值、均值、聚合\n低频 → 高频：前向填充\n\n挑战2：覆盖范围不一致\n不同数据源的覆盖范围不同：\n\n行情数据：覆盖全市场\n财务数据：覆盖部分股票（停牌、退市等）\n行业数据：覆盖全市场\n\n解决方案：缺失值处理\n\n删除缺失值过多的时刻\n用行业均值填充\n用历史均值填充\n\n挑战3：时间偏差\n某些数据存在发布延迟：\n\n财报数据：公布日期晚于财务期间\n宏观数据：公布日期晚于统计期间\n\n解决方案：\n\n使用公布日期作为有效日期\n对于预测，只能使用公布前的数据\n\n\n4.3 问题三：如何避免数据泄漏 + 保证可回测性\n问题描述\n数据泄漏是量化回测中最致命的问题，会导致：\n\n回测收益虚高\n实盘表现惨淡\n策略完全失效\n\n常见的数据泄漏场景\n场景1：未来信息泄露\n在计算 t 时刻的因子时，不小心用到了 t+1, t+2, \\dots 时刻的数据。\n错误示例：\n# 计算t时刻的波动率，使用了未来数据\nvolatility[t] = std(price[t-10:t+10])  # 包含t+1到t+10的数据\n正确示例：\n# 只使用历史数据\nvolatility[t] = std(price[t-20:t])  # 只使用t-20到t的数据\n场景2：样本外信息混入\n在训练模型时，使用了测试集的信息。\n错误示例：\n# 在整个数据集上标准化\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)  # 使用了全部数据\n正确示例：\n# 只在训练集上标准化，然后应用到测试集\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n场景3：Look-ahead Bias\n在回测时，使用了未来才能知道的信息。\n错误示例：\n# 回测时，知道t时刻的收盘价，然后决定是否买入\nif price[t] &gt; ma[t]:\n    buy()  # 但实际上，t时刻你不知道price[t]\n正确示例：\n# 回测时，只能使用t-1时刻的信息\nif price[t-1] &gt; ma[t-1]:\n    buy()  # t-1时刻的收盘价，t时刻开盘前决定\nQlib的防护体系\nQlib通过多层防护机制避免数据泄漏：\n防护1：算子级约束\n所有算子强制只能使用历史数据：\n\\forall \\text{Op}, \\forall t, \\text{Op}(X_t) = f(X_{&lt;t})\n系统会在算子定义时检查，违反约束的算子无法注册。\n防护2：表达式级验证\n表达式系统会自动验证因果性：\n# 正确表达式\nfactor = Mean($close, 20)  # 只使用t-19到t的数据\n \n# 错误表达式（系统会拒绝）\nfactor = Mean(Ref($close, -10), 20)  # 包含未来数据\n防护3：时间戳追踪\nQlib会追踪每个因子的时间戳，确保时间对齐正确：\n# 每个因子都有时间戳\nfactor = Mean($close, 20)\n# factor的时间戳 = t（使用t-19到t的数据）\n \nlabel = Ref($close, -5) / $close - 1\n# label的时间戳 = t（使用t到t+5的数据）\n防护4：回测一致性检查\nQlib提供回测一致性检查工具，检测潜在的数据泄漏：\n# 检查因子是否与未来价格相关\ncheck_lookahead_bias(factor, price)\n \n# 检查IC是否异常高\ncheck_ic_validity(ic_series)\n防护5：审计日志\nQlib会记录每个因子的计算过程，便于审计：\n[2024-01-01] Factor: ma20\n  Input: $close\n  Operation: Mean($close, 20)\n  Time range: [2023-12-10, 2024-01-01]\n  Causality check: ✓ Passed\n\n回测一致性的定义\n回测一致性指：历史回测的表现能够外推到实盘。\n如何保证回测一致性\nQlib通过以下机制保证：\n机制1：严格时序划分\n严格划分训练集、验证集、测试集，确保时间顺序：\nTrain: [t_1, t_2]\nValidate: [t_2, t_3]\nTest: [t_3, t_4]\n\n机制2：增量学习\n模型在每个时间点只使用历史数据学习，然后预测未来：\nfor t in time_grid:\n    # 使用t之前的数据训练\n    model.fit(X[:t], y[:t])\n \n    # 预测t之后的收益\n    pred[t] = model.predict(X[t:t+horizon])\n机制3：样本外验证\n严格使用样本外数据验证，绝不使用样本内数据调参：\n# 错误：使用全部数据调参\nbest_params = grid_search(X, y, param_grid)\n \n# 正确：只在训练集上调参\nbest_params = grid_search(X_train, y_train, param_grid)\n机制4：前瞻性预测\n预测未来的收益，而不是拟合过去的收益：\n# 正确：预测未来收益\npred = model.predict(current_features)\n \n# 错误：拟合过去收益\npred = model.predict(historical_features)  # 这是过拟合\n\n5. 适用场景与局限性\n5.1 适用场景清单\nQlib特征工程在以下场景中表现优异：\n场景1：多因子选股\n这是Qlib最核心的应用场景。多因子选股的核心思想是：\n\n构建多个因子（动量、价值、质量、波动等）\n通过因子组合选出表现最好的股票\n定期调仓（如每月、每季度）\n\nQlib的优势：\n\n表达式系统快速定义和测试因子\n横截面标准化和中性化自动处理风格因子\nIC/IR评估系统量化因子质量\n\n示例：\n# 定义5个因子\nfactors = {\n    &#039;momentum&#039;: ($close / Ref($close, 20)) - 1,\n    &#039;value&#039;: 1 / $pe_ratio,\n    &#039;quality&#039;: $roe / $debt_ratio,\n    &#039;volatility&#039;: Std($return, 20),\n    &#039;liquidity&#039;: $turnover_rate\n}\n \n# 横截面标准化\nfactors_z = zscore_cross_sectional(factors)\n \n# 中性化\nfactors_neu = neutralize(factors_z, industry, market_cap)\n \n# 组合\ncomposite_factor = 0.3 * factors_neu[&#039;momentum&#039;] + \\\n                   0.3 * factors_neu[&#039;value&#039;] + \\\n                   0.2 * factors_neu[&#039;quality&#039;] + \\\n                   0.1 * factors_neu[&#039;volatility&#039;] + \\\n                   0.1 * factors_neu[&#039;liquidity&#039;]\n \n# 选股（Top 10%）\nselected = rank(composite_factor) &gt; 0.9\n场景2：CTA策略因子构建\nCTA（Commodity Trading Advisor）策略主要基于趋势跟踪，Qlib的表达式系统非常适合构建趋势因子。\nQlib的优势：\n\n表达式系统方便定义趋势指标（MA、MACD、RSI等）\n时间结构层支持Rolling、Lag、Decay等操作\n可以快速测试不同参数的稳健性\n\n示例：\n# 趋势因子\ntrend_factor = ($close - Mean($close, 60)) / Mean($close, 60)\n \n# 动量因子\nmomentum_factor = ($close / Ref($close, 20)) - 1\n \n# 波动率因子\nvolatility_factor = Std($return, 20)\n \n# 组合\ncta_factor = 0.5 * trend_factor + 0.3 * momentum_factor - 0.2 * volatility_factor\n \n# 交易信号\nlong_signal = cta_factor &gt; threshold\nshort_signal = cta_factor &lt; -threshold\n场景3：因子库管理\n对于量化团队，管理数百个因子是一个挑战。Qlib提供了系统的因子库管理方案。\nQlib的优势：\n\n表达式系统统一因子定义\n因子评估系统（IC/IR/衰减周期）\n因子组合和去重\n因子监控和预警\n\n示例：\n# 因子库\nfactor_library = {\n    &#039;ma_cross&#039;: Mean($close, 5) / Mean($close, 20) - 1,\n    &#039;rsi&#039;: RSI($close, 14),\n    &#039;boll_width&#039;: (Mean($close, 20) + 2*Std($close, 20) - \\\n                   (Mean($close, 20) - 2*Std($close, 20))) / Mean($close, 20),\n    # ... 更多因子\n}\n \n# 批量评估\nfactor_performance = {}\nfor name, expr in factor_library.items():\n    ic_series = compute_ic(expr)\n    factor_performance[name] = {\n        &#039;ic_mean&#039;: ic_series.mean(),\n        &#039;ic_std&#039;: ic_series.std(),\n        &#039;ir&#039;: ic_series.mean() / ic_series.std()\n    }\n \n# 筛选优质因子\ngood_factors = [name for name, perf in factor_performance.items()\n                if perf[&#039;ir&#039;] &gt; 0.7]\n场景4：学术研究\nQlib也适合学术界进行量化研究，特别是因子挖掘和资产定价研究。\nQlib的优势：\n\n开源免费，可复现性强\n表达式系统灵活，便于实验\n内置常用因子库\n支持自定义算子\n\n场景5：链上数据分析\n随着DeFi和Web3的发展，链上数据量激增，Qlib的特征工程框架也可以用于链上数据分析。\nQlib的优势：\n\n表达式系统可以定义链上行为因子\n时间结构层支持高频数据分析\n横截面层可以比较不同Token/协议的表现\n\n详见文档5。\n\n5.2 局限性与替代方案\n局限性1：不适合纯深度学习端到端\nQlib的特征工程基于人工设计的因子，如果使用深度学习进行端到端学习，Qlib可能不是最优选择。\n原因：\n\n深度学习（如Transformer、LSTM）可以从原始序列中自动提取特征\nQlib的因子定义可能限制了模型的学习能力\n深度学习更适合高频、微观结构数据\n\n替代方案：\n\n使用PyTorch/TensorFlow搭建端到端深度学习模型\n使用专业的深度学习量化框架（如DeepQuant、FinRL）\n\n局限性2：高频微秒级策略\n对于微秒级的高频策略，Qlib的性能可能不够。\n原因：\n\nQlib基于表达式系统，计算开销较大\n没有针对实时流数据的优化\n横截面操作在N很大时性能瓶颈明显\n\n替代方案：\n\n使用C++编写的高频交易系统\n使用专业的实时数据流处理框架（如Flink、Kafka Streams）\n\n局限性3：复杂结构数据\nQlib主要处理时序数据，对于复杂结构数据（如图谱、文本、图像）支持有限。\n原因：\n\nQlib的数据结构是[时间, 资产, 因子]的三维张量\n不支持图谱、树、图等结构数据\n没有内置自然语言处理或计算机视觉模块\n\n替代方案：\n\n图谱数据：使用图神经网络（GNN）\n文本数据：使用NLP模型（如BERT、GPT）\n图像数据：使用CNN或Vision Transformer\n\n局限性4：实时流数据处理\nQlib的设计主要是批处理，对于实时流数据支持不足。\n原因：\n\nQlib的计算图是静态的，不支持动态更新\n没有内置流式处理机制\n横截面操作需要等待所有数据到达\n\n替代方案：\n\n使用流式处理框架（如Apache Flink、Apache Spark Streaming）\n使用专业的实时数据平台（如Kafka、Pulsar）\n\n局限性5：多市场、多资产类别\nQlib主要针对单一市场（如A股），对于跨市场、多资产类别的场景支持有限。\n原因：\n\n不同市场的交易时间、币种、规则不同\n多资产类别（股票、债券、期货、期权、加密货币）的数据结构差异大\n横截面操作需要处理不同资产类别的差异\n\n替代方案：\n\n使用专门的多资产管理系统（如Bloomberg、Wind）\n自研多市场数据平台\n\n\n总结\nQlib特征工程是一个专为量化投资设计的系统化框架，它通过四层架构解决了金融时序数据的三大核心挑战：非平稳性、自相关性和低信噪比。\n核心要点回顾：\n\n四层架构：原始字段层 → 表达式层 → 时间结构层 → 横截面层\nPipeline形式化：\\mathcal{F} = \\mathcal{N} \\circ \\mathcal{C} \\circ \\mathcal{T} \\circ \\mathcal{E}(\\mathcal{D}_{\\text{raw}})\n三大核心问题：因子定义、时间&amp;横截面对齐、数据泄漏防护\n适用场景：多因子选股、CTA策略、因子库管理、学术研究、链上数据分析\n局限性：不适合端到端深度学习、微秒级高频、复杂结构数据、实时流处理、多市场\n\nQlib的独特价值：\nQlib不是简单的特征工程库，而是一个可审计的计算图系统，它强制你在设计特征时就考虑因果性、时序对齐和回测一致性，从而避免量化研究中最致命的”未来函数”陷阱。\n在下一文档中，我们将深入探讨Qlib特征工程中最核心的操作：Horizon对齐（Label Shift），这是量化投资中”消除未来函数并建立因果预测关系”的关键技术。"},"quant/qlib/week1/02-horizon对齐详解":{"slug":"quant/qlib/week1/02-horizon对齐详解","filePath":"quant/qlib/week1/02-horizon对齐详解.md","title":"02-horizon对齐详解","links":[],"tags":[],"content":"Horizon对齐（Label Shift）详解\n引言\n在量化投资和机器学习建模中，“Horizon对齐”（Horizon Alignment，又称Label Shift）是一个最基础、最核心，却最容易被误解的概念。它解决的是量化投资中最根本的问题：\n“我现在的因子，如何对应未来的收益？”\n简单来说，Horizon对齐就是消除”未来函数”并建立因果预测关系。\n本文将从数学定义、详细示例、实现对比、不同Horizon的影响、常见错误陷阱等多个维度，全面解析Horizon对齐的原理与实践。\n\n1. 核心概念与数学定义\n1.1 Horizon的定义\n**Horizon（预测时长/视界）**是指从当前时刻 t 到未来时刻 t+h 的时间跨度，记为 h。\n常见的Horizon设置：\n\nh=1：预测未来1天（日频）\nh=5：预测未来5天（周频）\nh=20：预测未来20天（月频）\nh=60：预测未来60天（季频）\n\nHorizon的选择取决于：\n\n交易频率：高频交易 h 较小，低频投资 h 较大\n因子类型：动量因子 h 较小，价值因子 h 较大\n持仓周期：短线策略 h 较小，长线策略 h 较大\n\n1.2 收益率的数学定义\n离散收益率（Simple Return）\n从 t 时刻到 t+h 时刻的收益率定义为：\nR_{t \\to t+h} = \\frac{P_{t+h} - P_t}{P_t}\n其中：\n\nP_t：t 时刻的价格\nP_{t+h}：t+h 时刻的价格\nR_{t \\to t+h}：从 t 到 t+h 的收益率\n\n对数收益率（Log Return）\n对数收益率具有可加性，是量化研究中常用的形式：\nr_{t \\to t+h} = \\ln\\left(\\frac{P_{t+h}}{P_t}\\right) = \\ln(P_{t+h}) - \\ln(P_t)\n两者的关系\n当收益率较小时（|R| \\ll 1），对数收益率约等于离散收益率：\nr \\approx R\n推导（泰勒展开）：\n\\ln(1+R) = R - \\frac{R^2}{2} + \\frac{R^3}{3} - \\cdots \\approx R\n优势对比\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n收益率类型优势劣势离散收益率直观、易懂不可加（跨时间）对数收益率可加、对称性解释性稍差\n可加性示例\n假设有3天的价格：P_1=100, P_2=110, P_3=121\n离散收益率：\nR_{1 \\to 2} = (110-100)/100 = 10\\%\nR_{2 \\to 3} = (121-110)/110 = 10\\%\nR_{1 \\to 3} = (121-100)/100 = 21\\% \\neq R_{1 \\to 2} + R_{2 \\to 3}\n对数收益率：\nr_{1 \\to 2} = \\ln(110/100) = 0.0953\nr_{2 \\to 3} = \\ln(121/110) = 0.0953\nr_{1 \\to 3} = \\ln(121/100) = 0.1906 = r_{1 \\to 2} + r_{2 \\to 3} \\quad \\checkmark\n1.3 Label Shift的推导过程\n问题提出\n在量化回测或机器学习建模中：\n\n因子（Factor/Feature）：是我们在 t 时刻就能观察到的数据（如：t 时刻的收盘价、PE、成交量等）\n收益率（Label/Target）：是我们要预测的目标，通常是从 t 时刻到未来 t+h 时刻的涨跌幅\n\n问题是：如何将”当前的因子”与”未来的收益”对齐到同一行数据中？\n错误的对齐方式（Look-ahead Bias）\n\\text{Row}_t = \\{ \\underbrace{\\text{Factor}_t}_{\\text{t时刻可观测}}, \\underbrace{\\text{Return}_t}_{\\text{t时刻收益（已实现）}} \\}\n问题分析：\n\n\\text{Return}_t 是 t 时刻到 t-1 时刻的收益（已实现）\n在 t 时刻，我们不知道 \\text{Return}_t（要等到 t+1 时刻才知道）\n如果用 \\text{Return}_t 训练模型，模型会”看到未来”，这是Look-ahead Bias\n\n正确的对齐方式（Label Shift）\n\\text{Row}_t = \\{ \\underbrace{\\text{Factor}_t}_{\\text{t时刻可观测}}, \\underbrace{\\text{Return}_{t \\to t+h}}_{\\text{未来h期收益}} \\}\n其中：\n\\text{Return}_{t \\to t+h} = \\frac{P_{t+h}}{P_t} - 1\nLabel Shift的操作\nLabel Shift的本质是将 t+h 时刻才产生的收益率，“平移”到 t 时刻的因子行上：\n\\text{Label}_t = \\text{Return}_{t \\to t+h}\n时序因果约束\nHorizon对齐必须满足时序因果约束：\n\\text{Factor}_t \\leftarrow \\text{Label}_{t \\to t+h} \\quad \\checkmark\n\\text{Factor}_{t+1} \\leftarrow \\text{Label}_{t \\to t+h} \\quad \\times\n约束解释：\n\n在 t 时刻，我们可以观测到 \\text{Factor}_t\n在 t 时刻，我们预测的是 \\text{Label}_{t \\to t+h}（未来收益）\n不能在 t 时刻预测 \\text{Label}_{t \\to t+h}（这已经是过去的收益）\n\n1.4 Dataset形式化表示\n训练集构造\n整个训练集可以形式化为：\n\\mathcal{D} = \\{ (\\mathbf{X}_t, y_{t+h}) \\mid t = 1, 2, \\dots, T-h \\}\n其中：\n\n\\mathbf{X}_t \\in \\mathbb{R}^{N \\times F}：t 时刻 N 个资产的 F 个因子\ny_{t+h} \\in \\mathbb{R}^N：t+h 时刻 N 个资产的收益\nT：总时间长度\nN：资产数量\nF：因子数量\n\n维度解释\n时间维度（Time）：\n\n训练集时间范围：[t_1, t_{T-h}]\n标签时间范围：[t_{1+h}, t_T]\n注意：训练集最后 h 个时刻没有标签（因为未来数据不存在）\n\n资产维度（Number of instruments）：\n\n横截面：每个时刻有 N 个资产\n模型可以学习”同一时刻不同资产之间的关系”（横截面信息）\n\n因子维度（Factors）：\n\n每个资产有 F 个因子\n模型可以学习”同一资产不同因子之间的关系”（时序信息）\n\n矩阵形式\n特征矩阵 \\mathbf{X} \\in \\mathbb{R}^{(T-h) \\times N \\times F}：\n\\mathbf{X}_1 \\\\\n\\mathbf{X}_2 \\\\\n\\vdots \\\\\n\\mathbf{X}_{T-h}\n\\end{bmatrix} = \\begin{bmatrix}\n\\begin{bmatrix}x_{1,1,1} &amp; \\cdots &amp; x_{1,1,F} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{1,N,1} &amp; \\cdots &amp; x_{1,N,F}\\end{bmatrix} \\\\\n\\begin{bmatrix}x_{2,1,1} &amp; \\cdots &amp; x_{2,1,F} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{2,N,1} &amp; \\cdots &amp; x_{2,N,F}\\end{bmatrix} \\\\\n\\vdots \\\\\n\\begin{bmatrix}x_{T-h,1,1} &amp; \\cdots &amp; x_{T-h,1,F} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{T-h,N,1} &amp; \\cdots &amp; x_{T-h,N,F}\\end{bmatrix}\n\\end{bmatrix} $$\n\n**标签矩阵** $\\mathbf{Y} \\in \\mathbb{R}^{(T-h) \\times N}$：\n$$ \\mathbf{Y} = \\begin{bmatrix}\ny_{1+h} \\\\\ny_{2+h} \\\\\n\\vdots \\\\\ny_T\n\\end{bmatrix} = \\begin{bmatrix}\ny_{1+h,1} &amp; \\cdots &amp; y_{1+h,N} \\\\\ny_{2+h,1} &amp; \\cdots &amp; y_{2+h,N} \\\\\n\\vdots &amp; \\ddots &amp; \\vdots \\\\\ny_{T,1} &amp; \\cdots &amp; y_{T,N}\n\\end{bmatrix} $$\n\n**对齐关系**\n\n$$ \\mathbf{X}_t \\leftrightarrow y_{t+h} $$\n\n其中 $t = 1, 2, \\dots, T-h$。\n\n---\n\n## 2. 详细示例演示\n\n### 2.1 原始数据表\n\n假设我们有某只股票3天的价格数据：\n\n| 日期 (t) | 收盘价 (Price) | 因子值 (Factor) |\n|---------|---------------|-----------------|\n| 2023-01-01 | 100 | 1.5 |\n| 2023-01-02 | 105 | 1.6 |\n| 2023-01-03 | 103 | 1.4 |\n\n**解释**：\n- 01-01：价格100，因子1.5\n- 01-02：价格105，因子1.6\n- 01-03：价格103，因子1.4\n\n### 2.2 Step 1：计算收益率\n\n假设我们要预测未来1天的收益率（$h=1$）。\n\n**计算公式**：\n\n$$ R_{t \\to t+1} = \\frac{P_{t+1} - P_t}{P_t} $$\n\n**计算过程**：\n\n**01-01 到 01-02**：\n$$ R_{1 \\to 2} = \\frac{105 - 100}{100} = 5\\% $$\n\n**01-02 到 01-03**：\n$$ R_{2 \\to 3} = \\frac{103 - 105}{105} = -1.9\\% $$\n\n**收益率表**：\n\n| 日期 (t) | 收盘价 (Price) | 因子值 (Factor) | 收益率 (Return) |\n|---------|---------------|-----------------|-----------------|\n| 2023-01-01 | 100 | 1.5 | 5% |\n| 2023-01-02 | 105 | 1.6 | -1.9% |\n| 2023-01-03 | 103 | 1.4 | NaN |\n\n**注意**：01-03没有收益率，因为还没有01-04的数据。\n\n### 2.3 Step 2：Label Shift\n\n现在我们需要将未来的收益率对齐到当前时刻。\n\n**对齐规则**：\n\n$$ \\text{Label}_t = R_{t \\to t+1} $$\n\n**对齐过程**：\n\n**01-01**：\n- 因子：1.5\n- 对齐的标签：$R_{1 \\to 2} = 5\\%$（01-01预测01-02的收益）\n\n**01-02**：\n- 因子：1.6\n- 对齐的标签：$R_{2 \\to 3} = -1.9\\%$（01-02预测01-03的收益）\n\n**01-03**：\n- 因子：1.4\n- 对齐的标签：NaN（没有未来数据）\n\n**对齐后的数据表**：\n\n| 日期 (t) | 因子值 (Factor) | 标签 (Label) |\n|---------|-----------------|-------------|\n| 2023-01-01 | 1.5 | 5% |\n| 2023-01-02 | 1.6 | -1.9% |\n| 2023-01-03 | 1.4 | NaN |\n\n**解读**：\n\n- 01-01：用因子1.5预测未来1天的收益（5%）\n- 01-02：用因子1.6预测未来1天的收益（-1.9%）\n- 01-03：没有未来数据，无法预测（删除或填充）\n\n**Pandas代码实现**：\n\n```python\nimport pandas as pd\n\n# 原始数据\ndf = pd.DataFrame({\n    &#039;date&#039;: [&#039;2023-01-01&#039;, &#039;2023-01-02&#039;, &#039;2023-01-03&#039;],\n    &#039;price&#039;: [100, 105, 103],\n    &#039;factor&#039;: [1.5, 1.6, 1.4]\n})\n\n# 计算收益率\ndf[&#039;return&#039;] = df[&#039;price&#039;].pct_change(1)\n\n# Label Shift（向上移动h行）\nh = 1\ndf[&#039;label&#039;] = df[&#039;return&#039;].shift(-h)\n\n# 清理缺失值\ndf_clean = df.dropna(subset=[&#039;label&#039;])\n\nprint(df_clean[[&#039;date&#039;, &#039;factor&#039;, &#039;label&#039;]])\n```\n\n**输出**：\n\n```\n        date  factor  label\n0 2023-01-01     1.5   0.05\n1 2023-01-02     1.6  -0.019\n```\n\n### 2.4 不同Horizon对比\n\n**Horizon = 1（预测未来1天）**\n\n| 日期 (t) | 因子 | 标签（Label） |\n|---------|------|-------------|\n| 01-01 | 1.5 | 5% |\n| 01-02 | 1.6 | -1.9% |\n\n**解读**：\n- 用01-01的因子（1.5）预测01-01到01-02的收益（5%）\n- 用01-02的因子（1.6）预测01-02到01-03的收益（-1.9%）\n\n**Horizon = 5（预测未来5天）**\n\n假设我们有更长时间的数据：\n\n| 日期 | 价格 |\n|------|------|\n| 01-01 | 100 |\n| 01-02 | 105 |\n| 01-03 | 103 |\n| 01-04 | 107 |\n| 01-05 | 110 |\n| 01-06 | 115 |\n\n**计算收益率**：\n\n$$ R_{1 \\to 6} = \\frac{115 - 100}{100} = 15\\% $$\n$$ R_{2 \\to 7} = \\text{未知} $$\n$$ \\vdots $$\n\n**Label Shift**：\n\n| 日期 (t) | 因子 | 标签（Label） |\n|---------|------|-------------|\n| 01-01 | 1.5 | 15% |\n| 01-02 | 1.6 | NaN |\n\n**解读**：\n- 用01-01的因子（1.5）预测01-01到01-06的收益（15%）\n- 01-02没有未来5天的数据（如果数据只有到01-06）\n\n**Horizon = 20（预测未来20天）**\n\n类似地，计算20天的收益率，然后向上移动20行。\n\n**不同Horizon的影响**：\n\n| Horizon | 信号时效性 | IC均值 | 噪声水平 | 适用场景 |\n|---------|-----------|--------|---------|---------|\n| 1 | 极强 | 0.02-0.05 | 极高 | 高频交易 |\n| 5 | 强 | 0.05-0.08 | 高 | 短线策略 |\n| 20 | 中 | 0.08-0.12 | 中 | 中线策略 |\n| 60 | 弱 | 0.12-0.15 | 低 | 长线策略 |\n\n**趋势**：\n- Horizon越大，IC通常越高（因为长期趋势更清晰）\n- Horizon越大，信号时效性越差（因为未来信息太多）\n\n---\n\n## 3. 实现代码对比\n\n### 3.1 Pandas实现\n\n**核心代码**：\n\n```python\nimport pandas as pd\n\n# 计算收益率\ndf[&#039;return&#039;] = df[&#039;price&#039;].pct_change(h)\n\n# Label Shift（向上移动h行）\ndf[&#039;label&#039;] = df[&#039;return&#039;].shift(-h)\n\n# 清理缺失值\ndf = df.dropna(subset=[&#039;label&#039;])\n```\n\n**完整示例**：\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# 模拟数据\nnp.random.seed(42)\ndates = pd.date_range(&#039;2020-01-01&#039;, &#039;2023-12-31&#039;)\nn = len(dates)\nprices = 100 * np.cumprod(1 + np.random.randn(n) * 0.01)\nfactors = np.random.randn(n) * 0.1\n\ndf = pd.DataFrame({\n    &#039;date&#039;: dates,\n    &#039;price&#039;: prices,\n    &#039;factor&#039;: factors\n})\n\n# 设置Horizon\nh = 5\n\n# Step 1: 计算收益率\ndf[&#039;return&#039;] = df[&#039;price&#039;].pct_change(h)\n\n# Step 2: Label Shift\ndf[&#039;label&#039;] = df[&#039;return&#039;].shift(-h)\n\n# Step 3: 清理缺失值\ndf_clean = df.dropna(subset=[&#039;label&#039;])\n\n# 验证对齐\nprint(f&quot;原始数据量: {len(df)}&quot;)\nprint(f&quot;对齐后数据量: {len(df_clean)}&quot;)\nprint(f&quot;损失数据量: {len(df) - len(df_clean)}&quot;)\nprint(f&quot;数据损失率: {(len(df) - len(df_clean)) / len(df):.2%}&quot;)\n\n# 验证对齐正确性\nsample = df_clean.iloc[0]\nprint(f&quot;\\n示例验证:&quot;)\nprint(f&quot;日期: {sample[&#039;date&#039;]}&quot;)\nprint(f&quot;因子: {sample[&#039;factor&#039;]:.4f}&quot;)\nprint(f&quot;标签: {sample[&#039;label&#039;]:.4f}&quot;)\nprint(f&quot;验证: {sample[&#039;label&#039;]:.4f} ≈ {(df.loc[df[&#039;date&#039;] == sample[&#039;date&#039;] + pd.Timedelta(days=h), &#039;price&#039;].values[0] / sample[&#039;price&#039;] - 1):.4f}&quot;)\n```\n\n**输出示例**：\n\n```\n原始数据量: 1096\n对齐后数据量: 1091\n损失数据量: 5\n数据损失率: 0.46%\n\n示例验证:\n日期: 2020-01-01 00:00:00\n因子: 0.0046\n标签: 0.0492\n验证: 0.0492 ≈ 0.0492\n```\n\n### 3.2 Qlib实现\n\n**核心代码**：\n\n```python\nfrom qlib import Expression\nfrom qlib.data import D\n\n# 定义因子\nfactor_expr = Expression($close / Ref($close, 1) - 1)\n\n# 定义Label（Horizon对齐）\nlabel_expr = Expression(Ref($close, -h) / $close - 1)\n\n# 加载数据（自动对齐）\nfeatures = D.features([factor_expr], start_time, end_time)\nlabels = D.features([label_expr], start_time, end_time)\n```\n\n**完整示例**：\n\n```python\nfrom qlib import init\nfrom qlib.data import D\nfrom qlib.expr import Expression\nimport pandas as pd\n\n# 初始化Qlib\ninit(provider_uri=&quot;data/qlib/qlib_data/cn_data&quot;)\n\n# 定义时间范围\nstart_time = &quot;2020-01-01&quot;\nend_time = &quot;2023-12-31&quot;\n\n# 定义Horizon\nh = 5\n\n# 定义因子（示例：动量因子）\nfactor_expr = Expression(\n    ($close / Ref($close, 1)) - 1\n)\n\n# 定义Label（Horizon对齐）\nlabel_expr = Expression(\n    Ref($close, -h) / $close - 1\n)\n\n# 加载数据（Qlib自动对齐）\ninstruments = D.instruments(market=&quot;all&quot;)\nfeatures = D.features(\n    instruments,\n    [factor_expr],\n    start_time=start_time,\n    end_time=end_time\n)\nlabels = D.features(\n    instruments,\n    [label_expr],\n    start_time=start_time,\n    end_time=end_time\n)\n\n# 验证对齐\nprint(f&quot;特征矩阵形状: {features.shape}&quot;)  # [T, N, 1]\nprint(f&quot;标签矩阵形状: {labels.shape}&quot;)    # [T, N, 1]\n\n# 提取单只股票\nstock = &quot;000001.SZ&quot;\nfeature_series = features.xs(stock, level=&quot;instrument&quot;).iloc[:, 0]\nlabel_series = labels.xs(stock, level=&quot;instrument&quot;).iloc[:, 0]\n\n# 验证对齐正确性\nidx = feature_series.index[0]\nprint(f&quot;\\n示例验证:&quot;)\nprint(f&quot;股票: {stock}&quot;)\nprint(f&quot;日期: {idx}&quot;)\nprint(f&quot;因子: {feature_series[idx]:.4f}&quot;)\nprint(f&quot;标签: {label_series[idx]:.4f}&quot;)\n```\n\n**Qlib的优势**：\n- 自动处理时间对齐\n- 支持表达式系统\n- 高效的增量计算\n- 自动处理缺失值\n\n### 3.3 实现对比总结\n\n| 维度 | Pandas | Qlib |\n|------|--------|------|\n| **易用性** | 中等 | 高（表达式系统） |\n| **性能** | 低（逐列计算） | 高（增量计算） |\n| **灵活性** | 高（任意操作） | 中等（算子约束） |\n| **可维护性** | 低（代码分散） | 高（声明式） |\n| **可回测性** | 低（易泄漏） | 高（强制因果） |\n\n---\n\n## 4. 不同Horizon的影响分析\n\n### 4.1 短期（h=1）：高频噪声问题\n\n**优势**：\n- 信号时效性强：模型学到的是&quot;最近&quot;的信息\n- 适合高频交易：可以快速调整持仓\n- 交易机会多：每天都有新的预测\n\n**劣势**：\n- 日内噪声严重：短期价格波动主要由随机噪声驱动\n- IC波动大：因子表现不稳定，今天IC=0.05，明天IC=-0.02\n- 交易成本高：频繁交易导致交易成本侵蚀收益\n\n**IC分析**：\n\n假设因子IC均值=0.03，标准差=0.1：\n\n$$ \\text{IR} = \\frac{0.03}{0.1} = 0.3 $$\n\nIR &lt; 0.5，说明因子不稳定。\n\n**适用场景**：\n- 高频交易（分钟级、秒级）\n- 市场中性策略（对冲市场风险）\n- 流动性好的市场（如美股）\n\n**不适用场景**：\n- 流动性差的市场（如小盘股、新兴市场）\n- 交易成本高的策略（如期权、期货）\n- 长线投资\n\n### 4.2 中期（h=5）：信号稳定性\n\n**优势**：\n- 信号稳定性好：IC均值较高，波动较小\n- 噪声相对可控：5天内的价格波动有一定趋势\n- 适合量化选股：可以选出表现较好的股票\n\n**劣势**：\n- 周期性效应：可能受周效应、月效应影响\n- 持仓周期中等：需要定期调仓（如每周调仓）\n\n**IC分析**：\n\n假设因子IC均值=0.05，标准差=0.08：\n\n$$ \\text{IR} = \\frac{0.05}{0.08} = 0.625 $$\n\nIR &gt; 0.5，说明因子可用。\n\n**适用场景**：\n- 量化选股（A股多因子策略）\n- 市场中性对冲基金\n- 指数增强\n\n### 4.3 长期（h=20）：趋势主导\n\n**优势**：\n- 趋势信号清晰：20天的价格波动主要由基本面驱动\n- IC较高：长期来看，因子与收益相关性更强\n- 适合价值投资：低频交易，交易成本低\n\n**劣势**：\n- 对基本面变化反应慢：需要较长时间才能识别基本面变化\n- 持仓周期长：需要长时间持有，可能错过短期机会\n- 市场风格切换：如果市场风格切换，因子可能失效\n\n**IC分析**：\n\n假设因子IC均值=0.08，标准差=0.06：\n\n$$ \\text{IR} = \\frac{0.08}{0.06} = 1.33 $$\n\nIR &gt; 1.0，说明因子非常稳定。\n\n**适用场景**：\n- 价值投资（低PE、高ROE）\n- 基本面投资\n- 长线基金\n\n### 4.4 Horizon选择建议\n\n**根据因子类型选择**：\n\n| 因子类型 | 推荐Horizon | 原因 |\n|---------|-------------|------|\n| 动量因子 | 1-5天 | 动量效应短期存在 |\n| 均值回归 | 5-20天 | 均值回归需要一定时间 |\n| 价值因子 | 20-60天 | 价值发现需要时间 |\n| 质量因子 | 20-60天 | 质量效应长期存在 |\n\n**根据交易频率选择**：\n\n| 交易频率 | 推荐Horizon | 原因 |\n|---------|-------------|------|\n| 高频（分钟级） | 1-5天 | 需要快速调整 |\n| 中频（日频） | 5-20天 | 平衡时效性和稳定性 |\n| 低频（周频/月频） | 20-60天 | 长线持有 |\n\n**根据市场特征选择**：\n\n| 市场特征 | 推荐Horizon | 原因 |\n|---------|-------------|------|\n| 流动性好 | 1-5天 | 可以快速调仓 |\n| 流动性差 | 20-60天 | 降低交易成本 |\n| 有效性高 | 20-60天 | 长期趋势清晰 |\n| 有效性低 | 1-5天 | 捕捉短期机会 |\n\n---\n\n## 5. 常见错误与调试\n\n### 5.1 Look-ahead Bias典型案例\n\n**案例1：未来均值**\n\n**错误代码**：\n\n```python\n# 计算20日均值，包含未来数据\ndf[&#039;ma20&#039;] = df[&#039;price&#039;].rolling(20).mean()\n\n# 问题：在t时刻，ma20包含了t+1到t+19的数据\n```\n\n**正确代码**：\n\n```python\n# 计算t-19到t的均值\ndf[&#039;ma20&#039;] = df[&#039;price&#039;].rolling(20, min_periods=20).mean().shift(1)\n```\n\n**案例2：未来波动率**\n\n**错误代码**：\n\n```python\n# 计算20日波动率，包含未来数据\ndf[&#039;volatility&#039;] = df[&#039;return&#039;].rolling(20).std()\n```\n\n**正确代码**：\n\n```python\n# 只用历史数据计算波动率\ndf[&#039;volatility&#039;] = df[&#039;return&#039;].rolling(20, min_periods=20).std().shift(1)\n```\n\n**案例3：未来相关性**\n\n**错误代码**：\n\n```python\n# 计算与市场的相关性，包含未来数据\ndf[&#039;correlation&#039;] = df[&#039;return&#039;].rolling(20).corr(market_return)\n```\n\n**正确代码**：\n\n```python\n# 只用历史数据计算相关性\ndf[&#039;correlation&#039;] = df[&#039;return&#039;].rolling(20, min_periods=20).corr(market_return).shift(1)\n```\n\n### 5.2 检测方法\n\n**方法1：IC突然变高**\n\n如果训练集IC=0.05，但回测IC=0.5，说明可能存在Look-ahead Bias。\n\n**方法2：回测夏普异常高**\n\n如果策略回测夏普&gt;5，说明可能存在Look-ahead Bias。\n\n**方法3：因子与未来价格相关性**\n\n检验因子是否与未来价格相关：\n\n```python\n# 检测因子是否与未来价格相关\nfor lag in [-1, -2, -5, -10]:\n    corr = df[&#039;factor&#039;].shift(lag).corr(df[&#039;price&#039;])\n    print(f&quot;Lag {lag}: {corr:.4f}&quot;)\n```\n\n如果 `lag=-1` 的相关性显著大于其他lag，说明因子包含未来信息。\n\n---\n\n## 6. 实盘vs回测的差异\n\n### 6.1 回测阶段\n\n在回测阶段，我们有完整的历史数据：\n\n**流程**：\n1. 加载历史数据（$t=1$ 到 $t=T$）\n2. 计算 Label Shift：$\\text{Label}_t = \\text{Return}_{t \\to t+h}$\n3. 训练模型：$\\text{Model}(\\text{Factor}_t) \\to \\text{Label}_t$\n4. 验证模型：计算IC、回测收益等\n\n**关键**：Label Shift是可行的，因为我们有未来数据。\n\n### 6.2 实盘阶段\n\n在实盘阶段，我们只有当前和过去的数据：\n\n**流程**：\n1. 加载实时数据（$t=1$ 到 $t=\\text{now}$）\n2. 计算当前因子：$\\text{Factor}_{\\text{now}}$\n3. 预测未来收益：$\\hat{\\text{Label}}_{\\text{now} \\to \\text{now}+h} = \\text{Model}(\\text{Factor}_{\\text{now}})$\n4. 交易决策：根据预测进行交易\n\n**关键**：Label不存在，我们只能预测。\n\n### 6.3 差异总结\n\n| 维度 | 回测 | 实盘 |\n|------|------|------|\n| **数据完整性** | 有完整历史和未来数据 | 只有当前和过去数据 |\n| **Label可用性** | 可用（Label Shift） | 不可用（需要预测） |\n| **验证方式** | 可以验证预测准确性 | 只能等待h期后验证 |\n| **风险** | 过拟合风险 | 实盘失败风险 |\n\n---\n\n## 7. 总结\n\nHorizon对齐（Label Shift）是量化投资中最基础、最核心的技术，它解决了&quot;当前的因子如何对应未来的收益&quot;这一根本问题。\n\n### 核心要点回顾\n\n1. **Horizon定义**：预测时长 $h$，从 $t$ 到 $t+h$ 的时间跨度\n2. **收益率公式**：\n   - 离散收益率：$R_{t \\to t+h} = (P_{t+h} - P_t) / P_t$\n   - 对数收益率：$r_{t \\to t+h} = \\ln(P_{t+h} / P_t)$\n3. **Label Shift**：\n   - 错误对齐：$\\{ \\text{Factor}_t, \\text{Return}_t \\}$（Look-ahead Bias）\n   - 正确对齐：$\\{ \\text{Factor}_t, \\text{Return}_{t \\to t+h} \\}$（Label Shift）\n4. **Pandas实现**：`df[&#039;label&#039;] = df[&#039;return&#039;].shift(-h)`\n5. **Qlib实现**：`Expression(Ref($close, -h) / $close - 1)`\n6. **Horizon选择**：\n   - 短期（h=1）：高频噪声，IC不稳定\n   - 中期（h=5）：信号稳定，IC中等\n   - 长期（h=20）：趋势主导，IC较高\n7. **常见错误**：Look-ahead Bias、未来均值、未来波动率等\n8. **实盘vs回测**：\n   - 回测：有完整数据，可以Label Shift\n   - 实盘：只有当前数据，需要预测\n\n### 量化投资的核心思想\n\nHorizon对齐的本质是**建立因果预测关系**：\n\n$$ \\text{Cause (t)} \\rightarrow \\text{Effect (t+h)} $$\n\n而不是：\n\n$$ \\text{Cause (t)} \\leftrightarrow \\text{Effect (t)} $$\n\n这个看似简单的技术，却是量化投资区别于&quot;赌大小&quot;的分水岭：\n- 赌大小：预测明天涨不涨（短期随机）\n- 量化投资：用当前因子预测未来收益（因果预测）\n\n在下一文档中，我们将探讨另一个核心主题：从&quot;预测绝对价格&quot;到&quot;预测相对强弱&quot;的量化思维转变。"},"quant/qlib/week1/03-横截面标准化与中性化":{"slug":"quant/qlib/week1/03-横截面标准化与中性化","filePath":"quant/qlib/week1/03-横截面标准化与中性化.md","title":"03-横截面标准化与中性化","links":[],"tags":[],"content":"横截面标准化与中性化\n引言\n在量化投资的因子研究中，横截面处理（Cross-sectional Processing）是最容易被忽视，却是最关键的技术环节。它不仅仅是一种数学技巧，更是一种投资哲学的转变：从”预测绝对价格”到”预测相对强弱”。\n本文将深入探讨横截面标准化的三大核心工具：Z-score、Rank和Neutralization，从数学原理到金融含义，从理论推导到实践应用，全面解析量化投资的横截面工程。\n\n1. Z-score数学原理\n1.1 标准化公式推导\nZ-score（标准分数）是最常用的标准化方法，它将数据转换为标准正态分布。在横截面处理的语境下，Z-score将同一时刻所有资产的因子值标准化。\n基础公式\n对于 t 时刻的横截面数据 \\{x_{1,t}, x_{2,t}, \\dots, x_{N,t}\\}，其中 N 是资产数量，Z-score定义为：\nz_{i,t} = \\frac{x_{i,t} - \\mu_t}{\\sigma_t}\n其中：\n\n\\mu_t = \\frac{1}{N}\\sum_{j=1}^N x_{j,t} 是 t 时刻的横截面均值\n\\sigma_t = \\sqrt{\\frac{1}{N}\\sum_{j=1}^N (x_{j,t} - \\mu_t)^2} 是横截面标准差\n\n性质证明\n性质1：标准化后的均值为0\n证明：\nE[z_{i,t}] = \\frac{1}{N}\\sum_{i=1}^N \\frac{x_{i,t} - \\mu_t}{\\sigma_t}\n= \\frac{1}{N\\sigma_t}\\sum_{i=1}^N (x_{i,t} - \\mu_t)\n= \\frac{1}{N\\sigma_t}\\left(\\sum_{i=1}^N x_{i,t} - N\\mu_t\\right)\n= \\frac{1}{N\\sigma_t}(N\\mu_t - N\\mu_t)\n= 0 \\quad \\blacksquare\n性质2：标准化后的方差为1\n证明：\nVar[z_{i,t}] = E[z_{i,t}^2] - (E[z_{i,t}])^2\n= E[z_{i,t}^2] - 0 \\quad \\text{（由性质1）}\n= \\frac{1}{N}\\sum_{i=1}^N \\left(\\frac{x_{i,t} - \\mu_t}{\\sigma_t}\\right)^2\n= \\frac{1}{N\\sigma_t^2}\\sum_{i=1}^N (x_{i,t} - \\mu_t)^2\n= \\frac{1}{N\\sigma_t^2} \\cdot N\\sigma_t^2\n= 1 \\quad \\blacksquare\n性质3：Z-score对线性变换不变\n对于线性变换 y = ax + b（a &gt; 0），标准化后的结果相同：\n证明：\ny_{i,t} = a x_{i,t} + b\n\\mu_y = \\frac{1}{N}\\sum_{j=1}^N (a x_{j,t} + b) = a\\mu_x + b\n\\sigma_y = \\sqrt{\\frac{1}{N}\\sum_{j=1}^N (a x_{j,t} + b - a\\mu_x - b)^2}\n= \\sqrt{\\frac{1}{N}\\sum_{j=1}^N a^2(x_{j,t} - \\mu_x)^2}\n= a\\sigma_x\nz_y = \\frac{y_{i,t} - \\mu_y}{\\sigma_y} = \\frac{a x_{i,t} + b - a\\mu_x - b}{a\\sigma_x}\n= \\frac{a(x_{i,t} - \\mu_x)}{a\\sigma_x} = \\frac{x_{i,t} - \\mu_x}{\\sigma_x} = z_x \\quad \\blacksquare\n这个性质非常重要：无论原始因子的量纲和单位如何，标准化后都统一到相同的尺度。\n1.2 金融意义：偏离均值的标准差倍数\nZ-score在金融中的含义远不止”标准化”，它度量的是一个资产相对于横截面平均水平的位置。\n解释维度\n维度1：相对强弱\n\nz = 0：资产 i 的因子值等于横截面平均水平\nz = 1：资产 i 的因子值比平均水平高1个标准差\nz = -1：资产 i 的因子值比平均水平低1个标准差\nz = 2：资产 i 的因子值显著高于平均水平（正态分布下约2.3%概率）\nz = -2：资产 i 的因子值显著低于平均水平（正态分布下约2.3%概率）\n\n维度2：风险度量\nZ-score可以视为风险暴露的度量：\n\n|z| 大：该资产的因子值显著偏离平均水平，可能是高暴露或高风险\n|z| 小：该资产的因子值接近平均水平，暴露正常\n\n维度3：标准化信号\n标准化后的信号可以作为模型输入：\n\n多个不同单位的因子（PE、ROE、波动率）可以线性组合\n模型训练更稳定（避免数值问题）\n因子权重可解释性更强\n\n实际案例\n假设我们有3只股票的PE值：\n\n股票A：PE = 30\n股票B：PE = 20\n股票C：PE = 10\n\n步骤1：计算均值和标准差\n\\mu = (30 + 20 + 10) / 3 = 20\n\\sigma = \\sqrt{((30-20)^2 + (20-20)^2 + (10-20)^2) / 3}\n= \\sqrt{(100 + 0 + 100) / 3} = \\sqrt{66.67} = 8.16\n步骤2：计算Z-score\nz_A = (30 - 20) / 8.16 = 1.22\nz_B = (20 - 20) / 8.16 = 0\nz_C = (10 - 20) / 8.16 = -1.22\n解读\n\n股票A：PE比平均水平高1.22个标准差，属于”高估值”股票\n股票B：PE等于平均水平，属于”中性估值”股票\n股票C：PE比平均水平低1.22个标准差，属于”低估值”股票\n\n如果我们在做价值投资（低PE买入），那么股票C是最佳选择（Z-score最小）。\n1.3 跨因子可加性的数学证明\nZ-score标准化的核心价值在于：不同单位、不同量纲的因子可以线性组合。\n问题提出\n假设我们有两个因子：\n\n因子1：PE（市盈率），单位是倍数，范围[5, 100]\n因子2：ROE（净资产收益率），单位是百分比，范围[-10%, 30%]\n\n如果我们想构建综合因子：\n\\text{Composite} = w_1 \\cdot \\text{PE} + w_2 \\cdot \\text{ROE}\n问题：\n\nPE的值是ROE的3-10倍，ROE的权重会被PE淹没\n不同单位无法直接比较\n\n解决方案：Z-score标准化\n对两个因子分别标准化：\nz_{\\text{PE}} = \\frac{\\text{PE} - \\mu_{\\text{PE}}}{\\sigma_{\\text{PE}}}\nz_{\\text{ROE}} = \\frac{\\text{ROE} - \\mu_{\\text{ROE}}}{\\sigma_{\\text{ROE}}}\n构建综合因子：\n\\text{Composite} = w_1 \\cdot z_{\\text{PE}} + w_2 \\cdot z_{\\text{ROE}}\n可加性证明\n我们要证明：标准化后的因子满足”可加性”，即不同因子的贡献是可比的。\n命题：对于任意两个因子 X 和 Y，标准化后 z_X 和 z_Y 在同一尺度上。\n证明：\n标准化后的因子满足：\nE[z_X] = E[z_Y] = 0\nVar[z_X] = Var[z_Y] = 1\n因此：\n\\text{Composite} = w_1 z_X + w_2 z_Y\nE[\\text{Composite}] = w_1 \\cdot 0 + w_2 \\cdot 0 = 0\nVar[\\text{Composite}] = w_1^2 \\cdot 1 + w_2^2 \\cdot 1 + 2 w_1 w_2 \\text{Cov}(z_X, z_Y)\n= w_1^2 + w_2^2 + 2 w_1 w_2 \\rho_{XY}\n其中 \\rho_{XY} = \\text{Corr}(z_X, z_Y) 是相关系数。\n关键点：\n\nz_X 和 z_Y 都在相似的范围（均值0，方差1）\n权重 w_1, w_2 直接反映因子的相对重要性\nw_1 = w_2 意味着两个因子同等重要 \\quad \\blacksquare\n\n实际应用\n假设我们构建价值因子：\n\\text{Value} = 0.6 \\cdot z_{\\text{PE}} - 0.4 \\cdot z_{\\text{ROE}}\n解读：\n\n低PE（z_{\\text{PE}} 小）是好\n高ROE（z_{\\text{ROE}} 大）是好\n因此：PE取负号，ROE取正号\n\n如果不标准化：\n\\text{Value} = 0.6 \\cdot \\text{PE} - 0.4 \\cdot \\text{ROE}\n问题：\n\nPE ≈ 30，ROE ≈ 0.15\nPE项 = 18，ROE项 = -0.06\nROE的贡献几乎可以忽略，被PE完全淹没\n\n1.4 时序Z-score vs 横截面Z-score\nQlib中Z-score有两种计算方式：时序（Time-Series）和横截面（Cross-Sectional），它们的金融含义完全不同。\n时序Z-score\n对于资产 i 在时间 t 的因子值 x_{i,t}，时序Z-score定义为：\nz^{\\text{ts}}_{i,t} = \\frac{x_{i,t} - \\mu_i}{\\sigma_i}\n其中：\n\n\\mu_i = \\frac{1}{T}\\sum_{t=1}^T x_{i,t} 是资产 i 的时序均值\n\\sigma_i = \\sqrt{\\frac{1}{T}\\sum_{t=1}^T (x_{i,t} - \\mu_i)^2} 是时序标准差\n\n金融含义：\n\n度量资产 i 的因子值相对于自身历史的位置\nz^{\\text{ts}}_{i,t} = 1：当前值比历史平均高1个标准差\nz^{\\text{ts}}_{i,t} = -1：当前值比历史平均低1个标准差\n\n应用场景：\n\n动量因子：当前价格是否显著高于历史平均\n均值回归策略：当前值是否显著偏离历史均值\n\n横截面Z-score\n对于 t 时刻的横截面数据 \\{x_{1,t}, x_{2,t}, \\dots, x_{N,t}\\}，横截面Z-score定义为：\nz^{\\text{cs}}_{i,t} = \\frac{x_{i,t} - \\mu_t}{\\sigma_t}\n其中：\n\n\\mu_t = \\frac{1}{N}\\sum_{j=1}^N x_{j,t} 是 t 时刻的横截面均值\n\\sigma_t = \\sqrt{\\frac{1}{N}\\sum_{j=1}^N (x_{j,t} - \\mu_t)^2} 是横截面标准差\n\n金融含义：\n\n度量资产 i 相对于其他资产的位置\nz^{\\text{cs}}_{i,t} = 1：当前值比其他资产平均水平高1个标准差\nz^{\\text{cs}}_{i,t} = -1：当前值比其他资产平均水平低1个标准差\n\n应用场景：\n\n相对强弱：当前资产是否比其他资产强\n横截面选股：选出比其他资产表现好的股票\n\n对比分析\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n维度时序Z-score横截面Z-score比较基准自身历史其他资产计算维度时间维度资产维度金融含义当前是否偏离历史平均当前是否优于其他资产策略类型均值回归、趋势跟踪相对强弱、多因子选股Qlib默认否是\n实际案例\n假设我们有2只股票5天的价格：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n时间股票A价格股票B价格Day 110050Day 211055Day 312060Day 413065Day 514070\n时序Z-score（对每只股票单独计算）\n股票A：\n\n\\mu_A = (100 + 110 + 120 + 130 + 140) / 5 = 120\n\\sigma_A = \\sqrt{((100-120)^2 + \\cdots + (140-120)^2) / 5} = 15.81\nDay 5: z_A = (140 - 120) / 15.81 = 1.26\n\n股票B：\n\n\\mu_B = (50 + 55 + 60 + 65 + 70) / 5 = 60\n\\sigma_B = 7.91\nDay 5: z_B = (70 - 60) / 7.91 = 1.26\n\n解读：\n\n两只股票在Day 5都比各自历史高1.26个标准差\n如果做均值回归，都应该卖出\n\n横截面Z-score（对每一天单独计算）\nDay 5：\n\n\\mu_{\\text{Day5}} = (140 + 70) / 2 = 105\n\\sigma_{\\text{Day5}} = \\sqrt{((140-105)^2 + (70-105)^2) / 2} = 35\nz_A = (140 - 105) / 35 = 1\nz_B = (70 - 105) / 35 = -1\n\n解读：\n\n股票A比股票B强2个标准差（z_A - z_B = 1 - (-1) = 2）\n如果做多空策略：买入股票A，做空股票B\n\n关键差异\n时序Z-score关注”当前是否偏离历史”，横截面Z-score关注”当前是否优于其他”。在多因子选股中，我们更关注后者，因此Qlib默认使用横截面Z-score。\n\n2. Rank标准化\n2.1 Rank公式与离散化\nRank标准化将连续的因子值转换为离散的排序位置，再缩放到 [0, 1] 区间。它是对异常值最鲁棒的标准化方法。\n基础公式\n对于 t 时刻的横截面数据 \\{x_{1,t}, x_{2,t}, \\dots, x_{N,t}\\}，Rank标准化定义为：\n\\text{Rank}(x_{i,t}) = \\frac{\\text{rank}(x_{i,t}) - 1}{N - 1}\n其中 \\text{rank}(x_{i,t}) 是 x_{i,t} 在 \\{x_{1,t}, x_{2,t}, \\dots, x_{N,t}\\} 中的排序位置（从1开始，1是最小值，N是最大值）。\n离散化过程\nRank标准化实际上是三步过程：\nStep 1：排序\nr_i = \\text{rank}(x_{i,t})\n将连续值 x_{i,t} 转换为离散的排序位置 r_i \\in \\{1, 2, \\dots, N\\}\nStep 2：平移\nr&#039;_i = r_i - 1\n将排序位置从 [1, N] 平移到 [0, N-1]\nStep 3：缩放\nz_i = \\frac{r&#039;_i}{N - 1}\n将排序位置从 [0, N-1] 缩放到 [0, 1]\n性质\n性质1：输出在 [0, 1] 区间\n证明：\n\n最小值：z_{\\min} = (1 - 1) / (N - 1) = 0\n最大值：z_{\\max} = (N - 1) / (N - 1) = 1\n任意值：0 \\le z_i \\le 1 \\quad \\blacksquare\n\n性质2：均匀分布\n在无重复值的情况下，排序后的值在 [0, 1] 上均匀分布：\nP(z_i \\le \\alpha) = \\alpha, \\quad \\forall \\alpha \\in [0, 1]\n这意味着：\n\n排名前10%的股票 z &gt; 0.9\n排名后10%的股票 z &lt; 0.1\n\n性质3：对单调变换不变\n对于任意单调递增函数 f(\\cdot)，标准化后的排序不变：\n\\text{Rank}(f(x_i)) = \\text{Rank}(x_i)\n证明：\n单调递增函数保持顺序：\nx_i &lt; x_j \\iff f(x_i) &lt; f(x_j)\n因此排序位置相同：\n \\text{rank}(f(x_i)) = \\text{rank}(x_i) \\quad \\blacksquare\n这个性质非常重要：Rank只关心”谁比谁大”，不关心大多少。\n2.2 异常值处理的鲁棒性证明\nRank标准化最大的优势在于对异常值（Outlier）的鲁棒性。\n问题场景\n假设我们有4只股票的PE值：\n\n股票A：PE = 15\n股票B：PE = 20\n股票C：PE = 25\n股票D：PE = 10000  ← 异常值（数据错误或极端情况）\n\nZ-score标准化\n步骤1：计算均值和标准差\n\\mu = (15 + 20 + 25 + 10000) / 4 = 2515\n\\sigma = \\sqrt{((15-2515)^2 + (20-2515)^2 + (25-2515)^2 + (10000-2515)^2) / 4}\n= \\sqrt{(2495^2 + 2495^2 + 2490^2 + 7485^2) / 4}\n= \\sqrt{(6,225,025 + 6,225,025 + 6,200,100 + 56,025,225) / 4}\n= \\sqrt{74,675,375 / 4} = \\sqrt{18,668,843.75} = 4321\n\\approx 4321\n步骤2：计算Z-score\nz_A = (15 - 2515) / 4321 = -0.58\nz_B = (20 - 2515) / 4321 = -0.58\nz_C = (25 - 2515) / 4321 = -0.58\nz_D = (10000 - 2515) / 4321 = 1.73\n问题分析：\n\n股票D的异常值（10000）严重影响了均值和标准差\nA、B、C三只股票的Z-score几乎相同（-0.58），无法区分\n股票D的Z-score是1.73，看起来不极端，但实际上它是严重异常值\n\nRank标准化\n步骤1：排序\nx_A = 15, x_B = 20, x_C = 25, x_D = 10000\n\\text{rank}(x_A) = 1\n\\text{rank}(x_B) = 2\n\\text{rank}(x_C) = 3\n\\text{rank}(x_D) = 4\n步骤2：标准化\nz_A = (1 - 1) / (4 - 1) = 0\nz_B = (2 - 1) / 3 = 0.33\nz_C = (3 - 1) / 3 = 0.67\nz_D = (4 - 1) / 3 = 1\n优势分析：\n\n股票D的异常值只影响它自己的排序（第4名），不影响其他股票\nA、B、C三只股票的排序清晰区分（0, 0.33, 0.67）\n股票D虽然是异常值，但在Rank中只是”第4名”，不会拉高或拉低其他股票的分数\n\n鲁棒性证明\n命题：Rank标准化对异常值不敏感。\n证明：\n设横截面数据 \\{x_1, x_2, \\dots, x_N\\}，其中 x_N 是异常值（远大于其他数据）。\nZ-score的影响：\n\\mu = \\frac{1}{N}\\sum_{i=1}^N x_i = \\frac{1}{N}\\left(\\sum_{i=1}^{N-1} x_i + x_N\\right)\n当 x_N \\gg x_i（i &lt; N）时，\\mu \\approx x_N / N，均值被拉高。\n\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu)^2}\n当 x_N 很大时，(x_N - \\mu)^2 很大，标准差被拉高。\n对于正常数据 x_i（i &lt; N）：\nz_i = \\frac{x_i - \\mu}{\\sigma}\n由于 \\mu 和 \\sigma 都被 x_N 拉高，z_i 的值会变得接近，难以区分。\nRank的影响：\n对于任意 i &lt; j &lt; N：\nx_i &lt; x_j \\implies \\text{rank}(x_i) &lt; \\text{rank}(x_j)\n排序不受 x_N 的大小影响，只受相对大小影响。\n\\text{Rank}(x_i) = \\frac{\\text{rank}(x_i) - 1}{N - 1}\nx_N 只影响它自己的排序，不影响其他数据的排序位置。\\quad \\blacksquare\n2.3 适用场景分析\nRank标准化在以下场景中表现优异：\n场景1：因子分布严重偏态\n如果因子的分布不是正态分布，而是严重偏态（如指数分布、幂律分布），Z-score的效果会变差。\n案例：市值因子\n大多数股票的市值较小，少数股票市值巨大（如茅台、腾讯）。市值的分布是右偏的（长尾分布）。\n\nZ-score：会被大市值股票拉高均值和标准差，中小市值股票的Z-score会集中在负值区域\nRank：不受分布形状影响，只看相对排序\n\n场景2：存在极端异常值\n如果因子中存在极端异常值（数据错误、黑天鹅事件），Z-score会失效。\n案例：停牌后复牌的股票\n某股票停牌3年后复牌，涨了10倍。这个极端值会严重拉高Z-score的均值和标准差。\n\nZ-score：受极端值影响严重\nRank：异常值只是”第1名”，不影响其他股票\n\n场景3：只关注相对强弱\n如果你的策略只关心”哪个股票比哪个强”，而不关心强多少，Rank是最佳选择。\n案例：多空对冲策略\n做多前20%股票，做空后20%股票。策略只关心排名，不关心具体分数。\n\nZ-score：需要选择阈值（如 z &gt; 1），阈值的选择会影响策略\nRank：直接选前20%（z &gt; 0.8），阈值清晰\n\n场景4：非线性关系\n如果因子与收益的关系是非线性的，Rank可以捕捉到这种非线性。\n案例：动量因子\n动量与收益的关系可能是非线性的：\n\n\n动量前10%：收益最高\n\n\n动量10%-50%：收益中等\n\n\n动量后50%：收益最低\n\n\nZ-score：假设线性关系，可能忽略非线性结构\n\n\nRank：自然处理非线性，只看排序分组\n\n\n不适用场景\n场景1：需要精确比较\n如果需要精确比较两个因子的值，Rank不适用，因为它丢失了数值信息。\n案例：风险管理\n如果需要计算因子的波动率、VaR等，Z-score更合适。\n场景2：因子分布接近正态\n如果因子的分布接近正态分布，Z-score的统计性质更好（均值0、方差1）。\n案例：因子组合的权重计算\nZ-score的均值为0、方差为1，便于理论推导和权重计算。\n\n3. Neutralization回归模型\n3.1 数学推导\nNeutralization（中性化）是横截面处理中最复杂、最强大的工具。它的核心思想是通过回归剔除”作弊因素”（如行业、市值），只保留”纯Alpha”。\n线性回归模型\n对于 t 时刻的横截面数据，我们构建多元线性回归模型：\nx_i = \\beta_0 + \\sum_{j=1}^K \\beta_j \\cdot f_{i,j} + \\varepsilon_i\n其中：\n\nx_i：资产 i 的因子值（如PE、ROE、动量等）\n\\beta_0：截距项\n\\beta_j：第 j 个控制变量的回归系数\nf_{i,j}：资产 i 在第 j 个控制变量上的暴露（如行业哑变量、市值）\n\\varepsilon_i：残差（中性化后的因子值）\n\n矩阵形式\n设：\n\ny \\in \\mathbb{R}^N：因子值向量 \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N\\end{bmatrix}\nX \\in \\mathbb{R}^{N \\times (K+1)}：控制变量矩阵 \\begin{bmatrix}1 &amp; f_{1,1} &amp; \\cdots &amp; f_{1,K} \\\\ 1 &amp; f_{2,1} &amp; \\cdots &amp; f_{2,K} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; f_{N,1} &amp; \\cdots &amp; f_{N,K}\\end{bmatrix}\n\\beta \\in \\mathbb{R}^{K+1}：回归系数向量 \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_K\\end{bmatrix}\n\\varepsilon \\in \\mathbb{R}^N：残差向量 \\begin{bmatrix}\\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_N\\end{bmatrix}\n\n回归方程：\ny = X\\beta + \\varepsilon\nOLS估计\n通过最小二乘法估计 \\beta：\n\\hat{\\beta} = \\arg\\min_\\beta \\|y - X\\beta\\|^2\n求导：\n\\frac{\\partial}{\\partial \\beta} \\|y - X\\beta\\|^2 = -2X^T(y - X\\beta)\n令导数为0：\n-2X^T(y - X\\beta) = 0\nX^T y - X^T X \\beta = 0\nX^T X \\beta = X^T y\n\\beta = (X^T X)^{-1} X^T y\n残差计算\n\\varepsilon = y - X\\beta\n= y - X(X^T X)^{-1} X^T y\n= (I - H)y\n其中 H = X(X^T X)^{-1} X^T 是投影矩阵（Hat Matrix）。\n中性化后的因子\nx^{\\text{neu}}_i = \\varepsilon_i = x_i - (\\beta_0 + \\sum_{j=1}^K \\beta_j \\cdot f_{i,j})\n3.2 行业中性化\n目的\n剔除行业因子暴露，确保因子收益不来自行业轮动。\n场景示例\n假设你有3只股票：\n\n股票A：行业=白酒，因子值=10\n股票B：行业=白酒，因子值=8\n股票C：行业=科技，因子值=9\n\n不中性化的问题\n如果白酒板块整体上涨（行业Beta），股票A和B的收益部分来自行业因子，而不是你的因子有效。\n中性化过程\n步骤1：构造行业哑变量\n构造行业哑变量矩阵 X：\nX = \\begin{bmatrix}1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1\\end{bmatrix}\n其中：\n\n第1列：截距项（全为1）\n第2列：白酒行业哑变量（1=是，0=否）\n第3列：科技行业哑变量（1=是，0=否）\n\n步骤2：回归\ny = \\begin{bmatrix}10 \\\\ 8 \\\\ 9\\end{bmatrix} = X\\beta + \\varepsilon\n计算 \\beta：\n\\beta = (X^T X)^{-1} X^T y\nX^T X = \\begin{bmatrix}1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1\\end{bmatrix} \\begin{bmatrix}1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1\\end{bmatrix} = \\begin{bmatrix}3 &amp; 2 &amp; 1 \\\\ 2 &amp; 2 &amp; 0 \\\\ 1 &amp; 0 &amp; 1\\end{bmatrix}\n(X^T X)^{-1} = \\frac{1}{2}\\begin{bmatrix}2 &amp; -2 &amp; -2 \\\\ -2 &amp; 2 &amp; 2 \\\\ -2 &amp; 2 &amp; 4\\end{bmatrix} = \\begin{bmatrix}1 &amp; -1 &amp; -1 \\\\ -1 &amp; 1 &amp; 1 \\\\ -1 &amp; 1 &amp; 2\\end{bmatrix}\nX^T y = \\begin{bmatrix}1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1\\end{bmatrix} \\begin{bmatrix}10 \\\\ 8 \\\\ 9\\end{bmatrix} = \\begin{bmatrix}27 \\\\ 18 \\\\ 9\\end{bmatrix}\n\\beta = \\begin{bmatrix}1 &amp; -1 &amp; -1 \\\\ -1 &amp; 1 &amp; 1 \\\\ -1 &amp; 1 &amp; 2\\end{bmatrix} \\begin{bmatrix}27 \\\\ 18 \\\\ 9\\end{bmatrix} = \\begin{bmatrix}27 - 18 - 9 \\\\ -27 + 18 + 9 \\\\ -27 + 18 + 18\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 0 \\\\ 9\\end{bmatrix}\n步骤3：计算残差\n\\varepsilon = y - X\\beta = \\begin{bmatrix}10 \\\\ 8 \\\\ 9\\end{bmatrix} - \\begin{bmatrix}1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1\\end{bmatrix} \\begin{bmatrix}0 \\\\ 0 \\\\ 9\\end{bmatrix}\n= \\begin{bmatrix}10 \\\\ 8 \\\\ 9\\end{bmatrix} - \\begin{bmatrix}0 \\\\ 0 \\\\ 9\\end{bmatrix} = \\begin{bmatrix}10 \\\\ 8 \\\\ 0\\end{bmatrix}\n解读\n\n股票A：残差=10，说明它比白酒行业平均（9）高1\n股票B：残差=8，说明它比白酒行业平均（9）低1\n股票C：残差=0，说明它等于科技行业平均（9）\n\n经济含义\n\\varepsilon_i 是”剔除行业因素后的纯Alpha”：\n\n股票A的因子收益中，剔除行业贡献后，还剩10\n股票B的因子收益中，剔除行业贡献后，只剩8\n股票C的因子收益中，科技行业贡献了9，纯Alpha为0\n\n3.3 市值中性化\n目的\n剔除市值风格因子暴露，确保因子收益不来自大小盘风格切换。\n场景示例\n假设我们有4只股票：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n股票行业市值（亿）因子值A白酒10010B白酒1008C科技509D科技507\n回归模型\nx_i = \\beta_0 + \\beta_1 \\cdot \\text{industry}_{i} + \\beta_2 \\cdot \\log(\\text{cap}_i) + \\varepsilon_i\n其中：\n\n\\text{industry}_i：行业哑变量（1=白酒，0=科技）\n\\log(\\text{cap}_i)：对数市值（市值通常服从对数正态分布）\n\n构造控制变量矩阵\nX = \\begin{bmatrix}1 &amp; 1 &amp; \\log(100) \\\\ 1 &amp; 1 &amp; \\log(100) \\\\ 1 &amp; 0 &amp; \\log(50) \\\\ 1 &amp; 0 &amp; \\log(50)\\end{bmatrix} = \\begin{bmatrix}1 &amp; 1 &amp; 4.61 \\\\ 1 &amp; 1 &amp; 4.61 \\\\ 1 &amp; 0 &amp; 3.91 \\\\ 1 &amp; 0 &amp; 3.91\\end{bmatrix}\n计算残差\n（省略详细计算，假设回归结果为 \\beta = [0, 0.5, 0.5]^T）\n\\varepsilon = y - X\\beta\n经济含义\n\\varepsilon_i 是”剔除行业和市值效应后的纯Alpha”：\n\n股票A的因子值=10，但考虑到行业效应（+0.5）和市值效应（0.5 \\times 4.61 = 2.31），纯Alpha≈7.19\n股票B的因子值=8，纯Alpha≈5.19\n股票C的因子值=9，但市值较小（0.5 \\times 3.91 = 1.96），纯Alpha≈7.04\n股票D的因子值=7，纯Alpha≈5.04\n\n3.4 双重中性化\n定义\n同时进行行业中性化和市值中性化，剔除两个维度的”作弊因素”。\n回归模型\nx_i = \\beta_0 + \\sum_{j=1}^J \\alpha_j \\cdot \\text{industry}_{i,j} + \\beta_1 \\cdot \\log(\\text{cap}_i) + \\varepsilon_i\n其中：\n\n\\text{industry}_{i,j}：行业 j 的哑变量（J 个行业）\n\\alpha_j：行业 j 的回归系数\n\\log(\\text{cap}_i)：对数市值\n\\varepsilon_i：残差（双重中性化后的因子）\n\n残差的经济含义\n\\varepsilon_i = \\text{纯Alpha} = x_i - \\underbrace{\\sum_{j=1}^J \\alpha_j \\cdot \\text{industry}_{i,j}}_{\\text{行业效应}} - \\underbrace{\\beta_1 \\cdot \\log(\\text{cap}_i)}_{\\text{市值效应}}\n\\varepsilon_i 表示：\n\n剔除了行业效应（同一行业内的股票）\n剔除了市值效应（同等市值下的股票）\n剩下的才是”纯选股能力”\n\n3.5 残差的可解释性证明\n命题1：残差均值为0\n证明：\nE[\\varepsilon] = \\frac{1}{N}\\sum_{i=1}^N \\varepsilon_i = \\frac{1}{N}\\sum_{i=1}^N (y_i - X_i \\beta)\n= \\frac{1}{N}\\left(\\sum_{i=1}^N y_i - \\sum_{i=1}^N X_i \\beta\\right)\n= \\frac{1}{N}\\left(N\\bar{y} - N\\bar{X}\\beta\\right)\n由于 \\hat{\\beta} 是OLS估计，满足正规方程：\nX^T(y - X\\beta) = 0\n即：\n\\sum_{i=1}^N X_i(y_i - X_i \\beta) = 0\n取第一行（截距项，X_{i,0} = 1）：\n\\sum_{i=1}^N (y_i - X_i \\beta) = 0\n\\implies \\frac{1}{N}\\sum_{i=1}^N (y_i - X_i \\beta) = 0\n\\implies E[\\varepsilon] = 0 \\quad \\blacksquare\n命题2：残差与控制变量正交\n证明：\nX^T \\varepsilon = X^T(y - X\\beta) = X^T y - X^T X \\beta\n代入 \\beta = (X^T X)^{-1} X^T y：\n= X^T y - X^T X (X^T X)^{-1} X^T y\n= X^T y - X^T y = 0\n因此：\n\\text{Cov}(\\varepsilon, X) = \\frac{1}{N} X^T \\varepsilon = 0 \\quad \\blacksquare\n金融含义\n残差与控制变量不相关，说明：\n\n残差中不包含行业因子（如果控制变量包含行业哑变量）\n残差中不包含市值因子（如果控制变量包含市值）\n残差是”剔除了风格因子后的纯Alpha”\n\n\n4. 三者组合实践\n4.1 最佳实践流程\n横截面处理通常是多个步骤的组合，最佳实践流程如下：\nStep 1：Z-score标准化\n将不同单位的因子标准化到同一尺度：\n\\text{Factor}_z = \\frac{\\text{Factor} - \\mu}{\\sigma}\nStep 2：中性化\n剔除行业、市值等风格因子：\n\\text{Factor}_{\\text{neu}} = \\text{Factor}_z - X\\beta\n其中 X 是控制变量矩阵（行业、市值等），\\beta 是回归系数。\nStep 3：二次标准化（可选）\n中性化后，因子的分布可能偏离标准正态分布，可以再次标准化：\n\\text{Factor}_{\\text{final}} = \\frac{\\text{Factor}_{\\text{neu}} - \\mu_{\\text{neu}}}{\\sigma_{\\text{neu}}}\n4.2 顺序依赖关系\n顺序A：Z-score → Neutralize ✓\n这是正确的顺序。\n理由：\n\nZ-score将不同单位的因子标准化（如PE和ROE）\n标准化后的因子在同一尺度上，可以放入同一个回归模型\n中性化时，回归系数 \\beta 的单位统一，解释性更强\n\n顺序B：Neutralize → Z-score ✗\n这是错误的顺序。\n问题：\n\n中性化前的因子量纲不同（PE=倍数，ROE=百分比）\n回归系数 \\beta 的单位不统一，解释性差\n不同因子的残差不在同一尺度上，难以比较\n\n4.3 性能优化技巧\n技巧1：向量化回归\n对于大型数据集（如A股5000只股票），应该使用向量化回归，而不是for循环：\n慢（for循环）：\nfor t in time_grid:\n    for i in range(N):\n        residual[i] = factor[i] - sum(beta[j] * control[i, j] for j in range(K))\n快（向量化）：\nresidual = factor - X @ beta\n技巧2：行业哑变量稀疏矩阵存储\n行业哑变量矩阵 X 是稀疏的（大部分元素为0），应该使用稀疏矩阵存储：\n节省内存：\n\n密集矩阵：5000 \\times 30 个float，约 1.2 MB\n稀疏矩阵：只存储非零元素，约 0.05 MB（节省96%）\n\n技巧3：批量中性化\n对于时间序列数据，可以批量中性化：\n# 一次性中性化所有时刻\nresidual_matrix = factor_matrix - X @ beta_matrix\n而不是逐时刻中性化：\n# 逐时刻中性化（慢）\nfor t in range(T):\n    residual_matrix[t] = factor_matrix[t] - X @ beta[t]\n\n5. 总结\n横截面标准化与中性化是量化投资中从”绝对预测”到”相对强弱”转变的关键技术。\n核心要点回顾\n\n\nZ-score：\n\n公式：z = (x - \\mu) / \\sigma\n金融含义：偏离均值的标准差倍数\n优势：不同单位因子可线性组合\n劣势：对异常值敏感\n\n\n\nRank：\n\n公式：z = (r - 1) / (N - 1)，其中 r 是排序位置\n金融含义：只在乎”谁比谁强”，不在乎强多少\n优势：对异常值鲁棒\n劣势：丢失数值信息\n\n\n\nNeutralization：\n\n公式：\\varepsilon = y - X\\beta\n金融含义：剔除风格因子后的纯Alpha\n优势：纯化信号，剥离风格暴露\n劣势：计算复杂\n\n\n\n组合实践：\n\n最佳顺序：Z-score → Neutralize → (可选)再次标准化\n性能优化：向量化回归、稀疏矩阵、批量处理\n\n\n\n量化投资哲学的转变\n通过横截面标准化和中性化，量化投资的思维发生了质的变化：\n从”预测价格”到”预测相对强弱”：\n\n传统：预测股票A明天涨不涨\n现代：预测股票A明天是否比股票B强\n\n从”赚取Beta”到”提取Alpha”：\n\n传统：赌行业轮动、风格切换\n现代：剔除风格，只捕获纯Alpha\n\n从”追求精确”到”追求稳定”：\n\n传统：预测具体价格（难）\n现代：预测相对排名（容错率高）\n\n横截面处理不是数学技巧，而是一种投资策略：\n\n我们不赌国运涨跌，也不赌行业轮动，我们只赌”同一环境下，谁比谁强”。\n\n在下一文档中，我们将深入探讨另一个核心主题：Horizon对齐（Label Shift），这是消除未来函数并建立因果预测关系的关键技术。"},"quant/qlib/week1/04-相对强弱预测的量化思维":{"slug":"quant/qlib/week1/04-相对强弱预测的量化思维","filePath":"quant/qlib/week1/04-相对强弱预测的量化思维.md","title":"04-相对强弱预测的量化思维","links":[],"tags":[],"content":"从绝对预测到相对强弱的量化思维转变\n引言\n在量化投资中，“预测相对强弱”而非”预测绝对价格”是区分”散户思维”与”机构量化思维”的分水岭。这个转变不仅是技术层面的，更是思维层面的质变。\n本文将从绝对预测与相对预测的本质差异、市场噪声过滤机制、Alpha/Beta分解理论、评估指标详解等多个维度，全面解析”预测相对强弱”的量化思维。\n\n1. 绝对预测 vs 相对预测的本质\n1.1 绝对预测定义\n目标：预测资产价格或收益率的绝对值。\n数学形式：\n\\text{Model}: \\mathbf{X}_t \\to P_{t+h} \\text{ 或 } R_{t+h}\n其中：\n\n\\mathbf{X}_t：t 时刻的因子特征\nP_{t+h}：t+h 时刻的绝对价格\nR_{t+h}：t+h 时刻的收益率\n\n示例：\n\n“明天茅台涨不涨？”\n“上证指数下个月到多少点？”\n“比特币年底价格是多少？”\n\n评价体系：\n\n准确率（Accuracy）：预测方向正确的比例\n均方误差（MSE）：预测值与真实值的平方误差\n均方根误差（RMSE）：预测值与真实值的误差平方根\n\\text{MSE} = \\frac{1}{N}\\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n\\text{RMSE} = \\sqrt{\\text{MSE}}\n\n1.2 相对预测定义\n目标：预测资产在横截面上的相对强弱（排名）。\n数学形式：\n\\text{Model}: \\mathbf{X}_t \\to \\text{Rank}_t\n其中：\n\n\\mathbf{X}_t：t 时刻的因子特征\n\\text{Rank}_t \\in [0, 1]：资产的横截面排名（标准化到[0,1]）\n\n示例：\n\n“在所有股票里，茅台明天是不是表现最好的那10%？”\n“在所有DeFi协议中，AAVE未来1个月是否跑赢平均？”\n“在所有Token中，BTC未来1周是否排名前20%？”\n\n评价体系：\n\n\n信息系数（IC）：因子值与未来收益的相关系数\n\\text{IC} = \\text{Corr}(F_t, R_{t \\to t+h}) = \\frac{\\text{Cov}(F_t, R_{t \\to t+h})}{\\sigma_{F_t} \\sigma_{R_{t \\to t+h}}}\n\n\n信息比率（IR）：IC的均值与标准差之比\n\\text{IR} = \\frac{\\text{Mean}(\\text{IC})}{\\text{Std}(\\text{IC})}\n\n\n1.3 两种思维的对比\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n维度绝对预测相对预测预测目标价格/收益率排名/相对强弱输出空间连续值 \\mathbb{R}排名 [0, 1]噪声敏感性高低容错率低高适用场景趋势跟踪、择时多因子选股、市场中性评价体系RMSE/MAEIC/IR对市场依赖强（依赖Beta）弱（剥离Beta）对精度要求极高中等（只需要相关性）风险控制困难容易（分散投资）\n1.4 数学对比\n绝对预测的困难\n假设我们要预测股票 i 在 t+h 时刻的价格：\nP_{i,t+h} = P_{i,t} \\cdot (1 + R_{i,t \\to t+h})\n其中收益率 R_{i,t \\to t+h} 可以分解为：\nR_{i,t \\to t+h} = \\underbrace{\\beta_i \\cdot R_{m,t \\to t+h}}_{\\text{市场Beta}} + \\underbrace{\\alpha_{i,t \\to t+h}}_{\\text{特质收益}} + \\underbrace{\\varepsilon_{i,t \\to t+h}}_{\\text{噪声}}\n绝对预测的问题：\n\n市场Beta \\beta_i \\cdot R_{m,t \\to t+h}：无法准确预测市场涨跌\n特质收益 \\alpha_{i,t \\to t+h}：信噪比低，难以准确预测\n噪声 \\varepsilon_{i,t \\to t+h}：随机性强，完全无法预测\n\n相对预测的优势\n在横截面上，我们比较的是：\n\\text{Rank}(R_{i,t \\to t+h}) = \\text{Rank}(\\beta_i \\cdot R_{m,t \\to t+h} + \\alpha_{i,t \\to t+h} + \\varepsilon_{i,t \\to t+h})\n由于市场Beta R_{m,t \\to t+h} 对所有股票相同，可以约去：\n\\text{Rank}(R_{i,t \\to t+h}) \\approx \\text{Rank}(\\beta_i + \\alpha_{i,t \\to t+h} + \\varepsilon_{i,t \\to t+h})\n因此，相对预测：\n\n不依赖市场涨跌：市场涨跌对所有股票影响相同，不影响排名\n剥离市场噪声：只关注”谁比谁强”，不关注”强多少”\n降低预测难度：不需要预测绝对收益，只需要预测相对强弱\n\n\n2. 市场噪声过滤机制\n2.1 市场系统性风险模型\n资产收益分解\n根据资本资产定价模型（CAPM），资产收益可以分解为：\nR_{i,t} = \\alpha_i + \\beta_i \\cdot R_{m,t} + \\varepsilon_{i,t}\n其中：\n\nR_{i,t}：资产 i 在 t 时刻的收益\n\\alpha_i：资产 i 的Alpha（特质收益）\n\\beta_i：资产 i 对市场的敏感度\nR_{m,t}：市场在 t 时刻的收益\n\\varepsilon_{i,t}：随机噪声（期望为0，与市场不相关）\n\n扩展到多因子模型：\nR_{i,t} = \\alpha_i + \\sum_{j=1}^K \\beta_{i,j} \\cdot R_{f_j,t} + \\varepsilon_{i,t}\n其中 R_{f_j,t} 是第 j 个因子的收益（如市场、规模、价值等）。\n系统性风险 vs 特质风险：\n\n系统性风险：\\sum_{j=1}^K \\beta_{i,j} \\cdot R_{f_j,t}，无法通过分散投资消除\n特质风险：\\varepsilon_{i,t}，可以通过分散投资消除\n\n2.2 横截面处理的去噪原理\n绝对视角的困境\n假设市场收益率 R_m = -5\\%（普跌5%），我们有两只股票：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n股票AlphaBeta预测收益实际收益A+2%1.0+2%-3%B-3%1.0-3%-8%\n绝对视角：\n\n股票A：预测+2%，实际-3%，误差-5%（看似模型失效）\n股票B：预测-3%，实际-8%，误差-5%（看似模型失效）\n\n相对视角：\n\n横截面比较：A（-3%）&gt; B（-8%）\nAlpha比较：A（+2%）&gt; B（-3%）\n模型有效！A比B强\n\n横截面处理的去噪原理\n在横截面标准化后，我们计算：\nz_{i,t} = \\frac{R_{i,t} - \\mu_t}{\\sigma_t}\n其中 \\mu_t 是 t 时刻的横截面均值，\\sigma_t 是标准差。\n假设市场收益率 R_m = -5\\%，\\mu_t \\approx -5\\%：\nz_A = \\frac{-3\\% - (-5\\%)}{\\sigma_t} = \\frac{2\\%}{\\sigma_t}\nz_B = \\frac{-8\\% - (-5\\%)}{\\sigma_t} = \\frac{-3\\%}{\\sigma_t}\n因此：\n\nz_A &gt; 0：A强于平均水平\nz_B &lt; 0：B弱于平均水平\n\n关键洞察：\n\n横截面处理自动剔除了市场系统性风险 R_m\n只保留特质收益 \\alpha_i 和部分噪声 \\varepsilon_i\n大大降低了预测难度\n\n2.3 实际案例：普跌环境下的表现\n场景设置\n2022年A股熊市，上证指数从3700点跌到3000点，跌幅约-19%。\n因子：动量因子（20日收益率）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n日期上证指数动量因子IC因子IC累计2022-01-0137000.050.052022-02-0136500.040.092022-03-0135500.030.122022-04-0134000.020.142022-05-0132500.010.152022-06-013000-0.010.14\n绝对视角（困惑）：\n\n绝对收益：平均-15%（看似因子失效）\n动量因子IC逐渐下降（因子稳定性变差）\n结论：动量因子在熊市失效？\n\n相对视角（正确）：\n\n虽然全市场普跌，但动量因子IC &gt; 0（因子仍然有效）\n买入前20%股票：平均跌幅-10%\n卖出后20%股票：平均跌幅-20%\n多空收益：-10% - (-20%) = +10%\n\n关键结论：\n\\text{Alpha收益} = \\text{多头收益} - \\text{空头收益}\n= (-10\\%) - (-20\\%) = +10\\%\n\n绝对收益：-15%（市场Beta）\n相对收益（Alpha）：+10%（因子选股能力）\n因子有效！\n\n2.4 市场噪声过滤的数学证明\n命题：横截面标准化可以完全消除系统性风险（假设所有资产Beta相同）。\n证明：\n设市场收益率 R_m，所有资产的Beta相同 \\beta_i = \\beta。\n资产 i 的收益：\nR_i = \\alpha_i + \\beta \\cdot R_m + \\varepsilon_i\n横截面均值：\n\\mu = \\frac{1}{N}\\sum_{i=1}^N R_i = \\frac{1}{N}\\sum_{i=1}^N (\\alpha_i + \\beta \\cdot R_m + \\varepsilon_i)\n= \\bar{\\alpha} + \\beta \\cdot R_m + \\bar{\\varepsilon}\n其中 \\bar{\\alpha} = \\frac{1}{N}\\sum \\alpha_i 是Alpha均值，\\bar{\\varepsilon} = \\frac{1}{N}\\sum \\varepsilon_i 是噪声均值（期望为0）。\n横截面标准化：\nz_i = \\frac{R_i - \\mu}{\\sigma}\n= \\frac{(\\alpha_i + \\beta R_m + \\varepsilon_i) - (\\bar{\\alpha} + \\beta R_m + \\bar{\\varepsilon})}{\\sigma}\n= \\frac{(\\alpha_i - \\bar{\\alpha}) + (\\varepsilon_i - \\bar{\\varepsilon})}{\\sigma}\n= \\frac{\\alpha_i - \\bar{\\alpha}}{\\sigma} + \\frac{\\varepsilon_i - \\bar{\\varepsilon}}{\\sigma}\n由于 E[\\varepsilon_i] = 0，\\bar{\\varepsilon} \\approx 0：\nz_i \\approx \\frac{\\alpha_i - \\bar{\\alpha}}{\\sigma} + \\frac{\\varepsilon_i}{\\sigma}\n结论：\n\n系统性风险 \\beta R_m 被完全消除\n只保留特质收益 \\alpha_i 和噪声 \\varepsilon_i\n横截面处理实现了”市场中性” \\quad \\blacksquare\n\n\n3. Alpha/Beta分解理论\n3.1 CAPM模型基础\n资本资产定价模型（CAPM）：\nE[R_i] = R_f + \\beta_i \\cdot (E[R_m] - R_f)\n其中：\n\nE[R_i]：资产 i 的期望收益\nR_f：无风险收益率\n\\beta_i：资产 i 对市场的敏感度\nE[R_m]：市场的期望收益\nE[R_m] - R_f：市场风险溢价\n\nBeta的定义：\n\\beta_i = \\frac{\\text{Cov}(R_i, R_m)}{\\text{Var}(R_m)}\n经济含义：\n\n\\beta_i = 1：资产收益与市场收益同步\n\\beta_i &gt; 1：资产收益波动大于市场（进攻型）\n\\beta_i &lt; 1：资产收益波动小于市场（防御型）\n\n3.2 Alpha的数学定义与分解\n扩展的CAPM模型（实际收益 vs 期望收益）：\nR_{i,t} = R_f + \\beta_i \\cdot (R_{m,t} - R_f) + \\alpha_{i,t} + \\varepsilon_{i,t}\n其中：\n\n\\alpha_{i,t}：资产 i 在 t 时刻的Alpha（超额收益）\n\\varepsilon_{i,t}：随机噪声（期望为0）\n\nAlpha的定义：\n\\alpha_{i,t} = R_{i,t} - R_f - \\beta_i \\cdot (R_{m,t} - R_f) - \\varepsilon_{i,t}\nAlpha的来源：\nAlpha可以分解为三个来源：\n来源1：选股能力（Stock Selection）\n\\alpha_{\\text{stock}} = \\text{因子选股产生的超额收益}\n示例：动量因子选出的股票平均收益+5%，市场平均收益+2%，选股Alpha=+3%。\n来源2：择时能力（Market Timing）\n\\alpha_{\\text{timing}} = \\text{预测市场涨跌产生的超额收益}\n示例：预测到市场下跌，提前降低仓位，避免了-5%的损失，择时Alpha=+5%。\n来源3：执行能力（Execution）\n\\alpha_{\\text{execution}} = \\text{交易执行产生的超额收益}\n示例：优化交易时机，降低交易成本，执行Alpha=+0.5%。\n总Alpha：\n\\alpha_{\\text{total}} = \\alpha_{\\text{stock}} + \\alpha_{\\text{timing}} + \\alpha_{\\text{execution}}\n3.3 Beta暴露的量化方法\n回归估计\n使用历史数据回归估计Beta：\n\\hat{\\beta}_i = \\frac{\\text{Cov}(R_i, R_m)}{\\text{Var}(R_m)} = \\frac{\\sum_{t=1}^T (R_{i,t} - \\bar{R}_i)(R_{m,t} - \\bar{R}_m)}{\\sum_{t=1}^T (R_{m,t} - \\bar{R}_m)^2}\n其中：\n\nT：历史数据长度\n\\bar{R}_i = \\frac{1}{T}\\sum_{t=1}^T R_{i,t}：资产 i 的平均收益\n\\bar{R}_m = \\frac{1}{T}\\sum_{t=1}^T R_{m,t}：市场平均收益\n\n多因子暴露\n假设有 K 个因子（市场、规模、价值、动量等），资产 i 的收益：\nR_{i,t} = \\alpha_i + \\sum_{j=1}^K \\beta_{i,j} \\cdot R_{f_j,t} + \\varepsilon_{i,t}\n矩阵形式：\n\\mathbf{R}_t = \\boldsymbol{\\alpha} + \\mathbf{B} \\cdot \\mathbf{R}_{f,t} + \\boldsymbol{\\varepsilon}_t\n其中：\n\n\\mathbf{R}_t \\in \\mathbb{R}^N：N 个资产的收益向量\n\\boldsymbol{\\alpha} \\in \\mathbb{R}^N：Alpha向量\n\\mathbf{B} \\in \\mathbb{R}^{N \\times K}：Beta暴露矩阵\n\\mathbf{R}_{f,t} \\in \\mathbb{R}^K：K 个因子的收益向量\n\n回归估计Beta矩阵：\n\\hat{\\mathbf{B}} = (\\mathbf{R}_f^T \\mathbf{R}_f)^{-1} \\mathbf{R}_f^T \\mathbf{R}\n其中 \\mathbf{R}_f \\in \\mathbb{R}^{T \\times K} 是因子收益矩阵，\\mathbf{R} \\in \\mathbb{R}^{T \\times N} 是资产收益矩阵。\nBeta暴露控制\n在构建组合时，我们可以控制Beta暴露：\n\\text{约束：} \\sum_{i=1}^N w_i \\cdot \\beta_{i,j} = 0, \\quad \\forall j\n其中 w_i 是资产 i 的权重。\n示例：市场中性策略\n约束 \\sum w_i \\cdot \\beta_{i,\\text{market}} = 0，确保组合对市场涨跌中性。\n3.4 多因子模型下的Alpha提取\n步骤1：计算因子暴露矩阵\n\\beta_{1,1} &amp; \\beta_{1,2} &amp; \\cdots &amp; \\beta_{1,K} \\\\\n\\beta_{2,1} &amp; \\beta_{2,2} &amp; \\cdots &amp; \\beta_{2,K} \\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n\\beta_{N,1} &amp; \\beta_{N,2} &amp; \\cdots &amp; \\beta_{N,K}\n\\end{bmatrix} $$\n\n其中 $\\beta_{i,j}$ 是资产 $i$ 对因子 $j$ 的暴露。\n\n**步骤2：回归得到因子收益**\n\n$$ \\boldsymbol{\\lambda} = (\\mathbf{B}^T \\mathbf{B})^{-1} \\mathbf{B}^T \\mathbf{R} $$\n\n其中 $\\boldsymbol{\\lambda} \\in \\mathbb{R}^K$ 是 $K$ 个因子的收益向量。\n\n**步骤3：提取Alpha**\n\n$$ \\boldsymbol{\\alpha} = \\mathbf{R} - \\mathbf{B} \\boldsymbol{\\lambda} $$\n\n其中 $\\boldsymbol{\\alpha} \\in \\mathbb{R}^N$ 是 $N$ 个资产的Alpha向量。\n\n**经济含义**：\n\n$$ \\alpha_i = R_i - \\sum_{j=1}^K \\beta_{i,j} \\cdot \\lambda_j $$\n\n$\\alpha_i$ 表示：\n- 剔除了所有风格因子暴露后的纯选股能力\n- 不依赖市场Beta\n- 不依赖规模、价值、动量等风格因子\n- 只捕获&quot;纯Alpha&quot;\n\n### 3.5 Alpha与IC的关系\n\n**命题**：IC是Alpha的横截面相关性。\n\n**证明**：\n\n定义：\n- $F_t$：$t$ 时刻的因子值向量\n- $R_{t \\to t+h}$：$t+h$ 时刻的收益向量\n\nIC的定义：\n\n$$ \\text{IC} = \\text{Corr}(F_t, R_{t \\to t+h}) = \\frac{\\text{Cov}(F_t, R_{t \\to t+h})}{\\sigma_{F_t} \\sigma_{R_{t \\to t+h}}} $$\n\n假设因子已经标准化（$\\sigma_{F_t} = 1$），收益已经中性化（$\\bar{R} = 0$）：\n\n$$ \\text{IC} = \\frac{1}{\\sigma_R} \\cdot \\frac{1}{N} \\sum_{i=1}^N F_{i,t} \\cdot R_{i,t \\to t+h} $$\n\n$$ = \\frac{1}{\\sigma_R} \\cdot \\text{Mean}(F_t \\odot R_{t \\to t+h}) $$\n\n其中 $\\odot$ 是逐元素乘法。\n\n如果因子 $F_t$ 与Alpha $\\alpha_t$ 完全相关（$F_t \\propto \\alpha_t$）：\n\n$$ \\text{IC} \\propto \\text{Mean}(\\alpha_t \\odot R_{t \\to t+h}) $$\n\n由于 $R_{t \\to t+h} = \\alpha_{t \\to t+h} + \\text{Beta} + \\text{Noise}$，且横截面处理已剔除Beta：\n\n$$ \\text{IC} \\propto \\text{Mean}(\\alpha_t \\odot \\alpha_{t \\to t+h}) $$\n\n**结论**：IC度量的是因子与未来Alpha的横截面相关性 $\\quad \\blacksquare$\n\n---\n\n## 4. 评估指标详解\n\n### 4.1 IC（信息系数）\n\n**定义**\n\nIC（Information Coefficient，信息系数）是因子值与未来收益的相关系数。\n\n**公式**：\n\n$$ \\text{IC}_t = \\text{Corr}(F_t, R_{t \\to t+h}) = \\frac{\\text{Cov}(F_t, R_{t \\to t+h})}{\\sigma_{F_t} \\sigma_{R_{t \\to t+h}}} $$\n\n其中：\n- $F_t$：$t$ 时刻的因子值向量\n- $R_{t \\to t+h}$：$t+h$ 时刻的收益向量\n- $\\sigma_{F_t}$：因子值的横截面标准差\n- $\\sigma_{R_{t \\to t+h}}$：收益的横截面标准差\n\n**计算示例**\n\n假设有5只股票：\n\n| 股票 | 因子值 | 未来收益 |\n|------|--------|---------|\n| A | 1.5 | +5% |\n| B | 1.0 | +2% |\n| C | 0.5 | 0% |\n| D | 0.0 | -2% |\n| E | -0.5 | -5% |\n\n**计算均值**：\n\n$$ \\bar{F} = (1.5 + 1.0 + 0.5 + 0.0 - 0.5) / 5 = 0.5 $$\n$$ \\bar{R} = (5\\% + 2\\% + 0\\% - 2\\% - 5\\%) / 5 = 0\\% $$\n\n**计算标准差**：\n\n$$ \\sigma_F = \\sqrt{((1.5-0.5)^2 + (1.0-0.5)^2 + \\cdots + (-0.5-0.5)^2) / 5} = 0.707 $$\n$$ \\sigma_R = \\sqrt{((5\\%-0)^2 + (2\\%-0)^2 + \\cdots + (-5\\%-0)^2) / 5} = 3.74\\% $$\n\n**计算协方差**：\n\n$$ \\text{Cov} = \\frac{1}{5} \\sum_{i=1}^5 (F_i - \\bar{F})(R_i - \\bar{R}) $$\n\n$$ = \\frac{1}{5} [(1.5-0.5)(5\\%-0) + (1.0-0.5)(2\\%-0) + \\cdots + (-0.5-0.5)(-5\\%-0)] $$\n\n$$ = \\frac{1}{5} [1 \\cdot 5\\% + 0.5 \\cdot 2\\% + 0 \\cdot 0\\% + (-0.5) \\cdot (-2\\%) + (-1) \\cdot (-5\\%)] $$\n\n$$ = \\frac{1}{5} [5\\% + 1\\% + 0\\% + 1\\% + 5\\%] = \\frac{12\\%}{5} = 2.4\\% $$\n\n**计算IC**：\n\n$$ \\text{IC} = \\frac{\\text{Cov}}{\\sigma_F \\sigma_R} = \\frac{2.4\\%}{0.707 \\times 3.74\\%} = 0.905 $$\n\n**解释**：\n\n- IC = 0.905：因子与未来收益高度正相关\n- IC = 0.05：因子与未来收益弱正相关\n- IC = 0：因子与未来收益不相关\n- IC = -0.05：因子与未来收益弱负相关\n\n**经济含义**：\n\n- IC = 0.05：因子解释了 $0.05^2 = 0.25\\%$ 的收益方差\n- IC = 0.1：因子解释了 $0.1^2 = 1\\%$ 的收益方差\n- IC = 0.2：因子解释了 $0.2^2 = 4\\%$ 的收益方差\n\n**阈值**：\n\n- IC &gt; 0.05：有效的因子\n- IC &gt; 0.08：非常好的因子\n- IC &gt; 0.1：极好的因子（在A股市场中罕见）\n\n**统计显著性检验**：\n\n$$ t = \\text{IC} \\cdot \\sqrt{\\frac{N}{1 - \\text{IC}^2}} $$\n\n其中 $N$ 是横截面股票数。\n\n**示例**：IC = 0.05，N = 500\n\n$$ t = 0.05 \\cdot \\sqrt{\\frac{500}{1 - 0.05^2}} = 0.05 \\cdot \\sqrt{502.5} = 1.12 $$\n\n查t分布表：\n- $t &gt; 2$：显著（95%置信度）\n- $t &gt; 2.58$：非常显著（99%置信度）\n\n本例 $t = 1.12 &lt; 2$，不显著，说明IC = 0.05可能来自随机噪声。\n\n### 4.2 Rank IC（排序相关系数）\n\n**定义**\n\nRank IC是因子排名与收益排名的Spearman相关系数。\n\n**公式**：\n\n$$ \\text{RankIC}_t = \\text{Corr}(\\text{Rank}(F_t), \\text{Rank}(R_{t \\to t+h})) $$\n\n其中 $\\text{Rank}(\\cdot)$ 是排序函数。\n\n**计算示例**\n\n使用前面的例子：\n\n| 股票 | 因子值 | 排名 | 未来收益 | 排名 |\n|------|--------|------|---------|------|\n| A | 1.5 | 5 | +5% | 5 |\n| B | 1.0 | 4 | +2% | 4 |\n| C | 0.5 | 3 | 0% | 3 |\n| D | 0.0 | 2 | -2% | 2 |\n| E | -0.5 | 1 | -5% | 1 |\n\n**计算Spearman相关系数**：\n\n由于排名完全一致，Rank IC = 1.0。\n\n**优势**：\n\n- **对异常值鲁棒**：只看排名，不看具体数值\n- **适合非线性关系**：可以捕捉单调但不线性的关系\n- **适合偏态分布**：因子或收益分布严重偏态时仍然有效\n\n**IC vs Rank IC**：\n\n| 维度 | IC | Rank IC |\n|------|----|----|\n| **计算方式** | Pearson相关 | Spearman相关 |\n| **对异常值** | 敏感 | 鲁棒 |\n| **关系假设** | 线性 | 单调 |\n| **适用场景** | 正态分布 | 任意分布 |\n| **信息损失** | 小 | 大 |\n\n**何时使用Rank IC**：\n\n- 因子或收益分布严重偏态\n- 存在极端异常值\n- 因子与收益的关系是非线性的\n\n### 4.3 IR（信息比率）\n\n**定义**\n\nIR（Information Ratio，信息比率）是IC的均值与标准差之比，度量因子的稳定性。\n\n**公式**：\n\n$$ \\text{IR} = \\frac{\\text{Mean}(\\text{IC})}{\\text{Std}(\\text{IC})} = \\frac{\\bar{\\text{IC}}}{\\sigma_{\\text{IC}}} $$\n\n其中：\n- $\\text{Mean}(\\text{IC})$：IC的时间序列均值\n- $\\text{Std}(\\text{IC})$：IC的时间序列标准差\n\n**计算示例**\n\n假设5个月的IC序列：\n\n| 月份 | IC |\n|------|----|\n| 1 | 0.08 |\n| 2 | 0.06 |\n| 3 | 0.04 |\n| 4 | 0.02 |\n| 5 | 0.00 |\n\n**计算均值**：\n\n$$ \\bar{\\text{IC}} = (0.08 + 0.06 + 0.04 + 0.02 + 0.00) / 5 = 0.04 $$\n\n**计算标准差**：\n\n$$ \\sigma_{\\text{IC}} = \\sqrt{\\frac{(0.08-0.04)^2 + (0.06-0.04)^2 + \\cdots + (0.00-0.04)^2}{5}} = 0.028 $$\n\n**计算IR**：\n\n$$ \\text{IR} = \\frac{0.04}{0.028} = 1.43 $$\n\n**经济含义**：\n\n- IR &gt; 1：非常稳定的因子\n- IR &gt; 0.7：稳定的因子\n- IR &gt; 0.5：可用的因子\n- IR &lt; 0.5：因子不稳定\n\n**IR的重要性**：\n\nIR比IC更重要，因为：\n1. 高IC但低IR：因子表现不稳定，今天IC=0.1，明天IC=-0.05，无法实战\n2. 中IC但高IR：因子表现稳定，IC长期维持在0.03-0.05之间，可以实战\n\n**示例对比**：\n\n| 因子 | IC均值 | IC标准差 | IR | 评价 |\n|------|--------|---------|----|----|\n| A | 0.10 | 0.15 | 0.67 | 不稳定（波动大） |\n| B | 0.05 | 0.05 | 1.00 | 稳定（波动小） |\n| C | 0.03 | 0.02 | 1.50 | 非常稳定 |\n\n**结论**：因子B虽然IC较低，但IR高，实战效果可能更好。\n\n---\n\n## 5. 量化投资哲学探讨\n\n### 5.1 随机游走 vs 趋势跟踪\n\n**随机游走假设（Random Walk Hypothesis）**\n\n$$ P_{t+1} = P_t + \\varepsilon_{t+1} $$\n\n其中 $\\varepsilon_{t+1}$ 是白噪声，期望为0。\n\n**推论**：\n- 价格变化不可预测\n- IC应接近0\n- 策略无法获得超额收益\n- 赚取市场Beta\n\n**趋势跟踪假设（Trend Following Hypothesis）**\n\n$$ P_{t+1} = P_t + \\alpha_t + \\varepsilon_{t+1} $$\n\n其中 $\\alpha_t$ 是趋势信号，期望不为0。\n\n**推论**：\n- 价格存在趋势\n- IC &gt; 0（动量效应）\n- 策略可以获得超额收益\n- 赚取趋势Alpha\n\n**实证证据**：\n\n- **短期（1-5天）**：IC ≈ 0.02-0.04，动量效应弱\n- **中期（5-20天）**：IC ≈ 0.05-0.08，动量效应中等\n- **长期（20-60天）**：IC ≈ 0.08-0.12，动量效应强\n\n**结论**：\n- 价格不完全随机，存在一定的趋势\n- 趋势跟踪策略在A股市场有效\n- 但需要通过横截面处理剥离市场风险\n\n### 5.2 有效市场 vs 套利机会\n\n**有效市场假说（Efficient Market Hypothesis, EMH）**\n\n**强有效市场**：\n- 价格反映所有信息（包括内幕信息）\n- Alpha应消失\n- 无法获得超额收益\n\n**半强有效市场**：\n- 价格反映所有公开信息\n- Alpha应消失\n- 无法通过基本面分析获得超额收益\n\n**弱有效市场**：\n- 价格反映历史价格信息\n- 技术分析无效\n- 但基本面分析可能有效\n\n**行为金融（Behavioral Finance）**\n\n投资者非理性：\n- 过度自信（Overconfidence）\n- 损失厌恶（Loss Aversion）\n- 羊群效应（Herding）\n- 锚定效应（Anchoring）\n\n**推论**：\n- Alpha长期存在\n- 套利机会长期存在\n- 但套利能力有限（风险、成本、流动性）\n\n**A股市场的实证证据**：\n\n- **因子收益**：价值、动量、质量因子长期IC &gt; 0.05\n- **Alpha规模**：每年可获得的Alpha约5%-15%\n- **套利机会**：长期存在，但逐渐消失（市场越来越有效）\n\n**结论**：\n- A股市场不是完全有效的，存在Alpha机会\n- 但Alpha机会有限，需要量化方法系统化挖掘\n- 横截面处理是提取Alpha的关键技术\n\n### 5.3 量化思维总结\n\n**核心思想**：\n\n&gt; &quot;我们无法预测市场涨跌，但可以预测相对强弱。&quot;\n\n**三个层次的思维转变**：\n\n**层次1：承认无知**\n\n- 承认自己无法准确预测宏观经济和大盘波动\n- 不赌国运涨跌，不赌行业轮动\n- 只赌&quot;同一环境下，谁比谁强&quot;\n\n**层次2：寻找秩序**\n\n- 相信即便在乱世或盛世，资产之间总有&quot;好坏之分&quot;\n- 通过横截面处理，剔除市场Beta\n- 寻找稳定的Alpha信号\n\n**层次3：纯化信号**\n\n- 通过标准化和中性化，把那些&quot;搭便车&quot;的收益（行业、市值、大盘涨跌）全部扔掉\n- 只捕捉那一点点代表公司真正竞争力的纯Alpha\n- 通过大数定律，分散风险，稳定收益\n\n**量化投资的优势**：\n\n1. **剥离Beta，追求Alpha**：\n   - 绝对预测依赖市场涨跌\n   - 相对预测剥离市场风险\n\n2. **降低预测难度**：\n   - 不需要预测具体价格\n   - 只需要预测相对强弱\n\n3. **提高容错率**：\n   - 不需要绝对精确\n   - 只需要相关性（IC &gt; 0.05）\n\n4. **系统化风险管理**：\n   - 通过分散投资降低特质风险\n   - 通过对冲降低系统性风险\n\n**量化投资的本质**：\n\n量化投资不是&quot;猜大小&quot;，而是：\n- 系统化地挖掘Alpha\n- 科学化地管理风险\n- 工程化地执行交易\n\n横截面标准化不是数学技巧，而是一种投资策略：\n&gt; 我们不赌国运涨跌，也不赌行业轮动，我们只赌&quot;同一环境下，谁比谁强&quot;。\n\n---\n\n## 总结\n\n从&quot;预测绝对价格&quot;到&quot;预测相对强弱&quot;是量化投资思维的核心转变。这个转变不仅是技术层面的，更是哲学层面的质变。\n\n### 核心要点回顾\n\n1. **绝对预测 vs 相对预测**：\n   - 绝对预测：预测价格/收益的绝对值\n   - 相对预测：预测横截面上的相对强弱\n   - 相对预测更稳定、容错率更高\n\n2. **市场噪声过滤**：\n   - 横截面处理可以完全消除系统性风险\n   - 只保留特质收益Alpha和部分噪声\n   - 大大降低预测难度\n\n3. **Alpha/Beta分解**：\n   - 总收益 = Alpha + Beta + Noise\n   - Alpha = 选股能力 + 择时能力 + 执行能力\n   - 横截面处理剥离Beta，只保留Alpha\n\n4. **评估指标**：\n   - IC：因子与未来收益的相关系数\n   - Rank IC：因子排名与收益排名的相关系数\n   - IR：IC的稳定性（均值/标准差）\n\n5. **量化投资哲学**：\n   - 承认无知：无法预测市场涨跌\n   - 寻找秩序：资产之间总有&quot;好坏之分&quot;\n   - 纯化信号：只捕获纯Alpha\n\n### 实践建议\n\n**对于因子研究**：\n- 关注IC和IR，而不是绝对收益\n- 横截面标准化和中性化是必须的\n- 选择稳定的因子（高IR），而不仅仅是高IC\n\n**对于策略构建**：\n- 市场中性：剥离Beta，只赚Alpha\n- 分散投资：通过大数定律降低风险\n- 严格止损：控制下行风险\n\n**对于投资哲学**：\n- 不赌国运，只赌&quot;谁比谁强&quot;\n- 承认市场有效性的局限，但Alpha长期存在\n- 量化投资是科学+工程，不是赌大小\n\n在下一文档中，我们将探讨Qlib特征工程的实践指南，包括特征张量优化、Pipeline配置、因子质量评估、数据泄漏防护等内容。"},"quant/qlib/week1/05-qlib特征工程实践指南":{"slug":"quant/qlib/week1/05-qlib特征工程实践指南","filePath":"quant/qlib/week1/05-qlib特征工程实践指南.md","title":"05-qlib特征工程实践指南","links":[],"tags":[],"content":"Qlib特征工程实践指南\n引言\n前四个文档我们从理论和数学层面深入探讨了Qlib特征工程的核心概念：全景概览、横截面标准化与中性化、Horizon对齐、相对强弱预测的量化思维。\n本文将从实践角度出发，提供完整的Qlib特征工程实践指南，包括特征张量优化、Pipeline配置、因子质量评估、数据泄漏防护，以及链上数据集成的深度实践。\n\n1. 特征张量优化\n1.1 内存优化策略\n数据类型优化\n在量化因子计算中，数据的精度要求通常不高，可以通过降低数据类型来节省内存。\n优化示例：\n# 优化前：float64（8字节）\nimport numpy as np\nfactor = np.random.randn(5000, 1000).astype(&#039;float64&#039;)  # 5000只股票 x 1000天\nmemory = factor.nbytes / 1024**2  # 38.15 MB\n \n# 优化后：float32（4字节）\nfactor = factor.astype(&#039;float32&#039;)\nmemory = factor.nbytes / 1024**2  # 19.07 MB（节省50%）\n时间戳优化：\n# 优化前：datetime64[ns]（8字节）\ndates = pd.date_range(&#039;2020-01-01&#039;, &#039;2023-12-31&#039;)\nmemory = dates.nbytes / 1024**2  # 0.01 MB（小数据不显著）\n \n# 优化后：int64（时间戳）\ntimestamps = dates.astype(&#039;int64&#039;)  # Unix时间戳\nmemory = timestamps.nbytes / 1024**2  # 0.01 MB\n稀疏矩阵应用\n对于稀疏数据（如行业哑变量、指数衰减权重），使用稀疏矩阵可以大幅节省内存。\n示例：行业哑变量\nfrom scipy.sparse import csr_matrix\n \n# 优化前：密集矩阵\nindustry_dummy = np.zeros((5000, 30))  # 5000只股票 x 30个行业\n# 每个股票只属于1个行业，稀疏度约96.7%\nmemory = industry_dummy.nbytes / 1024**2  # 1.14 MB\n \n# 优化后：稀疏矩阵\nrow_indices = [0, 1, 2, ..., 4999]  # 股票索引\ncol_indices = [5, 10, 3, ..., 12]   # 行业索引\ndata = np.ones(5000)\n \nindustry_sparse = csr_matrix((data, (row_indices, col_indices)), shape=(5000, 30))\nmemory = industry_sparse.data.nbytes / 1024**2 + \\\n         industry_sparse.indptr.nbytes / 1024**2 + \\\n         industry_sparse.indices.nbytes / 1024**2  # 0.09 MB（节省92%）\n分片加载\n对于大规模因子库（如10,000个因子，5000只股票，10年数据），分片加载是必要的。\n策略1：按时间分片\n# 将10年数据分成10个年度分片\nyears = [&#039;2020&#039;, &#039;2021&#039;, &#039;2022&#039;, &#039;2023&#039;]\nfor year in years:\n    start_time = f&quot;{year}-01-01&quot;\n    end_time = f&quot;{year}-12-31&quot;\n    factors = D.features(instruments, factor_exprs, start_time, end_time)\n    # 处理当前年度数据\n策略2：按资产分片\n# 将5000只股票分成10个分片\nchunks = list(range(0, 5000, 500))  # [0, 500, 1000, ..., 4500]\nfor i in range(len(chunks)-1):\n    start_idx = chunks[i]\n    end_idx = chunks[i+1]\n    chunk_instruments = instruments[start_idx:end_idx]\n    factors = D.features(chunk_instruments, factor_exprs, start_time, end_time)\n    # 处理当前分片数据\n1.2 计算优化\n向量化操作\n避免使用for循环，使用numpy/pandas的向量化操作。\n慢示例：\n# 慢：for循环\ndef slow_rolling_mean(data, window):\n    result = np.zeros_like(data)\n    for i in range(window, len(data)):\n        result[i] = data[i-window:i].mean()\n    return result\n \n%timeit slow_rolling_mean(price, 20)\n# 输出：100 ms ± 5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n快示例：\n# 快：向量化\ndef fast_rolling_mean(data, window):\n    return data.rolling(window).mean()\n \n%timeit fast_rolling_mean(price, 20)\n# 输出：1 ms ± 0.1 ms per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n并行化\n对于大规模因子计算，使用多进程并行化。\n示例：\nfrom multiprocessing import Pool\n \ndef compute_factor(args):\n    stock_id, start_time, end_time = args\n    # 计算单个股票的因子\n    factor = D.features([stock_id], [factor_expr], start_time, end_time)\n    return factor\n \n# 串行（慢）\nfor stock_id in instruments:\n    factor = compute_factor((stock_id, start_time, end_time))\n \n# 并行（快）\nargs = [(stock_id, start_time, end_time) for stock_id in instruments]\nwith Pool(8) as p:  # 8个进程\n    results = p.map(compute_factor, args)\n滚动窗口优化\n对于滚动窗口计算，使用增量计算避免重复计算。\n慢示例：\n# 慢：每次重新计算\ndef slow_rolling_incremental(data, window):\n    result = np.zeros_like(data)\n    for i in range(window, len(data)):\n        result[i] = data[i-window:i].mean()  # 重复计算\n    return result\n快示例：\n# 快：增量计算\ndef fast_rolling_incremental(data, window):\n    result = np.zeros_like(data)\n    # 计算第一个窗口\n    result[window-1] = data[:window].mean()\n \n    # 增量更新\n    for i in range(window, len(data)):\n        # 减去离开窗口的值，加上进入窗口的值\n        result[i] = result[i-1] + (data[i] - data[i-window]) / window\n \n    return result\n \n# 证明正确性\n# result[i] = result[i-1] + (data[i] - data[i-window]) / window\n# = mean(data[i-window:i-1]) + (data[i] - data[i-window]) / window\n# = (sum(data[i-window:i-1]) + data[i] - data[i-window]) / window\n# = sum(data[i-window+1:i]) / window\n# = mean(data[i-window+1:i+1])\n# = mean(data[i-window+1:i])  # 下一轮的窗口\n1.3 大规模因子库管理\n存储方案\n方案1：HDF5\n适合中小规模因子库（10K因子）。\nimport h5py\n \n# 创建HDF5文件\nwith h5py.File(&#039;factor_library.h5&#039;, &#039;w&#039;) as f:\n    # 创建数据集\n    f.create_dataset(&#039;factor1&#039;, data=factor1, compression=&#039;gzip&#039;)\n    f.create_dataset(&#039;factor2&#039;, data=factor2, compression=&#039;gzip&#039;)\n \n    # 创建属性\n    f[&#039;factor1&#039;].attrs[&#039;name&#039;] = &#039;momentum_20&#039;\n    f[&#039;factor1&#039;].attrs[&#039;ic_mean&#039;] = 0.05\n    f[&#039;factor1&#039;].attrs[&#039;ir&#039;] = 1.2\n \n# 读取因子\nwith h5py.File(&#039;factor_library.h5&#039;, &#039;r&#039;) as f:\n    factor1 = f[&#039;factor1&#039;][:]\n    ic_mean = f[&#039;factor1&#039;].attrs[&#039;ic_mean&#039;]\n方案2：Parquet\n适合大规模因子库（100K+因子）。\nimport pyarrow.parquet as pq\nimport pandas as pd\n \n# 保存因子（Parquet格式）\nfactor_df = pd.DataFrame({\n    &#039;date&#039;: dates,\n    &#039;instrument&#039;: instruments,\n    &#039;factor1&#039;: factor1,\n    &#039;factor2&#039;: factor2\n})\nfactor_df.to_parquet(&#039;factor_library.parquet&#039;, engine=&#039;pyarrow&#039;)\n \n# 读取因子\nfactor_df = pd.read_parquet(&#039;factor_library.parquet&#039;)\n索引策略\n复合索引：\n# 设置复合索引\nfactor_df.set_index([&#039;date&#039;, &#039;instrument&#039;], inplace=True)\n \n# 快速查询\nfactor = factor_df.loc[&#039;2023-01-01&#039;, &#039;000001.SZ&#039;]\nB树索引：\n# 对于HDF5\nwith h5py.File(&#039;factor_library.h5&#039;, &#039;a&#039;) as f:\n    # 创建软链接作为索引\n    f.create_dataset(&#039;date_index&#039;, data=dates)\n    f.create_dataset(&#039;instrument_index&#039;, data=instruments)\n\n2. Pipeline配置模板\n2.1 基础模板：行情因子\n# config/baseline.yaml\n \n# 数据源\ndata:\n  provider: &quot;qlib.data.LocalFileProvider&quot;\n  uri: &quot;data/qlib/qlib_data/cn_data&quot;\n  region: &quot;cn&quot;\n \n# 因子定义\nfactors:\n  - name: &quot;ma_5&quot;\n    expr: &quot;Mean($close, 5)&quot;\n    description: &quot;5日均线&quot;\n \n  - name: &quot;ma_20&quot;\n    expr: &quot;Mean($close, 20)&quot;\n    description: &quot;20日均线&quot;\n \n  - name: &quot;momentum_10&quot;\n    expr: &quot;($close / Ref($close, 10)) - 1&quot;\n    description: &quot;10日动量&quot;\n \n# 标签\nlabel:\n  expr: &quot;Ref($close, -5) / $close - 1&quot;\n  horizon: 5\n  description: &quot;未来5日收益率&quot;\n \n# 标准化\nnormalization:\n  type: &quot;zscore&quot;\n  axis: &quot;cross_section&quot;\n \n# 回测配置\nbacktest:\n  start_time: &quot;2020-01-01&quot;\n  end_time: &quot;2023-12-31&quot;\n  rebalance_freq: &quot;5d&quot;\n  top_k: 100  # 选Top 100只股票\n2.2 进阶模板：多因子组合\n# config/advanced.yaml\n \n# 数据源\ndata:\n  provider: &quot;qlib.data.LocalFileProvider&quot;\n  uri: &quot;data/qlib/qlib_data/cn_data&quot;\n \n# 因子定义\nfactors:\n  # 技术因子\n  - name: &quot;rsi_14&quot;\n    expr: &quot;RSI($close, 14)&quot;\n    category: &quot;technical&quot;\n \n  - name: &quot;boll_upper&quot;\n    expr: &quot;Mean($close, 20) + 2 * Std($close, 20)&quot;\n    category: &quot;technical&quot;\n \n  - name: &quot;boll_lower&quot;\n    expr: &quot;Mean($close, 20) - 2 * Std($close, 20)&quot;\n    category: &quot;technical&quot;\n \n  # 动量因子\n  - name: &quot;momentum_20&quot;\n    expr: &quot;($close / Ref($close, 20)) - 1&quot;\n    category: &quot;momentum&quot;\n \n  - name: &quot;price_acceleration&quot;\n    expr: &quot;Ref($close, -1) / $close - Ref($close, -20) / Ref($close, -21)&quot;\n    category: &quot;momentum&quot;\n \n  # 波动因子\n  - name: &quot;volatility_20&quot;\n    expr: &quot;Std($close, 20) / Mean($close, 20)&quot;\n    category: &quot;volatility&quot;\n \n  - name: &quot;atr_14&quot;\n    expr: &quot;ATR($high, $low, $close, 14)&quot;\n    category: &quot;volatility&quot;\n \n  # 流动性因子\n  - name: &quot;turnover_rate&quot;\n    expr: &quot;$turnover&quot;\n    category: &quot;liquidity&quot;\n \n  - name: &quot;volume_ratio&quot;\n    expr: &quot;$volume / Mean($volume, 20)&quot;\n    category: &quot;liquidity&quot;\n \n# 因子组合\ncombination:\n  method: &quot;ic_weighted&quot;  # IC加权\n  weights: &quot;auto&quot;  # 自动学习\n  min_ic: 0.03  # 最小IC阈值\n \n# 中性化\nneutralization:\n  - &quot;industry&quot;  # 行业中性\n  - &quot;market_cap&quot;  # 市值中性\n \n# 标准化\nnormalization:\n  type: &quot;zscore&quot;\n  axis: &quot;cross_section&quot;\n  method: &quot;robust&quot;  # 鲁棒标准化（对异常值不敏感）\n \n# 回测配置\nbacktest:\n  start_time: &quot;2020-01-01&quot;\n  end_time: &quot;2023-12-31&quot;\n  rebalance_freq: &quot;5d&quot;\n  top_k: 50\n  max_weight: 0.02  # 单只股票最大权重2%\n\n3. 因子质量评估流程\n3.1 IC/IR分析流程\ndef evaluate_factor(factor_name, start_date, end_date, horizon=5):\n    &quot;&quot;&quot;\n    评估因子的IC/IR\n    &quot;&quot;&quot;\n    # 1. 加载数据\n    factor = load_factor(factor_name, start_date, end_date)\n    price = load_price(start_date, end_date)\n \n    # 2. 计算收益率\n    returns = price.pct_change(horizon).shift(-horizon)\n \n    # 3. 对齐数据\n    data = pd.DataFrame({\n        &#039;factor&#039;: factor,\n        &#039;return&#039;: returns\n    }).dropna()\n \n    # 4. 计算IC序列\n    ic_series = []\n    for date in data.index.get_level_values(&#039;date&#039;).unique():\n        subset = data.loc[date]\n        ic = subset[&#039;factor&#039;].corr(subset[&#039;return&#039;])\n        ic_series.append(ic)\n \n    ic_series = pd.Series(ic_series)\n \n    # 5. 统计指标\n    ic_mean = ic_series.mean()\n    ic_std = ic_series.std()\n    ir = ic_mean / ic_std if ic_std &gt; 0 else 0\n \n    # 6. IC显著性检验\n    n = len(data.loc[date])  # 横截面股票数\n    t_stat = ic_mean * np.sqrt(n / (1 - ic_mean**2))\n    p_value = 2 * (1 - t.cdf(abs(t_stat), df=n-2))\n \n    # 7. 分组回测\n    quintile_returns = quintile_backtest(data, n_groups=5)\n \n    return {\n        &#039;ic_mean&#039;: ic_mean,\n        &#039;ic_std&#039;: ic_std,\n        &#039;ir&#039;: ir,\n        &#039;t_stat&#039;: t_stat,\n        &#039;p_value&#039;: p_value,\n        &#039;quintile_returns&#039;: quintile_returns\n    }\n3.2 衰减周期测试\ndef decay_analysis(factor_name, max_horizon=60):\n    &quot;&quot;&quot;\n    测试因子在不同Horizon下的IC\n    &quot;&quot;&quot;\n    ic_by_horizon = []\n \n    for h in range(1, max_horizon+1):\n        ic = compute_ic(factor_name, horizon=h)\n        ic_by_horizon.append(ic)\n \n    # 找到最大IC的Horizon\n    best_h = np.argmax(ic_by_horizon) + 1\n \n    # 计算衰减率\n    decay_rate = compute_decay_rate(ic_by_horizon)\n \n    return {\n        &#039;ic_by_horizon&#039;: ic_by_horizon,\n        &#039;best_horizon&#039;: best_h,\n        &#039;decay_rate&#039;: decay_rate\n    }\n \ndef compute_decay_rate(ic_series, threshold=0.5):\n    &quot;&quot;&quot;\n    计算衰减率：IC从最大值衰减到50%所需的时间\n    &quot;&quot;&quot;\n    max_ic = max(ic_series)\n    half_max_ic = max_ic * threshold\n \n    for i, ic in enumerate(ic_series):\n        if ic &lt; half_max_ic:\n            return i + 1  # 返回衰减到50%所需的Horizon\n \n    return len(ic_series)  # 如果没有衰减到50%，返回总长度\n3.3 相关性矩阵与去重\ndef factor_correlation_analysis(factor_list, threshold=0.9):\n    &quot;&quot;&quot;\n    因子相关性分析与去重\n    &quot;&quot;&quot;\n    # 计算相关系数矩阵\n    corr_matrix = pd.DataFrame(index=factor_list, columns=factor_list)\n \n    for i, f1 in enumerate(factor_list):\n        for j, f2 in enumerate(factor_list):\n            if i &lt;= j:\n                corr = compute_correlation(f1, f2)\n                corr_matrix.loc[f1, f2] = corr\n                corr_matrix.loc[f2, f1] = corr\n \n    # 层次聚类去重\n    from scipy.cluster.hierarchy import linkage, fcluster\n    from scipy.spatial.distance import squareform\n \n    # 将相关系数转换为距离\n    distance_matrix = 1 - corr_matrix.values\n    distance_vector = squareform(distance_matrix)\n \n    # 层次聚类\n    Z = linkage(distance_vector, method=&#039;average&#039;)\n    clusters = fcluster(Z, t=threshold, criterion=&#039;distance&#039;)\n \n    # 每个聚类选择IC最高的因子\n    cluster_dict = {}\n    for i, cluster_id in enumerate(clusters):\n        if cluster_id not in cluster_dict:\n            cluster_dict[cluster_id] = []\n        cluster_dict[cluster_id].append(factor_list[i])\n \n    selected_factors = []\n    for cluster_id, factors in cluster_dict.items():\n        best_factor = max(factors, key=lambda f: get_ic(f))\n        selected_factors.append(best_factor)\n \n    return {\n        &#039;selected_factors&#039;: selected_factors,\n        &#039;corr_matrix&#039;: corr_matrix,\n        &#039;clusters&#039;: cluster_dict\n    }\n\n4. 数据泄漏防护\n4.1 常见泄漏场景\n场景1：未来数据泄露\n# 错误：计算t时刻的波动率，使用了未来数据\ndef wrong_volatility(price, window):\n    result = np.zeros_like(price)\n    for i in range(len(price)):\n        result[i] = price[i:i+window].std()  # 包含i+1到i+window-1的数据\n    return result\n \n# 正确：只使用历史数据\ndef correct_volatility(price, window):\n    result = np.zeros_like(price)\n    for i in range(window-1, len(price)):\n        result[i] = price[i-window+1:i+1].std()  # 只使用i-window+1到i的数据\n    return result\n \n# 更优：使用Qlib的Rolling算子\ndef qlib_volatility():\n    return Expression(Std($close, 20))  # Qlib自动处理时序\n场景2：样本外信息混入\n# 错误：在整个数据集上标准化\ndef wrong_standardization(X_train, X_test):\n    X_all = np.concatenate([X_train, X_test], axis=0)\n    scaler = StandardScaler().fit(X_all)  # 使用了测试集信息\n    X_train_scaled = scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n \n# 正确：只在训练集上标准化\ndef correct_standardization(X_train, X_test):\n    scaler = StandardScaler().fit(X_train)  # 只使用训练集信息\n    X_train_scaled = scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    return X_train_scaled, X_test_scaled\n4.2 检测方法\ndef detect_lookahead_bias(factor_df, price_df, horizon=5):\n    &quot;&quot;&quot;\n    检测Look-ahead Bias\n    &quot;&quot;&quot;\n    # 方法1：检查因子是否与未来价格相关\n    lookahead_corr = []\n \n    for lag in [-1, -2, -5, -10]:\n        shifted_factor = factor_df.shift(lag)\n        corr = shifted_factor.corrwith(price_df)\n        lookahead_corr.append(corr)\n \n    # 如果shift(-1)相关性显著 &gt; IC，说明有泄漏\n    ic_mean = compute_ic(factor_df, price_df, horizon=horizon)\n    if lookahead_corr[0].mean() &gt; 2 * ic_mean:\n        warn(&quot;Potential lookahead bias detected!&quot;)\n        return False\n \n    # 方法2：检查因子分布突变\n    factor_diff = factor_df.diff().abs()\n    threshold = factor_diff.std() * 5\n    if (factor_diff &gt; threshold).any():\n        warn(&quot;Factor has extreme jumps, check for data leaks!&quot;)\n        return False\n \n    return True\n4.3 最佳实践清单\n\n 因子计算只用历史数据（t 时刻的因子只用 &lt;t 的数据）\n 标签使用shift(-h)对齐（将未来收益对齐到当前时刻）\n 测试集严格在训练集之后（时间序列划分）\n 避免使用未来统计量（Rolling、Lag算子）\n 定期进行泄漏检测（detect_lookahead_bias）\n 审计日志记录每一步操作\n 样本外严格验证（不使用样本内信息调参）\n\n\n5. 链上数据集成\n5.1 链上数据特征工程挑战\n与传统的金融数据相比，链上数据有以下独特挑战：\n挑战1：数据频率极高\n\n区块链交易是7×24小时实时发生的\n数据频率可能达到秒级甚至毫秒级\n传统Qlib框架主要用于日频数据，需要适配\n\n挑战2：数据维度复杂\n\nToken级别（类似股票代码）\n地址级别（用户行为）\n协议级别（DeFi生态）\n交易级别（逐笔分析）\n\n挑战3：数据质量参差不齐\n\n部分数据缺失（如隐私币、跨链桥）\n数据清洗困难（Sybil攻击、女巫攻击）\n数据来源多样（RPC、The Graph、Dune Analytics）\n\n挑战4：链上链下融合\n\n需要将链上数据与链下数据融合\n时间戳对齐困难（区块链时间 vs 现实时间）\n不同链的数据格式不统一\n\n5.2 DeFi数据映射到Qlib（Token级别）\nToken级别的数据映射\n将Token视为”股票”，链上指标视为”因子”：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQlib概念链上概念示例股票代码Token地址或SymbolETH, BTC, UNI, AAVE行业链Ethereum, BSC, Polygon市值Token市值Market Cap成交量交易量Volume价格DEX价格Price from Uniswap\nDEX Swap行为因子\n因子1：Swap Volume Momentum（交易量动量）\n# Qlib Expression伪代码\nswap_momentum = (\n    Ref($swap_volume, -5) / $swap_volume - 1\n)\n金融含义：\n\n如果交易量在5天内增长了50%，说明市场关注度高\n动量因子通常为正，说明未来可能继续上涨\n\n因子2：Price Velocity（价格变化速率）\n# Qlib Expression伪代码\nprice_velocity = (\n    ($price / Ref($price, 1)) - 1\n) / sqrt(1 + $gas_price)  # 归一化Gas成本\n金融含义：\n\n价格变化速率考虑了Gas成本（交易成本）\n如果价格涨了10%，但Gas成本很高，实际净收益可能很低\n\n因子3：Liquidity Depth（流动性深度）\n# Qlib Expression伪代码\nliquidity_depth = (\n    $liquidity_0side + $liquidity_1side\n) / $price\n金融含义：\n\n流动性深度越大，滑点越小，交易成本越低\n流动性深度是DeFi协议健康度的重要指标\n\n因子4：Slippage Ratio（滑点比率）\n# Qlib Expression伪代码\nslippage = (\n    ($price - $twap_price) / $twap_price\n) * $trade_size\n金融含义：\n\n滑点比率衡量大额交易对价格的影响\n滑点大说明流动性差，不适合大额交易\n\n5.3 链上指标集成（地址级别→聚合）\n地址级别的因子\n因子1：Daily Active Addresses（DAA）动量\n# Qlib Expression伪代码\ndaa_momentum = (\n    Ref($daily_active_addresses, -7) /\n    $daily_active_addresses - 1\n)\n金融含义：\n\n活跃地址数7天内增长了50%，说明用户参与度高\nDAA是网络效应的重要指标\n\n因子2：Transaction Velocity（交易活跃度）\n# Qlib Expression伪代码\ntx_velocity = (\n    $transaction_count / $total_addresses\n)\n金融含义：\n\n每个地址的平均交易次数\n交易活跃度高说明Token流通性好\n\n因子3：Whale Activity（大户活动）\n# Qlib Expression伪代码\nwhale_activity = (\n    sum($transaction_size where $transaction_size &gt; 1000 ETH) /\n    $total_volume\n)\n金融含义：\n\n大户（Whale）交易占比\n大户活动多可能是”聪明钱”进场\n\n5.4 协议级别因子\n因子1：TVL（Total Value Locked）增长\n# Qlib Expression伪代码\ntvl_growth = (\n    Ref($protocol_tvl, -30) / $protocol_tvl - 1\n)\n金融含义：\n\nTVL 30天内增长了50%，说明协议发展迅速\nTVL是DeFi协议健康度的核心指标\n\n因子2：Staking APY（质押收益率）\n# Qlib Expression伪代码\napy_factor = (\n    $staking_apy / $risk_free_rate\n)\n金融含义：\n\n质押收益率相对于无风险收益率的倍数\nAPY高说明协议激励性强，可能吸引更多用户\n\n5.5 交易级别因子（高频分析）\n因子1：Order Book Imbalance（订单簿失衡）\n# Qlib Expression伪代码\norder_book_imbalance = (\n    ($bid_volume - $ask_volume) /\n    ($bid_volume + $ask_volume)\n)\n金融含义：\n\n订单簿失衡=1表示全是买单，= -1表示全是卖单\n订单簿失衡大说明市场情绪强（买方或卖方压倒性优势）\n\n因子2：Trade Size Distribution（交易规模分布）\n# Qlib Expression伪代码\nwhale_ratio = (\n    sum($trade_size where $trade_size &gt; 1000 ETH) /\n    $total_volume\n)\n金融含义：\n\n大额交易占比\n大额交易多可能是”聪明钱”或”鲸鱼”活动\n\n5.6 链上-传统数据融合\n融合案例1：CEX-DEX套利因子\n# Qlib Expression伪代码\n# 交易所价差\nexchange_spread = (\n    abs($cex_price - $dex_price) / $dex_price\n)\n \n# 套利信号（考虑流动性）\narbitrage_signal = (\n    exchange_spread *\n    min($cex_liquidity, $dex_liquidity) /  # 流动性限制\n    (1 + $gas_price)  # Gas成本\n)\n金融含义：\n\nCEX和DEX价差大，说明有套利机会\n但需要考虑流动性限制和Gas成本\n如果价差=5%，但Gas成本=2%，净套利收益=3%\n\n融合案例2：On-chain Momentum + Off-chain Volume\n# Qlib Expression伪代码\n# 混合动量因子\nhybrid_momentum = (\n    0.6 * ($on_chain_return) +\n    0.4 * ($off_chain_volume / Ref($off_chain_volume, -1))\n)\n金融含义：\n\n链上收益占60%，链下成交量占40%\n结合链上和链下信息，提高因子稳定性\n\n融合案例3：Cross-chain Arbitrage（跨链套利）\n# Qlib Expression伪代码\n# 跨链价差\nchain_spread = (\n    abs($eth_chain_price - $bsc_chain_price) /\n    $eth_chain_price\n)\n \n# 跨桥时间\nbridge_time = ($block_time_target - $block_time_source)\n \n# 跨桥费用\nbridge_fee = $bridge_gas * $gas_price\n \n# 套利收益\narbitrage_profit = (\n    chain_spread -\n    bridge_fee / $trade_size -  # 跨桥费用\n    bridge_time * $time_decay  # 时间衰减\n)\n金融含义：\n\n跨链价差大，说明有套利机会\n但需要考虑跨桥时间、跨桥费用\n如果价差=10%，但跨桥费用=5%，跨桥时间=10小时（时间衰减=2%），净套利收益=3%\n\n5.7 链上数据集成实践总结\n链上数据集成的三个层次：\n层次1：Token级别（类似股票）\n\n将Token映射为”股票代码”\n链上指标映射为”因子”\n适合传统量化框架\n\n层次2：地址级别（用户行为）\n\n将地址行为聚合为Token级别因子\n关注大户、活跃地址、交易活跃度\n适合用户行为分析\n\n层次3：协议级别（DeFi生态）\n\n将协议指标映射为”行业因子”\n关注TVL、APY、使用率\n适合DeFi生态分析\n\n链上数据集成的四个维度：\n维度1：Token级别（资产维度）\n\n价格、市值、成交量\nDEX Swap行为\n跨链套利机会\n\n维度2：地址级别（用户维度）\n\n活跃地址数\n大户活动\n用户留存率\n\n维度3：协议级别（生态维度）\n\nTVL增长\nAPY激励\n使用率\n\n维度4：交易级别（微观维度）\n\n订单簿失衡\n交易规模分布\n滑点分析\n\n链上数据融合的三个方向：\n方向1：链上-链下融合\n\nCEX-DEX价差\n链上收益 + 链下成交量\n跨链套利\n\n方向2：多链融合\n\nEthereum + BSC + Polygon\n跨链价差\n跨链流动性\n\n方向3：链上-链上融合\n\nToken价格 + 流动性\nDEX Swap + 链上指标\n协议TVL + APY\n\n\n6. 完整项目模板\n6.1 目录结构\nqlib-factor-project/\n├── config/                    # 配置文件\n│   ├── baseline.yaml         # 基础模板\n│   ├── advanced.yaml         # 进阶模板\n│   └── on_chain.yaml         # 链上数据模板\n├── data/                      # 数据目录\n│   ├── qlib_data/            # Qlib数据\n│   └── on_chain_data/        # 链上数据\n├── factors/                   # 因子定义\n│   ├── expressions.py        # Qlib表达式\n│   ├── on_chain_factors.py   # 链上因子\n│   └── traditional_factors.py # 传统因子\n├── evaluation/                # 评估模块\n│   ├── ic_analysis.py        # IC分析\n│   ├── backtest.py           # 回测\n│   └── report.py             # 报告生成\n├── utils/                     # 工具函数\n│   ├── data_loader.py        # 数据加载\n│   ├── normalization.py      # 标准化\n│   └── leakage_detector.py   # 泄漏检测\n├── logs/                      # 日志目录\n└── main.py                   # 主程序\n\n6.2 主程序模板\n# main.py\n \nfrom qlib import init\nfrom factors.expressions import load_traditional_factors\nfrom factors.on_chain_factors import load_on_chain_factors\nfrom evaluation.ic_analysis import evaluate_all_factors\nfrom evaluation.report import generate_report\n \ndef main():\n    # 1. 初始化Qlib\n    init(provider_uri=&quot;data/qlib/qlib_data/cn_data&quot;)\n \n    # 2. 加载传统因子\n    traditional_factors = load_traditional_factors(\n        config_path=&quot;config/advanced.yaml&quot;\n    )\n \n    # 3. 加载链上因子\n    on_chain_factors = load_on_chain_factors(\n        tokens=[&quot;ETH&quot;, &quot;BTC&quot;, &quot;UNI&quot;, &quot;AAVE&quot;],\n        config_path=&quot;config/on_chain.yaml&quot;\n    )\n \n    # 4. 合并因子\n    all_factors = {**traditional_factors, **on_chain_factors}\n \n    # 5. 评估因子\n    results = evaluate_all_factors(\n        factors=all_factors,\n        start_time=&quot;2020-01-01&quot;,\n        end_time=&quot;2023-12-31&quot;\n    )\n \n    # 6. 生成报告\n    generate_report(results, output_path=&quot;logs/factor_report.html&quot;)\n \n    print(&quot;Factor evaluation completed!&quot;)\n \nif __name__ == &quot;__main__&quot;:\n    main()\n6.3 配置文件模板\n# config/on_chain.yaml\n \n# 数据源\ndata:\n  on_chain:\n    source: &quot;dune_api&quot;  # 或 &quot;the_graph&quot;, &quot;custom_rpc&quot;\n    chains: [&quot;ethereum&quot;, &quot;bsc&quot;, &quot;polygon&quot;]\n    tokens: [&quot;ETH&quot;, &quot;BTC&quot;, &quot;UNI&quot;, &quot;AAVE&quot;, &quot;LINK&quot;]\n \n  metrics:\n    # Token级别\n    - name: &quot;swap_volume&quot;\n      type: &quot;token_level&quot;\n      frequency: &quot;1d&quot;\n \n    - name: &quot;liquidity_depth&quot;\n      type: &quot;token_level&quot;\n      frequency: &quot;1d&quot;\n \n    # 地址级别\n    - name: &quot;daily_active_addresses&quot;\n      type: &quot;address_level&quot;\n      frequency: &quot;1d&quot;\n \n    - name: &quot;whale_activity&quot;\n      type: &quot;address_level&quot;\n      frequency: &quot;1d&quot;\n \n    # 协议级别\n    - name: &quot;protocol_tvl&quot;\n      type: &quot;protocol_level&quot;\n      frequency: &quot;1d&quot;\n \n# 因子定义\nfactors:\n  - name: &quot;swap_volume_momentum&quot;\n    expr: &quot;Ref($swap_volume, -5) / $swap_volume - 1&quot;\n    category: &quot;momentum&quot;\n \n  - name: &quot;liquidity_depth_ratio&quot;\n    expr: &quot;$liquidity / $market_cap&quot;\n    category: &quot;liquidity&quot;\n \n  - name: &quot;daa_momentum&quot;\n    expr: &quot;Ref($daily_active_addresses, -7) / $daily_active_addresses - 1&quot;\n    category: &quot;on_chain&quot;\n \n  - name: &quot;whale_activity_ratio&quot;\n    expr: &quot;$whale_volume / $total_volume&quot;\n    category: &quot;on_chain&quot;\n \n  - name: &quot;tvl_growth&quot;\n    expr: &quot;Ref($protocol_tvl, -30) / $protocol_tvl - 1&quot;\n    category: &quot;protocol&quot;\n \n# 因子组合\ncombination:\n  on_chain_weight: 0.4\n  off_chain_weight: 0.6\n  method: &quot;ic_weighted&quot;\n \n# 回测配置\nbacktest:\n  start_time: &quot;2022-01-01&quot;\n  end_time: &quot;2023-12-31&quot;\n  rebalance_freq: &quot;1d&quot;  # 链上数据变化快，日频调仓\n  min_liquidity: 100000  # 最小流动性\n  max_gas_price: 50  # 最大Gas价格（Gwei）\n \n# 数据泄漏防护\nleakage_protection:\n  enable: true\n  check_lookahead: true\n  check_future_info: true\n  audit_log: true\n\n总结\n本文从实践角度提供了完整的Qlib特征工程指南，涵盖了从数据优化到链上集成的各个层面。\n核心要点回顾\n\n\n特征张量优化：\n\n内存优化：数据类型、稀疏矩阵、分片加载\n计算优化：向量化、并行化、增量计算\n存储优化：HDF5、Parquet、索引策略\n\n\n\nPipeline配置：\n\n基础模板：行情因子\n进阶模板：多因子组合、中性化\n链上模板：多链、多协议、多维度\n\n\n\n因子质量评估：\n\nIC/IR分析：相关性、稳定性、显著性\n衰减周期测试：不同Horizon的IC表现\n相关性矩阵与去重：层次聚类、IC加权\n\n\n\n数据泄漏防护：\n\n常见泄漏场景：未来数据、样本外信息\n检测方法：相关性检验、分布突变检测\n最佳实践清单：7项关键检查\n\n\n\n链上数据集成：\n\nToken级别：DEX Swap行为因子\n地址级别：活跃地址、大户活动\n协议级别：TVL、APY因子\n交易级别：订单簿、滑点分析\n链上-链下融合：CEX-DEX套利、混合因子\n\n\n\n链上数据集成的核心洞察\n链上数据集成不仅是技术问题，更是思维模式的转变：\n从单维度到多维度：\n\n传统：股票（单维度）\n链上：Token × 地址 × 协议 × 交易（四维度）\n\n从静态到动态：\n\n传统：日频数据（静态）\n链上：7×24小时实时数据（动态）\n\n从单一到融合：\n\n传统：传统金融数据（单一）\n链上：链上 × 链下 × 跨链（融合）\n\n从被动到主动：\n\n传统：被动接受数据\n链上：主动构建数据管道（RPC、The Graph、Dune）\n\n实践建议\n对于传统量化：\n\n关注IC/IR，而不是绝对收益\n横截面标准化和中性化是必须的\n严格防护数据泄漏\n\n对于链上量化：\n\n四维度分析：Token × 地址 × 协议 × 交易\n三个融合方向：链上-链下、多链、链上-链上\n动态数据管道：实时数据流处理\n\n对于项目构建：\n\n模块化设计：配置、因子、评估、工具分离\n日志审计：记录每一步操作\n持续迭代：因子库动态更新\n\n未来展望\n随着Web3和DeFi的快速发展，链上数据量化将成为量化投资的新前沿。Qlib的特征工程框架不仅适用于传统金融，也可以扩展到链上数据分析，为量化投资提供系统化的工具和思维。\n横截面标准化不是数学技巧，而是一种投资策略：\n\n我们不赌国运涨跌，也不赌行业轮动，我们只赌”同一环境下，谁比谁强”。\n\n链上数据集成不是技术问题，而是一种新范式：\n\n我们不局限于传统金融数据，我们融合链上、链下、跨链的多维信息。\n\n\n至此，Qlib特征工程的五个核心文档全部完成。从理论基础到实践指南，从传统金融到链上数据，形成了一个完整的知识体系。\n希望这些文档能够帮助你在量化投资的道路上走得更远、更稳。"},"quant/qlib/week1/index":{"slug":"quant/qlib/week1/index","filePath":"quant/qlib/week1/index.md","title":"index","links":["quant/qlib/week1/01-qlib特征工程全景概览","quant/qlib/week1/02-horizon对齐详解","quant/qlib/week1/03-横截面标准化与中性化","quant/qlib/week1/04-相对强弱预测的量化思维","quant/qlib/week1/05-qlib特征工程实践指南","LightGBM/","/"],"tags":[],"content":"Qlib 特征工程全景概览\nQlib 是微软亚洲研究院开源的量化投资平台，其核心优势在于系统化、标准化的特征工程框架。本文档深入讲解 Qlib 特征工程的核心理念与实践方法。\n\n📖 文档目录\n1️⃣ Qlib特征工程全景概览\n→ 阅读完整文档\n核心内容：\n\n量化特征工程的三大挑战：非平稳性、自相关性、低信噪比\nQlib的诞生背景与设计哲学\n表达式系统与因子计算引擎\n传统ML方法在量化中的失效案例\n\n适合人群：初次接触Qlib的开发者\n\n2️⃣ Horizon对齐详解\n→ 阅读完整文档\n核心内容：\n\nHorizon概念与预测周期的关系\n因子时序特性分析\nForward Fill与Fillna策略\n多Horizon特征构建方法\n\n适合人群：需要理解因子时序特性的开发者\n\n3️⃣ 横截面标准化与中性化\n→ 阅读完整文档\n核心内容：\n\n横截面标准化的方法与意义\n行业中性化与市值中性化\n新股中性化处理\n正交化与因子正交\n\n适合人群：需要进行因子处理的开发者\n\n4️⃣ 相对强弱预测的量化思维\n→ 阅读完整文档\n核心内容：\n\n相对强弱预测的本质\n排序学习的量化应用\nAlpha因子设计思维\n风险调整收益分析\n\n适合人群：理解量化投资本质的研究者\n\n5️⃣ Qlib特征工程实践指南\n→ 阅读完整文档\n核心内容：\n\nQlib完整工作流程\n因子回测与评估\n常见问题与解决方案\n最佳实践与技巧总结\n\n适合人群：准备使用Qlib进行实战的开发者\n\n🎯 核心优势\n🔄 因果性保证\nQlib的表达式系统强制所有计算只能使用历史数据，从根本上杜绝了未来信息泄露。\n📊 时序效率\n优化的Rolling操作和增量计算，大幅提升大规模时序数据的处理效率。\n🧩 模块化设计\n原子算子、复合算子、自定义算子的三级架构，灵活构建复杂因子。\n🚀 可复现性\n标准化的因子定义和计算流程，确保研究结果的可靠性和可复现性。\n\n🎓 学习建议\n🟢 入门阶段\n建议按顺序阅读：\n\nQlib特征工程全景概览\nHorizon对齐详解\n横截面标准化与中性化\n\n🟡 进阶阶段\n深入理解：\n\n相对强弱预测的量化思维\nQlib特征工程实践指南\n\n🔴 实战阶段\n结合具体项目，在实践中深入理解各个概念。\n\n💡 实用技巧\n\n从简单开始：先用基本算子构建简单因子，理解核心概念\n关注因果性：时刻注意避免未来信息泄露\n善用可视化：观察因子分布和相关性，快速发现问题\n逐步迭代：从原型到生产，逐步优化因子质量\n\n\n🔗 相关资源\n\nQlib官方文档\nQlib GitHub仓库\nLightGBM文档\n\n\n← 返回首页"},"quant/qlib/week2/01-Gradient-Boosting原理":{"slug":"quant/qlib/week2/01-Gradient-Boosting原理","filePath":"quant/qlib/week2/01-Gradient-Boosting原理.md","title":"01-Gradient-Boosting原理","links":[],"tags":[],"content":"Gradient Boosting 原理\n1. Boosting算法基础\n1.1 集成学习的核心思想\nBoosting是一种强大的集成学习方法，其核心思想是将多个弱学习器（weak learners）组合成一个强学习器（strong learner）。与Bagging（随机森林）不同，Boosting采用串行训练方式，每个新模型都专注于纠正前一个模型的错误。\n数学表达\n给定训练数据 D = \\{(x_i, y_i)\\}_{i=1}^N，Boosting通过以下方式构建模型：\nF(x) = \\sum_{m=1}^M \\alpha_m h_m(x)\n其中：\n\nh_m(x) 是第 m 个弱学习器\n\\alpha_m 是第 m 个学习器的权重\nM 是学习器总数\n\n关键特性\n\n串行训练：每个模型按顺序训练，依赖前一个模型\n错误聚焦：新模型重点关注前序模型预测错误的样本\n权重更新：样本权重或模型权重动态调整\n逐步优化：整体模型性能逐步提升\n\n1.2 Gradient Boosting的数学推导\n目标函数\nGradient Boosting将Boosting问题转化为优化问题，最小化损失函数：\n\\min_{F} L(y, F(x))\n其中 L 是损失函数，F(x) 是最终的集成模型。\n前向分步算法\n前向分步算法（Forward Stagewise Additive Modeling）是Gradient Boosting的核心：\n\n初始化：从常数值开始\n\nF_0(x) = \\arg\\min_{\\gamma} \\sum_{i=1}^N L(y_i, \\gamma)\n\n迭代优化：对于 m = 1 到 M：\n\na. 计算负梯度（伪残差）\nr_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x) = F_{m-1}(x)}\nb. 用基学习器拟合负梯度\nh_m(x) = \\arg\\min_{h} \\sum_{i=1}^N (r_{im} - h(x_i))^2\nc. 计算最优步长\n\\gamma_m = \\arg\\min_{\\gamma} \\sum_{i=1}^N L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\nd. 更新模型\nF_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\n为什么使用负梯度？\n泰勒展开给出了直观解释。在 F_{m-1}(x) 处对损失函数进行一阶展开：\nL(y, F_{m-1}(x) + \\Delta F(x)) \\approx L(y, F_{m-1}(x)) + \\left[\\frac{\\partial L(y, F_{m-1}(x))}{\\partial F_{m-1}(x)}\\right] \\Delta F(x)\n为了减少损失，我们需要：\n\\Delta F(x) = -\\left[\\frac{\\partial L(y, F_{m-1}(x))}{\\partial F_{m-1}(x)}\\right]\n这正是梯度下降的方向，因此称为Gradient Boosting。\n1.3 不同损失函数的负梯度\n回归任务\n对于平方损失 L(y, F) = \\frac{1}{2}(y - F)^2：\n\\frac{\\partial L}{\\partial F} = -(y - F)\n负梯度：\nr_{im} = y_i - F_{m-1}(x_i)\n即残差（residual），直观解释为预测误差。\n分类任务\n对于对数损失（Log Loss）：\nL(y, F) = \\log(1 + \\exp(-yF))\n其中 y \\in \\{-1, 1\\}。\n负梯度：\nr_{im} = \\frac{y_i}{1 + \\exp(y_i F_{m-1}(x_i))}\n2. LightGBM的核心创新\n2.1 GOSS（基于梯度的单边采样）\n传统采样的局限\n传统GBDT使用随机采样或无采样，存在两个问题：\n\n随机采样：部分重要样本可能被丢弃\n无采样：计算量大，训练速度慢\n\nGOSS的核心思想\nGOSS（Gradient-based One-Side Sampling）根据样本的梯度大小进行采样：\n\n保留大梯度样本：保留梯度绝对值最大的 a \\times N 个样本\n随机采样小梯度样本：从剩余样本中随机采样 b \\times N 个样本\n补偿小梯度：对随机采样的小梯度样本乘以权重 \\frac{1 - a}{b}\n\n算法流程\n# 伪代码\ndef goss_sampling(gradients, a=0.2, b=0.1):\n    N = len(gradients)\n \n    # 1. 选取大梯度样本\n    top_indices = argsort(abs(gradients))[-int(a * N):]\n \n    # 2. 随机采样小梯度样本\n    remaining_indices = set(range(N)) - set(top_indices)\n    random_indices = random.sample(remaining_indices, int(b * N))\n \n    # 3. 设置采样权重\n    sample_weights = np.ones(N)\n    sample_weights[random_indices] = (1 - a) / b\n \n    return top_indices + random_indices, sample_weights\n理论保证\nGOSS通过以下方式保证信息不丢失：\n\n大梯度样本对模型学习贡献大，全部保留\n小梯度样本通过随机采样保留部分信息\n权重补偿确保小梯度样本的统计贡献\n\n在量化中的应用\n对于量化场景，GOSS特别适合处理不平衡的股票样本：\n\n预测误差大的股票（大梯度）会被重点学习\n预测误差小的股票（小梯度）适当采样\n整体训练效率提升3-5倍\n\n2.2 EFB（互斥特征捆绑）\n特征稀疏性问题\n量化数据中，特征矩阵通常高度稀疏。例如：\n\n技术指标在不同股票间可能缺失\n行业因子在特定股票上为0\n时间序列存在自然稀疏性\n\nEFB的核心思想\nEFB（Exclusive Feature Bundling）利用特征的互斥性（Exclusive Feature）进行特征捆绑：\n\n识别互斥特征对：两个特征几乎不同时为非零值\n构建特征簇：将互斥特征聚合成簇\n特征合并：将簇内特征合并为单一特征\n\n互斥性定义\n特征 A 和 B 的互斥程度：\n\\text{Exclusivity}(A, B) = \\frac{|\\{i: A_i \\neq 0 \\text{ and } B_i \\neq 0\\}|}{|\\{i: A_i \\neq 0 \\text{ or } B_i \\neq 0\\}|}\n如果 \\text{Exclusivity}(A, B) &lt; \\epsilon，则认为 A 和 B 互斥。\n特征捆绑算法\n\n构建冲突图：节点代表特征，边代表非互斥关系\n图着色：对冲突图进行着色，相同颜色的特征可以捆绑\n特征合并：将同色特征合并，通过偏移量区分\n\n特征合并方式\n假设捆绑特征 A 和 B：\n\\text{BundledFeature} = A + \\text{offset}_B \\cdot B\n其中 \\text{offset}_B 选择为 A 的最大值 + 1。\n在量化中的应用\n对于量化因子，EFB可以：\n\n将行业分类特征捆绑\n合并稀疏的技术指标\n降低特征维度，加速训练\n\n2.3 Leaf-wise生长策略\nLevel-wise vs Leaf-wise\n传统GBDT（如XGBoost）使用Level-wise生长策略，而LightGBM使用Leaf-wise：\nLevel-wise策略\n\n每一层分裂所有叶子节点\n树生长平衡，但可能分裂无效节点\n计算资源浪费在低增益分裂上\n\nLeaf-wise策略\n\n每次选择增益最大的叶子节点进行分裂\n树生长不平衡，但更高效\n专注于最大化每步的信息增益\n\nLeaf-wise的优势\n对于相同深度的树：\n\nLeaf-wise能更快降低损失\n在量化场景中，能更快捕捉关键特征\n\n潜在问题与解决方案\nLeaf-wise可能导致过拟合，LightGBM通过以下方式控制：\n\n最大深度限制：max_depth 参数\n叶子节点数限制：num_leaves 参数\n最小增益阈值：min_split_gain 参数\n\n在量化中的应用\n对于量化场景，Leaf-wise特别适合：\n\n捕捉非线性的因子交互效应\n快速发现有效的因子组合\n在有限的迭代次数内达到最优性能\n\n3. LightGBM在量化中的优势\n3.1 处理大规模因子数据\n量化数据特点\n\n高维特征：数百到数千个因子\n时间维度：多个时间点的历史数据\n截面维度：数千只股票\n稀疏性：部分因子在部分股票上缺失\n\nLightGBM的优势\n\n\n内存效率\n\n使用binning技术，特征压缩到256 bins\n内存占用比传统GBDT减少60-80%\n\n\n\n计算速度\n\nGOSS采样减少样本数量\nEFB减少特征数量\nLeaf-wise快速收敛\n\n\n\n分布式训练\n\n支持多机多卡并行训练\n适合处理超大规模因子库\n\n\n\n3.2 处理非平衡样本\n量化样本不平衡问题\n\n牛市/熊市样本不均衡\n涨/跌样本比例波动\n不同股票的样本量差异大\n\nLightGBM的解决方案\n\n\nGOSS自适应采样\n\n自动关注难预测样本\n不需要手动设置样本权重\n\n\n\n加权损失\n\nscale_pos_weight 参数控制正负样本权重\n支持自定义样本权重\n\n\n\n类别平衡\n\nis_unbalance 参数自动处理类别不平衡\n\n\n\n3.3 支持自定义损失函数\n量化评估指标的特殊性\n量化投资关注IC、IR等非标准评估指标，需要自定义损失函数。\nLightGBM的自定义损失\nimport numpy as np\nimport lightgbm as lgb\n \ndef rank_ic_loss(preds, train_data):\n    labels = train_data.get_label()\n \n    # 计算Rank IC\n    rank_pred = np.argsort(preds)\n    rank_label = np.argsort(labels)\n    ic = np.corrcoef(rank_pred, rank_label)[0, 1]\n \n    # 返回梯度和Hessian\n    grad = -ic * (labels - preds)\n    hess = np.ones_like(preds)\n \n    return grad, hess\n \n# 使用自定义损失\ntrain_data = lgb.Dataset(X_train, label=y_train)\nparams = {\n    &#039;objective&#039;: &#039;custom&#039;,\n    &#039;metric&#039;: &#039;custom&#039;\n}\n \nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=1000,\n    fobj=rank_ic_loss\n)\n3.4 正则化防止过拟合\n量化过拟合风险\n\n因子数量多，样本相对少\n历史数据存在过拟合风险\n样本外表现可能下降\n\nLightGBM的正则化机制\n\n\nL1/L2正则化\n\nlambda_l1: L1正则化系数\nlambda_l2: L2正则化系数\n\n\n\n早停机制\n\nearly_stopping_rounds: 监控验证集，提前停止\n\n\n\n参数约束\n\nmin_data_in_leaf: 叶子节点最小样本数\nmax_depth: 树最大深度\nnum_leaves: 叶子节点最大数量\n\n\n\n4. LightGBM核心参数详解\n4.1 训练参数\n核心迭代参数\nparams = {\n    # 核心参数\n    &#039;num_leaves&#039;: 31,              # 叶子节点数量，影响模型复杂度\n    &#039;max_depth&#039;: -1,               # 树最大深度，-1表示无限制\n    &#039;learning_rate&#039;: 0.05,         # 学习率，控制模型收敛速度\n \n    # 采样参数\n    &#039;bagging_fraction&#039;: 0.8,       # 每次迭代使用的样本比例\n    &#039;feature_fraction&#039;: 0.8,       # 每次迭代使用的特征比例\n    &#039;bagging_freq&#039;: 5,             # bagging频率，0表示禁用\n \n    # 正则化参数\n    &#039;lambda_l1&#039;: 0.0,              # L1正则化\n    &#039;lambda_l2&#039;: 0.0,              # L2正则化\n    &#039;min_data_in_leaf&#039;: 20,        # 叶子节点最小样本数\n    &#039;min_sum_hessian_in_leaf&#039;: 1e-3,  # 叶子节点最小Hessian和\n \n    # GOSS参数\n    &#039;bagging_type&#039;: &#039;goss&#039;,        # 使用GOSS采样\n \n    # 其他参数\n    &#039;objective&#039;: &#039;regression&#039;,     # 目标函数\n    &#039;metric&#039;: &#039;rmse&#039;,              # 评估指标\n}\n4.2 重要参数调优策略\n快速调优流程\n\n第一步：设置num_leaves\n\n# num_leaves = 2^max_depth 的经验值\nmax_depth = 6\nnum_leaves = 2 ** 6  # 64\n \n# 实际可以略小于这个值，例如：\nnum_leaves = int(0.8 * 2 ** max_depth)\n\n第二步：调整learning_rate和n_estimators\n\n# 学习率越小，需要的迭代次数越多\nlearning_rate = 0.05\nn_estimators = 1000\n \n# 早停机制\nearly_stopping_rounds = 50\n\n第三步：调整bagging和feature fraction\n\n# 防止过拟合\nbagging_fraction = 0.8\nfeature_fraction = 0.8\nbagging_freq = 5\n\n第四步：调整正则化\n\n# 观察验证集表现\nlambda_l1 = 0.1\nlambda_l2 = 0.1\nmin_data_in_leaf = 20\n量化场景推荐配置\nquant_params = {\n    # 高维特征场景\n    &#039;num_leaves&#039;: 127,\n    &#039;max_depth&#039;: 8,\n    &#039;learning_rate&#039;: 0.03,\n \n    # 正则化\n    &#039;lambda_l1&#039;: 0.1,\n    &#039;lambda_l2&#039;: 0.1,\n    &#039;min_data_in_leaf&#039;: 100,  # 较大值防止过拟合\n \n    # 采样\n    &#039;bagging_fraction&#039;: 0.7,\n    &#039;feature_fraction&#039;: 0.7,\n    &#039;bagging_freq&#039;: 5,\n \n    # 早停\n    &#039;early_stopping_rounds&#039;: 100,\n}\n5. Bagging vs Boosting 对比\n5.1 核心区别\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n维度Bagging (随机森林)Boosting (GBDT)树的关系并行、独立串行、依赖前一棵采样方式有放回随机采样全部数据权重等权重平均学习率加权累加偏差-方差降低方差降低偏差过拟合不容易需要早停控制代表算法Random ForestXGBoost, LightGBM\n5.2 可视化对比\nimport matplotlib.pyplot as plt\nimport numpy as np\n \n# 模拟训练过程\nboosting_train_loss = [1.0, 0.8, 0.6, 0.5, 0.45, 0.42, 0.40, 0.39, 0.38, 0.38]\nboosting_val_loss = [1.0, 0.85, 0.72, 0.65, 0.62, 0.61, 0.62, 0.64, 0.66, 0.68]\n \nbagging_train_loss = [0.6, 0.58, 0.57, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56, 0.56]\nbagging_val_loss = [0.65, 0.63, 0.62, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61, 0.61]\n \n# 绘图\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n \n# Boosting\naxes[0].plot(boosting_train_loss, &#039;b-&#039;, label=&#039;Train Loss&#039;, linewidth=2)\naxes[0].plot(boosting_val_loss, &#039;r-&#039;, label=&#039;Val Loss&#039;, linewidth=2)\naxes[0].axhline(y=0.62, color=&#039;g&#039;, linestyle=&#039;--&#039;, label=&#039;Overfitting Point&#039;)\naxes[0].set_xlabel(&#039;Iterations&#039;)\naxes[0].set_ylabel(&#039;Loss&#039;)\naxes[0].set_title(&#039;Boosting: 逐步纠错，易过拟合&#039;)\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n \n# Bagging\naxes[1].plot(bagging_train_loss, &#039;b-&#039;, label=&#039;Train Loss&#039;, linewidth=2)\naxes[1].plot(bagging_val_loss, &#039;r-&#039;, label=&#039;Val Loss&#039;, linewidth=2)\naxes[1].set_xlabel(&#039;Iterations&#039;)\naxes[1].set_ylabel(&#039;Loss&#039;)\naxes[1].set_title(&#039;Bagging: 并行训练，难过拟合&#039;)\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n \nplt.tight_layout()\nplt.show()\n5.3 为什么量化偏爱 Boosting？\n量化场景特点：\n\n\n信号很弱 📊\n\n股票预测的 IC 通常只有 0.03~0.08\n需要模型精确捕捉微弱模式\n→ Boosting 擅长降低偏差，提取弱信号\n\n\n\n特征稀疏 🔍\n\n不是所有特征都有用\n需要自动选择重要特征\n→ LightGBM 的 feature_fraction 参数\n\n\n\n需要快速迭代 ⚡\n\n每天都有新数据\n需要快速重训练\n→ LightGBM 比 XGBoost 快 10x\n\n\n\n6. Boosting 的迭代纠错机制可视化\n6.1 残差学习过程\ndef visualize_residual_learning():\n    &quot;&quot;&quot;可视化残差学习过程&quot;&quot;&quot;\n    \n    # 生成模拟数据\n    np.random.seed(42)\n    X = np.linspace(0, 10, 100)\n    y = np.sin(X) + np.random.normal(0, 0.2, 100)\n    \n    # 使用 sklearn 的 GradientBoostingRegressor\n    from sklearn.ensemble import GradientBoostingRegressor\n    \n    # 逐步训练\n    n_trees = 5\n    gbr = GradientBoostingRegressor(n_estimators=n_trees, max_depth=2, \n                                    learning_rate=0.5, random_state=42)\n    gbr.fit(X.reshape(-1, 1), y)\n    \n    # 绘图\n    fig, axes = plt.subplots(2, n_trees, figsize=(15, 8))\n    \n    for i in range(n_trees):\n        # 获取第i棵树的预测\n        pred_i = gbr.predict(X.reshape(-1, 1), start=i, end=i+1)\n        \n        # 获取第i步的累计预测\n        pred_cumulative = gbr.predict(X.reshape(-1, 1), start=0, end=i+1)\n        \n        # 计算残差\n        if i == 0:\n            residual = y - np.mean(y)\n        else:\n            residual = y - gbr.predict(X.reshape(-1, 1), start=0, end=i)\n        \n        # 第i棵树的预测\n        axes[0, i].scatter(X, y, alpha=0.3, color=&#039;gray&#039;)\n        axes[0, i].plot(X, pred_i, &#039;r-&#039;, linewidth=2)\n        axes[0, i].set_title(f&#039;Tree {i+1}: Learning Residual&#039;)\n        axes[0, i].grid(True, alpha=0.3)\n        \n        # 累计预测\n        axes[1, i].scatter(X, y, alpha=0.3, color=&#039;gray&#039;)\n        axes[1, i].plot(X, pred_cumulative, &#039;b-&#039;, linewidth=2)\n        axes[1, i].set_title(f&#039;Cumulative: Trees 1-{i+1}&#039;)\n        axes[1, i].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n \n# 使用示例\nvisualize_residual_learning()\n6.2 迭代过程示意\n迭代过程:\nTree₁ → 预测 ŷ₁ → 残差 r₁ = y - ŷ₁\nTree₂ → 学习 r₁ → 预测 ŷ₂ → 残差 r₂ = r₁ - ŷ₂\nTree₃ → 学习 r₂ → 预测 ŷ₃ → 残差 r₃ = r₂ - ŷ₃\n...\n最终预测 = ŷ₁ + ŷ₂ + ŷ₃ + ...\n\n7. 总结\nGradient Boosting通过前向分步算法和梯度下降思想，将多个弱学习器组合成强学习器。LightGBM在传统GBDT基础上，通过GOSS、EFB和Leaf-wise三大创新，大幅提升了训练效率和性能。\n在量化投资中，LightGBM的优势体现在：\n\n处理大规模因子数据的高效性\n自适应处理不平衡样本\n支持自定义量化评估指标\n强大的正则化防止过拟合\n捕捉微弱预测信号的能力\n快速迭代适应市场变化\n\n理解LightGBM的原理，是构建有效量化模型的基础。"},"quant/qlib/week2/02-时序数据划分":{"slug":"quant/qlib/week2/02-时序数据划分","filePath":"quant/qlib/week2/02-时序数据划分.md","title":"02-时序数据划分","links":[],"tags":[],"content":"时序数据划分\n1. 量化时序数据的特殊性\n1.1 因果性约束\n核心问题\n在量化投资中，数据划分必须严格遵守因果性（Causality）原则：只能使用历史数据预测未来，绝不能使用未来信息。\n数学表达\n对于时间点 t 的预测 \\hat{y}_t：\n\\hat{y}_t = f(X_{\\leq t})\n其中 X_{\\leq t} 表示 t 时刻及之前的所有特征，不允许包含 X_{&gt;t}。\n违反因果性的典型错误\n# 错误示例1：全局标准化\nfrom sklearn.preprocessing import StandardScaler\n \nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)  # 使用全部数据计算均值和方差\n \n# 这导致 t 时刻的特征使用了 t+1, t+2, ... 的统计信息\n# 问题：scaler.fit使用全部数据的mean和std\n# 错误示例2：PCA降维\nfrom sklearn.decomposition import PCA\n \npca = PCA(n_components=5)\nX_pca = pca.fit_transform(X)  # 使用全部数据计算主成分\n \n# 问题：PCA的协方差矩阵基于全部数据\n正确做法\n# 正确示例1：滚动标准化\nfrom sklearn.preprocessing import StandardScaler\n \ndef rolling_standardize(X, window=252):\n    X_scaled = np.zeros_like(X)\n    for i in range(window, len(X)):\n        scaler = StandardScaler()\n        X_scaled[i] = scaler.fit_transform(X[i-window:i])[-1]\n    return X_scaled\n1.2 数据泄露的检测方法\n前向泄露\n特征计算中使用了未来数据：\nX_t = f(X_{t+1}, X_{t+2}, \\dots)\n检测方法\ndef detect_lookahead_leakage(X, window=5):\n    &quot;&quot;&quot;\n    检测特征是否使用了未来信息\n \n    方法：检查特征与未来目标的相关性\n    &quot;&quot;&quot;\n    correlations = []\n    for lag in range(1, window + 1):\n        # 计算当前特征与未来lag期目标的相关性\n        corr = np.corrcoef(X[:-lag], X[lag:])[0, 1]\n        correlations.append(corr)\n \n    # 如果相关性强，可能存在未来信息泄露\n    if max(correlations) &gt; 0.1:\n        print(f&quot;警告：检测到潜在的未来信息泄露，最大相关性: {max(correlations):.4f}&quot;)\n \n    return correlations\n横截面泄露\n使用了同截面其他股票的信息：\nX_{i,t} = f(X_{j,t}) \\quad \\text{where } j \\neq i\n检测方法\ndef detect_cross_section_leakage(X, stock_ids, n_stocks=100):\n    &quot;&quot;&quot;\n    检测是否存在横截面信息泄露\n \n    方法：检查股票间特征的瞬时相关性\n    &quot;&quot;&quot;\n    t_corr = []\n    for t in range(X.shape[1]):\n        # 计算t时刻所有股票间的相关性\n        Xt = X[:, t, :]\n        corr_matrix = np.corrcoef(Xt)\n \n        # 平均相关性\n        avg_corr = (np.sum(np.abs(corr_matrix)) - n_stocks) / (n_stocks * (n_stocks - 1))\n        t_corr.append(avg_corr)\n \n    return t_corr\n2. 时间序列交叉验证方法\n2.1 传统K-Fold的局限性\n为什么不能用随机K-Fold？\n# 错误示例：使用随机K-Fold\nfrom sklearn.model_selection import KFold\n \nkf = KFold(n_splits=5, shuffle=True)\nfor train_idx, val_idx in kf.split(X):\n    # 问题：训练集和验证集在时间上交错\n    # 例如：train包含2023年数据，val包含2022年数据\n    X_train, X_val = X[train_idx], X[val_idx]\n问题分析\n\n未来信息泄露：验证集可能包含训练集之前的数据\n回测失真：不符合实际投资场景\n评估偏差：虚高模型性能\n\n示例说明\n时间轴：2018 | 2019 | 2020 | 2021 | 2022 | 2023\n随机K-Fold可能产生：\nTrain: 2018, 2020, 2023\nVal:   2019, 2021, 2022\n\n这在实际中不可能！\n\n2.2 时间序列交叉验证（TimeSeriesSplit）\n基本原理\nfrom sklearn.model_selection import TimeSeriesSplit\n \ntscv = TimeSeriesSplit(n_splits=5)\n \nfor train_idx, val_idx in tscv.split(X):\n    # 训练集：[t_start, t_train_end]\n    # 验证集：[t_train_end+1, t_val_end]\n    X_train, X_val = X[train_idx], X[val_idx]\n可视化示意\nFold 1: |=== Train ===|== Val ==|.........|\nFold 2: |==== Train ====|=== Val ===|.....|\nFold 3: |===== Train =====|==== Val ====|..|\nFold 4: |====== Train ======|===== Val =====|\nFold 5: |======= Train =======|====== Val ==|\n\n代码实现\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import TimeSeriesSplit\nimport lightgbm as lgb\n \ndef time_series_cv(X, y, params, n_splits=5):\n    &quot;&quot;&quot;\n    时间序列交叉验证\n \n    参数:\n        X: 特征矩阵，shape=[n_samples, n_features]\n        y: 目标变量，shape=[n_samples]\n        params: LightGBM参数\n        n_splits: 折数\n \n    返回:\n        val_scores: 每折的验证集得分\n    &quot;&quot;&quot;\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    val_scores = []\n \n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n        print(f&quot;Fold {fold + 1}/{n_splits}&quot;)\n        print(f&quot;  Train: {train_idx[0]} - {train_idx[-1]}&quot;)\n        print(f&quot;  Val:   {val_idx[0]} - {val_idx[-1]}&quot;)\n \n        # 划分数据\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n \n        # 创建数据集\n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n \n        # 训练模型\n        model = lgb.train(\n            params,\n            train_data,\n            num_boost_round=1000,\n            valid_sets=[train_data, val_data],\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50, verbose=False),\n                lgb.log_evaluation(period=100)\n            ]\n        )\n \n        # 评估\n        y_pred = model.predict(X_val)\n        score = np.corrcoef(y_pred, y_val)[0, 1]\n        val_scores.append(score)\n        print(f&quot;  Validation IC: {score:.4f}&quot;)\n \n    return val_scores\n2.3 滚动窗口交叉验证（Rolling Window）\n适用场景\n当数据分布随时间剧烈变化时，使用固定窗口大小滚动训练。\n方法对比\n扩展窗口（Expanding Window）：\nFold 1: |=== Train ===|== Val ==|.........|\nFold 2: |==== Train ====|=== Val ===|.....|\nFold 3: |===== Train =====|==== Val ====|..|\n\n滚动窗口（Rolling Window）：\nFold 1: |=== Train ===|== Val ==|.........|\nFold 2: |...=== Train ===|== Val ==|.....|\nFold 3: |.....=== Train ===|== Val ==|..|\n\n代码实现\ndef rolling_window_cv(X, y, params, train_size=252, val_size=21, step=21):\n    &quot;&quot;&quot;\n    滚动窗口交叉验证\n \n    参数:\n        train_size: 训练窗口大小（例如252个交易日≈1年）\n        val_size: 验证窗口大小（例如21个交易日≈1个月）\n        step: 滚动步长\n \n    返回:\n        val_scores: 每折的验证集得分\n        models: 训练的模型列表\n    &quot;&quot;&quot;\n    val_scores = []\n    models = []\n \n    n_samples = len(X)\n    start_idx = train_size\n \n    fold = 0\n    while start_idx + val_size &lt;= n_samples:\n        print(f&quot;Fold {fold + 1}&quot;)\n \n        # 划分数据\n        train_start = start_idx - train_size\n        train_end = start_idx\n        val_start = start_idx\n        val_end = start_idx + val_size\n \n        print(f&quot;  Train: {train_start} - {train_end}&quot;)\n        print(f&quot;  Val:   {val_start} - {val_end}&quot;)\n \n        X_train, X_val = X[train_start:train_end], X[val_start:val_end]\n        y_train, y_val = y[train_start:train_end], y[val_start:val_end]\n \n        # 训练模型\n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n \n        model = lgb.train(\n            params,\n            train_data,\n            num_boost_round=1000,\n            valid_sets=[train_data, val_data],\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50, verbose=False),\n                lgb.log_evaluation(period=100)\n            ]\n        )\n \n        # 评估\n        y_pred = model.predict(X_val)\n        score = np.corrcoef(y_pred, y_val)[0, 1]\n        val_scores.append(score)\n        models.append(model)\n \n        print(f&quot;  Validation IC: {score:.4f}&quot;)\n \n        # 滚动窗口\n        start_idx += step\n        fold += 1\n \n    return val_scores, models\n在量化中的应用\n# 示例：使用滚动窗口验证量化因子\nparams = {\n    &#039;objective&#039;: &#039;regression&#039;,\n    &#039;metric&#039;: &#039;rmse&#039;,\n    &#039;num_leaves&#039;: 31,\n    &#039;learning_rate&#039;: 0.05,\n}\n \n# 训练窗口：1年（252个交易日）\n# 验证窗口：1个月（21个交易日）\n# 滚动步长：1个月\nval_scores, models = rolling_window_cv(\n    X, y, params,\n    train_size=252,\n    val_size=21,\n    step=21\n)\n \nprint(f&quot;平均IC: {np.mean(val_scores):.4f}&quot;)\nprint(f&quot;IC标准差: {np.std(val_scores):.4f}&quot;)\n2.4 步进验证（Walk-Forward Validation）\n核心思想\n模拟实际交易场景，每次验证后，将验证集加入训练集，向前滚动。\n与滚动窗口的区别\n\n滚动窗口：固定训练窗口大小\n步进验证：训练窗口逐步扩大\n\n代码实现\ndef walk_forward_validation(X, y, params, initial_train_size=252, val_size=21, step=21):\n    &quot;&quot;&quot;\n    步进验证\n \n    参数:\n        initial_train_size: 初始训练窗口大小\n        val_size: 验证窗口大小\n        step: 前进步长\n \n    返回:\n        predictions: 所有的预测结果\n        val_scores: 每折的验证集得分\n    &quot;&quot;&quot;\n    predictions = []\n    val_scores = []\n    models = []\n \n    n_samples = len(X)\n    start_idx = initial_train_size\n \n    fold = 0\n    while start_idx + val_size &lt;= n_samples:\n        print(f&quot;Fold {fold + 1}&quot;)\n \n        # 划分数据\n        train_end = start_idx\n        val_start = start_idx\n        val_end = start_idx + val_size\n \n        print(f&quot;  Train: 0 - {train_end}&quot;)\n        print(f&quot;  Val:   {val_start} - {val_end}&quot;)\n \n        X_train, X_val = X[:train_end], X[val_start:val_end]\n        y_train, y_val = y[:train_end], y[val_start:val_end]\n \n        # 训练模型\n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n \n        model = lgb.train(\n            params,\n            train_data,\n            num_boost_round=1000,\n            valid_sets=[train_data, val_data],\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50, verbose=False),\n                lgb.log_evaluation(period=100)\n            ]\n        )\n \n        # 预测\n        y_pred = model.predict(X_val)\n        predictions.extend(y_pred)\n \n        # 评估\n        score = np.corrcoef(y_pred, y_val)[0, 1]\n        val_scores.append(score)\n        models.append(model)\n \n        print(f&quot;  Validation IC: {score:.4f}&quot;)\n \n        # 向前滚动\n        start_idx += step\n        fold += 1\n \n    return predictions, val_scores, models\n3. Purging 和 Embargo (进阶)\n3.1 为什么还需要额外保护？\n问题场景：\n假设：\n\n训练集最后一天: 2021-12-31\n验证集第一天:   2022-01-01\n你的标签是: 未来5日收益\n\n这意味着：\n2021-12-31 的标签 = 2022-01-05 的信息\n2021-12-31 虽然在训练集，但它的标签包含了验证集时间段的信息！\n3.2 Purging (清除)\n原理：\n删除训练集末尾 N 天，N = 标签预测周期\ndef train_val_test_split_with_purging(X, y, dates, \n                                       train_ratio=0.6, \n                                       val_ratio=0.2, \n                                       test_ratio=0.2,\n                                       horizon=5):\n    &quot;&quot;&quot;\n    带Purging的数据划分\n    \n    参数:\n        X, y: 特征和标签\n        dates: 日期数组，shape=[n_samples]\n        train_ratio, val_ratio, test_ratio: 划分比例\n        horizon: 预测周期（标签使用的未来数据天数）\n        \n    返回:\n        (X_train, X_val, X_test), (y_train, y_val, y_test)\n    &quot;&quot;&quot;\n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) &lt; 1e-6\n    \n    # 获取唯一日期并排序\n    unique_dates = np.unique(dates)\n    n_dates = len(unique_dates)\n    \n    # 计算划分点\n    train_end_idx = int(n_dates * train_ratio)\n    val_end_idx = int(n_dates * (train_ratio + val_ratio))\n    \n    train_end_date = unique_dates[train_end_idx]\n    val_end_date = unique_dates[val_end_idx]\n    \n    print(f&quot;原始划分:&quot;)\n    print(f&quot;  训练集:  {unique_dates[0]} ~ {train_end_date}&quot;)\n    print(f&quot;  验证集:  {unique_dates[train_end_idx+1]} ~ {val_end_date}&quot;)\n    print(f&quot;  测试集:  {unique_dates[val_end_idx+1]} ~ {unique_dates[-1]}&quot;)\n    \n    # Purging: 删除训练集末尾horizon天\n    purge_start_date = unique_dates[train_end_idx - horizon]\n    \n    # 生成掩码\n    train_mask = dates &lt;= purge_start_date\n    valid_mask = (dates &gt; train_end_date) &amp; (dates &lt;= val_end_date)\n    test_mask = dates &gt; val_end_date\n    \n    print(f&quot;\\nPurging (删除训练集末尾{horizon}天):&quot;)\n    print(f&quot;  原训练集末尾: {train_end_date}&quot;)\n    print(f&quot;  新训练集末尾: {purge_start_date}&quot;)\n    \n    # 划分数据\n    X_train, y_train = X[train_mask], y[train_mask]\n    X_val, y_val = X[valid_mask], y[valid_mask]\n    X_test, y_test = X[test_mask], y[test_mask]\n    \n    print(f&quot;\\n最终划分:&quot;)\n    print(f&quot;  训练集: {len(X_train)} 样本&quot;)\n    print(f&quot;  验证集: {len(X_val)} 样本&quot;)\n    print(f&quot;  测试集: {len(X_test)} 样本&quot;)\n    \n    return (X_train, X_val, X_test), (y_train, y_val, y_test)\n \n# 使用示例\ndates = pd.date_range(&#039;2020-01-01&#039;, &#039;2023-12-31&#039;, freq=&#039;D&#039;)\nX = np.random.randn(len(dates), 10)\ny = np.random.randn(len(dates))\n \n(X_train, X_val, X_test), (y_train, y_val, y_test) = train_val_test_split_with_purging(\n    X, y, dates,\n    train_ratio=0.6,\n    val_ratio=0.2,\n    test_ratio=0.2,\n    horizon=5\n)\n3.3 Embargo (禁运)\n原理：\n验证集开头额外空出几天作为缓冲\ndef train_val_test_split_with_embargo(X, y, dates,\n                                      train_ratio=0.6,\n                                      val_ratio=0.2,\n                                      test_ratio=0.2,\n                                      horizon=5,\n                                      embargo=3):\n    &quot;&quot;&quot;\n    带Embargo的数据划分\n    \n    参数:\n        X, y: 特征和标签\n        dates: 日期数组\n        train_ratio, val_ratio, test_ratio: 划分比例\n        horizon: 预测周期\n        embargo: 禁运天数\n        \n    返回:\n        (X_train, X_val, X_test), (y_train, y_val, y_test)\n    &quot;&quot;&quot;\n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) &lt; 1e-6\n    \n    # 获取唯一日期并排序\n    unique_dates = np.unique(dates)\n    n_dates = len(unique_dates)\n    \n    # 计算划分点\n    train_end_idx = int(n_dates * train_ratio)\n    val_end_idx = int(n_dates * (train_ratio + val_ratio))\n    \n    train_end_date = unique_dates[train_end_idx]\n    val_end_date = unique_dates[val_end_idx]\n    \n    # Embargo: 验证集开头额外空出embargo天\n    embargo_start_date = unique_dates[train_end_idx + embargo]\n    \n    # 生成掩码\n    train_mask = dates &lt;= train_end_date\n    valid_mask = (dates &gt; embargo_start_date) &amp; (dates &lt;= val_end_date)\n    test_mask = dates &gt; val_end_date\n    \n    print(f&quot;\\nEmbargo (验证集开头空出{embargo}天):&quot;)\n    print(f&quot;  原验证集开头: {unique_dates[train_end_idx+1]}&quot;)\n    print(f&quot;  新验证集开头: {embargo_start_date}&quot;)\n    \n    # 划分数据\n    X_train, y_train = X[train_mask], y[train_mask]\n    X_val, y_val = X[valid_mask], y[valid_mask]\n    X_test, y_test = X[test_mask], y[test_mask]\n    \n    print(f&quot;\\n最终划分:&quot;)\n    print(f&quot;  训练集: {len(X_train)} 样本&quot;)\n    print(f&quot;  验证集: {len(X_val)} 样本 (删除了{embargo}天)&quot;)\n    print(f&quot;  测试集: {len(X_test)} 样本&quot;)\n    \n    return (X_train, X_val, X_test), (y_train, y_val, y_test)\n \n# 使用示例\n(X_train, X_val, X_test), (y_train, y_val, y_test) = train_val_test_split_with_embargo(\n    X, y, dates,\n    train_ratio=0.6,\n    val_ratio=0.2,\n    test_ratio=0.2,\n    horizon=5,\n    embargo=3\n)\n3.4 完整的 Purging + Embargo\ndef train_val_test_split_full(X, y, dates,\n                               train_ratio=0.6,\n                               val_ratio=0.2,\n                               test_ratio=0.2,\n                               horizon=5,\n                               embargo=3):\n    &quot;&quot;&quot;\n    完整的Purging + Embargo数据划分\n    \n    时间线:\n    │  训练集  │ Purge │ Embargo │   验证集   │\n    └──────────┘   ↓       ↓\n               删除这段   缓冲区\n    &quot;&quot;&quot;\n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) &lt; 1e-6\n    \n    # 获取唯一日期并排序\n    unique_dates = np.unique(dates)\n    n_dates = len(unique_dates)\n    \n    # 计算划分点\n    train_end_idx = int(n_dates * train_ratio)\n    val_end_idx = int(n_dates * (train_ratio + val_ratio))\n    \n    # Purging: 删除训练集末尾horizon天\n    purge_start_date = unique_dates[train_end_idx - horizon]\n    \n    # Embargo: 验证集开头空出embargo天\n    embargo_start_date = unique_dates[train_end_idx + embargo]\n    val_end_date = unique_dates[val_end_idx]\n    \n    # 生成掩码\n    train_mask = dates &lt;= purge_start_date\n    valid_mask = (dates &gt; embargo_start_date) &amp; (dates &lt;= val_end_date)\n    test_mask = dates &gt; val_end_date\n    \n    # 划分数据\n    X_train, y_train = X[train_mask], y[train_mask]\n    X_val, y_val = X[valid_mask], y[valid_mask]\n    X_test, y_test = X[test_mask], y[test_mask]\n    \n    print(f&quot;完整划分 (Purging + Embargo):&quot;)\n    print(f&quot;  训练集: {len(X_train)} 样本 (删除末尾{horizon}天)&quot;)\n    print(f&quot;  验证集: {len(X_val)} 样本 (删除开头{embargo}天)&quot;)\n    print(f&quot;  测试集: {len(X_test)} 样本&quot;)\n    \n    return (X_train, X_val, X_test), (y_train, y_val, y_test)\n \n# 使用示例\n(X_train, X_val, X_test), (y_train, y_val, y_test) = train_val_test_split_full(\n    X, y, dates,\n    train_ratio=0.6,\n    val_ratio=0.2,\n    test_ratio=0.2,\n    horizon=5,\n    embargo=3\n)\n3.5 Walk-Forward 验证 (滚动验证)\n更真实的验证方式：模拟实际交易场景\ndef walk_forward_validation_with_purging(X, y, dates, model_class, params,\n                                          initial_train_size=252,\n                                          val_size=21,\n                                          step=21,\n                                          horizon=5):\n    &quot;&quot;&quot;\n    带Purging的Walk-Forward验证\n    \n    模拟实际交易场景：\n    Window 1: Train(2020) → Test(2021 Q1)\n    Window 2: Train(2020~2021 Q1) → Test(2021 Q2)\n    Window 3: Train(2020~2021 Q2) → Test(2021 Q3)\n    ...\n    &quot;&quot;&quot;\n    unique_dates = np.unique(dates)\n    n_dates = len(unique_dates)\n    \n    val_scores = []\n    models = []\n    windows = []\n    \n    start_idx = initial_train_size\n    \n    fold = 0\n    while start_idx + val_size + horizon &lt;= n_dates:\n        print(f&quot;\\n{&#039;=&#039;*60}&quot;)\n        print(f&quot;Window {fold + 1}&quot;)\n        print(f&quot;{&#039;=&#039;*60}&quot;)\n        \n        # 训练集: 删除末尾horizon天 (Purging)\n        train_start = 0\n        train_end = start_idx - horizon\n        train_end_date = unique_dates[train_end]\n        \n        # 验证集: 从start_idx开始\n        val_start = start_idx\n        val_end = start_idx + val_size\n        val_start_date = unique_dates[val_start]\n        val_end_date = unique_dates[val_end]\n        \n        print(f&quot;训练集: {unique_dates[0]} ~ {train_end_date}&quot;)\n        print(f&quot;验证集: {val_start_date} ~ {val_end_date}&quot;)\n        print(f&quot;Purging: 删除 {unique_dates[train_end+1]} ~ {unique_dates[start_idx-1]} ({horizon}天)&quot;)\n        \n        # 划分数据\n        train_mask = (dates &gt;= unique_dates[0]) &amp; (dates &lt;= train_end_date)\n        val_mask = (dates &gt;= val_start_date) &amp; (dates &lt;= val_end_date)\n        \n        X_train, y_train = X[train_mask], y[train_mask]\n        X_val, y_val = X[val_mask], y[val_mask]\n        \n        print(f&quot;训练样本数: {len(X_train)}, 验证样本数: {len(X_val)}&quot;)\n        \n        # 训练模型\n        model = model_class(**params)\n        model.fit(X_train, y_train)\n        \n        # 预测\n        y_pred = model.predict(X_val)\n        \n        # 评估\n        from scipy.stats import pearsonr\n        ic = pearsonr(y_pred, y_val)[0]\n        val_scores.append(ic)\n        models.append(model)\n        \n        windows.append({\n            &#039;train_start&#039;: unique_dates[0],\n            &#039;train_end&#039;: train_end_date,\n            &#039;val_start&#039;: val_start_date,\n            &#039;val_end&#039;: val_end_date,\n            &#039;ic&#039;: ic\n        })\n        \n        print(f&quot;验证IC: {ic:.4f}&quot;)\n        \n        # 向前滚动\n        start_idx += step\n        fold += 1\n    \n    # 统计\n    print(f&quot;\\n{&#039;=&#039;*60}&quot;)\n    print(&quot;Walk-Forward验证统计&quot;)\n    print(f&quot;{&#039;=&#039;*60}&quot;)\n    print(f&quot;窗口数: {len(windows)}&quot;)\n    print(f&quot;平均IC: {np.mean(val_scores):.4f}&quot;)\n    print(f&quot;IC标准差: {np.std(val_scores):.4f}&quot;)\n    print(f&quot;ICIR: {np.mean(val_scores) / np.std(val_scores):.4f}&quot;)\n    print(f&quot;IC胜率: {(np.array(val_scores) &gt; 0).mean():.2%}&quot;)\n    \n    return val_scores, models, windows\n \n# 使用示例\nfrom lightgbm import LGBMRegressor\n \nparams = {\n    &#039;objective&#039;: &#039;regression&#039;,\n    &#039;num_leaves&#039;: 31,\n    &#039;learning_rate&#039;: 0.05,\n    &#039;verbosity&#039;: -1,\n}\n \nval_scores, models, windows = walk_forward_validation_with_purging(\n    X, y, dates, LGBMRegressor, params,\n    initial_train_size=252,  # 1年\n    val_size=21,             # 1个月\n    step=21,                 # 1个月\n    horizon=5                # 5天\n)\n \n# 绘制IC序列\nimport matplotlib.pyplot as plt\n \nplt.figure(figsize=(12, 6))\nplt.plot([w[&#039;ic&#039;] for w in windows], &#039;o-&#039;, linewidth=2, markersize=8)\nplt.axhline(y=np.mean(val_scores), color=&#039;r&#039;, linestyle=&#039;--&#039;, \n            label=f&#039;Mean IC: {np.mean(val_scores):.4f}&#039;)\nplt.axhline(y=0, color=&#039;gray&#039;, linestyle=&#039;-&#039;, alpha=0.3)\nplt.xlabel(&#039;Window&#039;)\nplt.ylabel(&#039;IC&#039;)\nplt.title(&#039;Walk-Forward Validation IC&#039;)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n示例输出：\nWindow 1 (2024-Q1): IC = 0.0536\nWindow 2 (2024-Q2): IC = 0.0456\nWindow 3 (2024-Q3): IC = 0.0408\nWindow 4 (2024-Q4): IC = 0.0305\n\n平均 IC: 0.0426 ± 0.0083\n\n→ 如果 IC 随时间下降，说明模型需要重训练\n\n4. 量化场景下的数据划分策略\n4.1 训练集/验证集/测试集划分\n推荐比例\n时间轴：|======== Train ======|==== Val ====|=== Test ===|\n      2018-2021           2022        2023\n      4年                 1年         1年\n\n代码实现\ndef train_val_test_split(X, y, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n    &quot;&quot;&quot;\n    时序数据的三层划分\n \n    参数:\n        train_ratio: 训练集比例\n        val_ratio: 验证集比例\n        test_ratio: 测试集比例\n \n    返回:\n        (X_train, X_val, X_test), (y_train, y_val, y_test)\n    &quot;&quot;&quot;\n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) &lt; 1e-6\n \n    n_samples = len(X)\n    train_end = int(n_samples * train_ratio)\n    val_end = int(n_samples * (train_ratio + val_ratio))\n \n    X_train, X_val, X_test = X[:train_end], X[train_end:val_end], X[val_end:]\n    y_train, y_val, y_test = y[:train_end], y[train_end:val_end], y[val_end:]\n \n    return (X_train, X_val, X_test), (y_train, y_val, y_test)\n实际应用\n# 示例：4年训练，1年验证，1年测试\n(X_train, X_val, X_test), (y_train, y_val, y_test) = train_val_test_split(\n    X, y,\n    train_ratio=4/6,\n    val_ratio=1/6,\n    test_ratio=1/6\n)\n \nprint(f&quot;Train: {len(X_train)} samples&quot;)\nprint(f&quot;Val:   {len(X_val)} samples&quot;)\nprint(f&quot;Test:  {len(X_test)} samples&quot;)\n3.2 多市场周期的划分\n为什么要考虑市场周期？\n不同市场状态（牛市/熊市/震荡市）下，因子表现差异巨大。\n识别市场状态\ndef identify_market_regime(prices, window=252):\n    &quot;&quot;&quot;\n    识别市场状态\n \n    参数:\n        prices: 价格序列，shape=[n_samples]\n        window: 滚动窗口大小\n \n    返回:\n        regimes: 市场状态标签（0=震荡, 1=牛市, -1=熊市）\n    &quot;&quot;&quot;\n    # 计算收益率\n    returns = prices.pct_change().dropna()\n \n    # 计算滚动趋势和波动率\n    trend = returns.rolling(window).mean()\n    volatility = returns.rolling(window).std()\n \n    # 定义阈值\n    trend_threshold = trend.mean() + 0.5 * volatility.mean()\n    volatility_threshold = volatility.mean()\n \n    regimes = np.zeros(len(prices))\n \n    for i in range(window, len(prices)):\n        if trend[i] &gt; trend_threshold:\n            regimes[i] = 1  # 牛市\n        elif trend[i] &lt; -trend_threshold:\n            regimes[i] = -1  # 熊市\n        else:\n            regimes[i] = 0  # 震荡市\n \n    return regimes\n按市场状态划分数据\ndef split_by_regime(X, y, regimes):\n    &quot;&quot;&quot;\n    按市场状态划分数据\n \n    返回:\n        dict: {regime: (X_regime, y_regime)}\n    &quot;&quot;&quot;\n    regime_splits = {}\n \n    for regime in [-1, 0, 1]:\n        mask = regimes == regime\n        regime_splits[regime] = (X[mask], y[mask])\n \n    return regime_splits\n \n# 使用示例\nregime_splits = split_by_regime(X, y, regimes)\n \nfor regime, (X_regime, y_regime) in regime_splits.items():\n    regime_name = {1: &#039;牛市&#039;, 0: &#039;震荡市&#039;, -1: &#039;熊市&#039;}[regime]\n    print(f&quot;{regime_name}: {len(X_regime)} samples&quot;)\n平衡不同市场状态的样本\ndef balance_regime_samples(X, y, regimes, target_ratio=0.3):\n    &quot;&quot;&quot;\n    平衡不同市场状态的样本比例\n \n    参数:\n        target_ratio: 牛市和熊市的目标比例\n \n    返回:\n        (X_balanced, y_balanced)\n    &quot;&quot;&quot;\n    # 计算当前比例\n    bull_ratio = np.sum(regimes == 1) / len(regimes)\n    bear_ratio = np.sum(regimes == -1) / len(regimes)\n \n    # 计算需要采样/重复的次数\n    bull_factor = target_ratio / bull_ratio if bull_ratio &gt; 0 else 0\n    bear_factor = target_ratio / bear_ratio if bear_ratio &gt; 0 else 0\n \n    X_balanced, y_balanced = [], []\n \n    for regime in [-1, 0, 1]:\n        mask = regimes == regime\n        X_regime, y_regime = X[mask], y[mask]\n \n        if regime == 1:\n            factor = bull_factor\n        elif regime == -1:\n            factor = bear_factor\n        else:\n            factor = 1.0\n \n        # 采样或重复\n        n_samples = int(len(X_regime) * factor)\n        if factor &gt; 1:\n            # 重复采样\n            indices = np.random.choice(len(X_regime), n_samples, replace=True)\n        else:\n            # 随机采样\n            indices = np.random.choice(len(X_regime), n_samples, replace=False)\n \n        X_balanced.append(X_regime[indices])\n        y_balanced.append(y_regime[indices])\n \n    X_balanced = np.vstack(X_balanced)\n    y_balanced = np.hstack(y_balanced)\n \n    return X_balanced, y_balanced\n3.3 按行业划分\n行业异质性\n不同行业的股票特征和收益模式差异显著，需要行业划分。\n代码实现\ndef split_by_industry(X, y, industry_codes):\n    &quot;&quot;&quot;\n    按行业划分数据\n \n    参数:\n        industry_codes: 行业代码，shape=[n_samples]\n \n    返回:\n        dict: {industry_code: (X_industry, y_industry)}\n    &quot;&quot;&quot;\n    industry_splits = {}\n \n    for code in np.unique(industry_codes):\n        mask = industry_codes == code\n        industry_splits[code] = (X[mask], y[mask])\n \n    return industry_splits\n行业中性化\ndef industry_neutralize(X, y, industry_codes):\n    &quot;&quot;&quot;\n    行业中性化：消除行业效应\n \n    方法：对每个行业进行标准化\n    &quot;&quot;&quot;\n    X_neutral = np.zeros_like(X)\n    y_neutral = np.zeros_like(y)\n \n    for code in np.unique(industry_codes):\n        mask = industry_codes == code\n \n        # 标准化特征\n        X_industry = X[mask]\n        X_mean = X_industry.mean(axis=0)\n        X_std = X_industry.std(axis=0)\n        X_neutral[mask] = (X_industry - X_mean) / (X_std + 1e-8)\n \n        # 标准化目标\n        y_industry = y[mask]\n        y_mean = y_industry.mean()\n        y_std = y_industry.std()\n        y_neutral[mask] = (y_industry - y_mean) / (y_std + 1e-8)\n \n    return X_neutral, y_neutral\n4. 数据划分的最佳实践\n4.1 完整的数据划分流程\nclass QuantDataSplitter:\n    &quot;&quot;&quot;\n    量化数据划分器\n \n    功能：\n    1. 时间序列划分\n    2. 市场状态识别\n    3. 行业中性化\n    4. 数据验证\n    &quot;&quot;&quot;\n \n    def __init__(self, prices, industry_codes=None):\n        self.prices = prices\n        self.industry_codes = industry_codes\n \n    def identify_regimes(self, window=252):\n        &quot;&quot;&quot;识别市场状态&quot;&quot;&quot;\n        returns = self.prices.pct_change().dropna()\n        trend = returns.rolling(window).mean()\n        volatility = returns.rolling(window).std()\n \n        trend_threshold = trend.mean() + 0.5 * volatility.mean()\n        regimes = np.zeros(len(self.prices))\n \n        for i in range(window, len(self.prices)):\n            if trend[i] &gt; trend_threshold:\n                regimes[i] = 1\n            elif trend[i] &lt; -trend_threshold:\n                regimes[i] = -1\n            else:\n                regimes[i] = 0\n \n        self.regimes = regimes\n        return regimes\n \n    def train_val_test_split(self, X, y, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n        &quot;&quot;&quot;三层划分&quot;&quot;&quot;\n        assert abs(train_ratio + val_ratio + test_ratio - 1.0) &lt; 1e-6\n \n        n_samples = len(X)\n        train_end = int(n_samples * train_ratio)\n        val_end = int(n_samples * (train_ratio + val_ratio))\n \n        splits = {\n            &#039;train&#039;: (X[:train_end], y[:train_end]),\n            &#039;val&#039;: (X[train_end:val_end], y[train_end:val_end]),\n            &#039;test&#039;: (X[val_end:], y[val_end:])\n        }\n \n        return splits\n \n    def validate_no_leakage(self, X_train, X_val):\n        &quot;&quot;&quot;验证无信息泄露&quot;&quot;&quot;\n        train_mean = X_train.mean(axis=0)\n        train_std = X_train.std(axis=0)\n        val_mean = X_val.mean(axis=0)\n \n        # 检查均值差异\n        mean_diff = np.abs(train_mean - val_mean) / (train_std + 1e-8)\n \n        if np.any(mean_diff &gt; 3):\n            print(&quot;警告：训练集和验证集分布差异较大&quot;)\n            print(f&quot;最大标准化差异: {np.max(mean_diff):.4f}&quot;)\n \n        return mean_diff\n \n    def split(self, X, y, neutralize_industry=True):\n        &quot;&quot;&quot;完整的划分流程&quot;&quot;&quot;\n        # 1. 识别市场状态\n        self.identify_regimes()\n \n        # 2. 行业中性化\n        if self.industry_codes is not None and neutralize_industry:\n            X, y = self.industry_neutralize(X, y)\n \n        # 3. 三层划分\n        splits = self.train_val_test_split(X, y)\n \n        # 4. 验证无信息泄露\n        X_train, X_val = splits[&#039;train&#039;][0], splits[&#039;val&#039;][0]\n        self.validate_no_leakage(X_train, X_val)\n \n        return splits\n使用示例\n# 创建划分器\nsplitter = QuantDataSplitter(prices, industry_codes)\n \n# 划分数据\nsplits = splitter.split(X, y)\n \n# 提取各部分数据\nX_train, y_train = splits[&#039;train&#039;]\nX_val, y_val = splits[&#039;val&#039;]\nX_test, y_test = splits[&#039;test&#039;]\n \n# 打印统计信息\nprint(f&quot;Train: {len(X_train)} samples&quot;)\nprint(f&quot;Val:   {len(X_val)} samples&quot;)\nprint(f&quot;Test:  {len(X_test)} samples&quot;)\n4.2 数据划分检查清单\n划分前检查\n\n 数据按时间排序\n 移除未来信息泄露\n 确认无NaN值或合理处理\n 特征标准化在划分后进行\n 行业/市场状态信息完整\n\n划分后检查\n\n 训练集、验证集、测试集按时间顺序排列\n 验证集和测试集严格在训练集之后\n 检查分布差异（均值、方差）\n 确认目标变量分布合理\n 记录划分比例和时间范围\n\n评估指标检查\n\n 训练集、验证集、测试集分别评估\n 检查IC/Rank IC在不同子集上的稳定性\n 分析过拟合程度\n 检查样本外性能衰减\n\n5. 总结\n时序数据划分是量化投资中至关重要的环节，核心原则是严格遵守因果性约束。主要方法包括：\n\n时间序列交叉验证：保证训练集严格在验证集之前\n滚动窗口验证：适应数据分布随时间变化\n步进验证：模拟实际交易场景\n多维度划分：考虑市场状态、行业等因素\n\n正确的数据划分是构建稳健量化模型的基础，必须在模型开发初期就建立严格的划分规范。"},"quant/qlib/week2/03-模型训练":{"slug":"quant/qlib/week2/03-模型训练","filePath":"quant/qlib/week2/03-模型训练.md","title":"03-模型训练","links":[],"tags":[],"content":"模型训练\n1. LightGBM训练流程\n1.1 基础训练流程\n数据准备\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n \n# 加载数据\nX = np.random.randn(10000, 100)  # 10000个样本，100个特征\ny = np.random.randn(10000)  # 目标变量（预测收益率）\n \n# 划分训练集和验证集\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.2, shuffle=False  # 时序数据不要shuffle\n)\n \n# 创建LightGBM数据集\ntrain_data = lgb.Dataset(X_train, label=y_train)\nval_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n参数设置\nparams = {\n    # 基础参数\n    &#039;objective&#039;: &#039;regression&#039;,\n    &#039;metric&#039;: &#039;rmse&#039;,\n \n    # 模型复杂度\n    &#039;num_leaves&#039;: 31,\n    &#039;max_depth&#039;: -1,\n    &#039;min_data_in_leaf&#039;: 20,\n \n    # 学习参数\n    &#039;learning_rate&#039;: 0.05,\n    &#039;n_estimators&#039;: 1000,\n \n    # 正则化\n    &#039;lambda_l1&#039;: 0.0,\n    &#039;lambda_l2&#039;: 0.0,\n \n    # 采样\n    &#039;bagging_fraction&#039;: 0.8,\n    &#039;feature_fraction&#039;: 0.8,\n    &#039;bagging_freq&#039;: 5,\n \n    # 其他\n    &#039;verbosity&#039;: -1,\n    &#039;n_jobs&#039;: -1,\n}\n训练模型\n# 训练\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=1000,\n    valid_sets=[train_data, val_data],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50, verbose=True),\n        lgb.log_evaluation(period=100)\n    ]\n)\n \n# 预测\ny_pred_train = model.predict(X_train)\ny_pred_val = model.predict(X_val)\n \n# 评估\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom scipy.stats import pearsonr, spearmanr\n \ntrain_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\nval_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n \ntrain_r2 = r2_score(y_train, y_pred_train)\nval_r2 = r2_score(y_val, y_pred_val)\n \ntrain_ic = pearsonr(y_train, y_pred_train)[0]\nval_ic = pearsonr(y_val, y_pred_val)[0]\n \ntrain_rank_ic = spearmanr(y_train, y_pred_train)[0]\nval_rank_ic = spearmanr(y_val, y_pred_val)[0]\n \nprint(f&quot;Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}&quot;)\nprint(f&quot;Train R2: {train_r2:.4f}, Val R2: {val_r2:.4f}&quot;)\nprint(f&quot;Train IC: {train_ic:.4f}, Val IC: {val_ic:.4f}&quot;)\nprint(f&quot;Train Rank IC: {train_rank_ic:.4f}, Val Rank IC: {val_rank_ic:.4f}&quot;)\n1.2 使用sklearn API\n优势\n\n与sklearn生态系统无缝集成\n支持Pipeline和GridSearchCV\n更符合sklearn用户习惯\n\n代码示例\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n \n# 使用sklearn API\nmodel = LGBMRegressor(\n    objective=&#039;regression&#039;,\n    num_leaves=31,\n    learning_rate=0.05,\n    n_estimators=1000,\n    min_data_in_leaf=20,\n    bagging_fraction=0.8,\n    feature_fraction=0.8,\n    bagging_freq=5,\n    verbosity=-1,\n    n_jobs=-1,\n)\n \n# 训练\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_train, y_train), (X_val, y_val)],\n    eval_metric=&#039;rmse&#039;,\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50, verbose=False),\n        lgb.log_evaluation(period=100)\n    ]\n)\n \n# 预测\ny_pred_train = model.predict(X_train)\ny_pred_val = model.predict(X_val)\n超参数调优\n# 定义参数网格\nparam_grid = {\n    &#039;num_leaves&#039;: [31, 63, 127],\n    &#039;learning_rate&#039;: [0.01, 0.05, 0.1],\n    &#039;min_data_in_leaf&#039;: [10, 20, 50],\n    &#039;bagging_fraction&#039;: [0.7, 0.8, 0.9],\n    &#039;feature_fraction&#039;: [0.7, 0.8, 0.9],\n}\n \n# 时间序列交叉验证\ntscv = TimeSeriesSplit(n_splits=5)\n \n# 网格搜索\ngrid_search = GridSearchCV(\n    estimator=LGBMRegressor(objective=&#039;regression&#039;, n_estimators=1000),\n    param_grid=param_grid,\n    cv=tscv,\n    scoring=&#039;neg_root_mean_squared_error&#039;,\n    n_jobs=-1,\n    verbose=2\n)\n \ngrid_search.fit(X_train, y_train)\n \nprint(f&quot;最佳参数: {grid_search.best_params_}&quot;)\nprint(f&quot;最佳分数: {-grid_search.best_score_:.4f}&quot;)\n \n# 使用最佳模型\nbest_model = grid_search.best_estimator_\n2. 量化场景下的特殊训练策略\n2.1 针对IC优化的训练\n为什么IC重要？\n在量化投资中，我们关注的是预测值和实际值的排序相关性（IC），而非精确的预测误差（RMSE）。\n自定义损失函数\ndef ic_loss(preds, train_data):\n    &quot;&quot;&quot;\n    基于IC的损失函数\n \n    思路：最大化预测值和真实值的Pearson相关系数\n    &quot;&quot;&quot;\n    labels = train_data.get_label()\n \n    # 计算IC\n    ic = np.corrcoef(preds, labels)[0, 1]\n \n    # 梯度：对IC求导\n    # IC = Cov(pred, label) / (std(pred) * std(label))\n    # dIC/dpred = (label - ic * pred) / (std(pred) * std(label))\n \n    std_pred = np.std(preds)\n    std_label = np.std(labels)\n \n    grad = -(labels - ic * preds) / (std_pred * std_label)\n \n    # Hessian：二阶导数（近似）\n    hess = np.ones_like(preds)\n \n    return grad, hess\n \ndef ic_metric(preds, train_data):\n    &quot;&quot;&quot;IC评估指标&quot;&quot;&quot;\n    labels = train_data.get_label()\n    ic = np.corrcoef(preds, labels)[0, 1]\n    return &#039;ic&#039;, ic, True\n使用自定义损失函数\n# 定义参数\nparams = {\n    &#039;objective&#039;: &#039;custom&#039;,  # 使用自定义目标\n    &#039;metric&#039;: &#039;custom&#039;,\n    &#039;num_leaves&#039;: 31,\n    &#039;learning_rate&#039;: 0.05,\n    &#039;min_data_in_leaf&#039;: 20,\n    &#039;bagging_fraction&#039;: 0.8,\n    &#039;feature_fraction&#039;: 0.8,\n    &#039;bagging_freq&#039;: 5,\n    &#039;verbosity&#039;: -1,\n}\n \n# 训练\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=1000,\n    valid_sets=[train_data, val_data],\n    fobj=ic_loss,  # 自定义损失函数\n    feval=ic_metric,  # 自定义评估指标\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50, verbose=True),\n        lgb.log_evaluation(period=100)\n    ]\n)\nRank IC优化\ndef rank_ic_loss(preds, train_data):\n    &quot;&quot;&quot;\n    基于Rank IC的损失函数\n \n    思路：最大化预测值和真实值的Spearman秩相关系数\n    &quot;&quot;&quot;\n    labels = train_data.get_label()\n \n    # 计算排名\n    rank_pred = pd.Series(preds).rank()\n    rank_label = pd.Series(labels).rank()\n \n    # 计算Rank IC\n    rank_ic = np.corrcoef(rank_pred, rank_label)[0, 1]\n \n    # 梯度：近似\n    # 对排名求导比较复杂，这里使用近似方法\n    grad = -(rank_label - rank_ic * rank_pred) / (np.std(rank_pred) * np.std(rank_label))\n    hess = np.ones_like(preds)\n \n    return grad, hess\n \ndef rank_ic_metric(preds, train_data):\n    &quot;&quot;&quot;Rank IC评估指标&quot;&quot;&quot;\n    labels = train_data.get_label()\n    rank_pred = pd.Series(preds).rank()\n    rank_label = pd.Series(labels).rank()\n    rank_ic = np.corrcoef(rank_pred, rank_label)[0, 1]\n    return &#039;rank_ic&#039;, rank_ic, True\n2.2 分组训练（Group-wise Training）\n为什么需要分组？\n量化数据通常有层级结构：\n\n时间维度（不同交易日的样本）\n股票维度（不同股票的样本）\n\n需要保证同一组内的样本不被分到训练集和验证集。\n代码实现\n# 假设我们有时间和股票信息\ntimes = np.arange(len(X)) // 21  # 假设每21个交易日为一个时间组\nstocks = np.arange(len(X)) % 100  # 假设有100只股票\n \n# 创建分组数据集\ntrain_data = lgb.Dataset(X_train, label=y_train, group=times[train_idx])\nval_data = lgb.Dataset(X_val, label=y_val, group=times[val_idx], reference=train_data)\nLambda Rank训练\n# 使用Lambda Rank（适合排序任务）\nparams = {\n    &#039;objective&#039;: &#039;lambdarank&#039;,\n    &#039;metric&#039;: &#039;ndcg&#039;,\n    &#039;num_leaves&#039;: 31,\n    &#039;learning_rate&#039;: 0.05,\n    &#039;min_data_in_leaf&#039;: 20,\n    &#039;verbosity&#039;: -1,\n}\n \nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=1000,\n    valid_sets=[train_data, val_data],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50, verbose=True),\n        lgb.log_evaluation(period=100)\n    ]\n)\n2.3 在线学习（Online Learning）\n适用场景\n\n数据源源不断产生\n需要实时更新模型\n市场环境快速变化\n\n增量训练\nclass OnlineLightGBM:\n    &quot;&quot;&quot;\n    在线学习LightGBM\n \n    策略：\n    1. 初始训练：使用历史数据\n    2. 增量更新：定期用新数据更新模型\n    3. 窗口控制：保持训练窗口大小\n    &quot;&quot;&quot;\n \n    def __init__(self, params, window_size=1000, update_freq=100):\n        self.params = params\n        self.window_size = window_size\n        self.update_freq = update_freq\n        self.model = None\n        self.train_data = None\n \n    def initial_train(self, X, y):\n        &quot;&quot;&quot;初始训练&quot;&quot;&quot;\n        self.train_data = lgb.Dataset(X, label=y)\n        self.model = lgb.train(\n            self.params,\n            self.train_data,\n            num_boost_round=1000,\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50, verbose=True),\n                lgb.log_evaluation(period=100)\n            ]\n        )\n \n    def update(self, X_new, y_new):\n        &quot;&quot;&quot;增量更新&quot;&quot;&quot;\n        if self.train_data is None:\n            self.initial_train(X_new, y_new)\n            return\n \n        # 合并新数据\n        X_train = self.train_data.data\n        y_train = self.train_data.label\n \n        X_combined = np.vstack([X_train, X_new])\n        y_combined = np.hstack([y_train, y_new])\n \n        # 窗口控制\n        if len(X_combined) &gt; self.window_size:\n            X_combined = X_combined[-self.window_size:]\n            y_combined = y_combined[-self.window_size:]\n \n        # 重新训练\n        self.train_data = lgb.Dataset(X_combined, label=y_combined)\n        self.model = lgb.train(\n            self.params,\n            self.train_data,\n            num_boost_round=100,\n            init_model=self.model,  # 继续训练\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50, verbose=True),\n                lgb.log_evaluation(period=100)\n            ]\n        )\n使用示例\n# 创建在线学习模型\nonline_model = OnlineLightGBM(params, window_size=2520, update_freq=21)\n \n# 初始训练\nX_initial, y_initial = X[:2520], y[:2520]\nonline_model.initial_train(X_initial, y_initial)\n \n# 在线更新\nfor i in range(2520, len(X), 21):\n    X_batch, y_batch = X[i:i+21], y[i:i+21]\n    online_model.update(X_batch, y_batch)\n \n    # 预测\n    y_pred = online_model.model.predict(X[i+21:i+42])\n3. 模型训练进阶技巧\n3.1 学习率调度\n学习率衰减\n# 定义学习率衰减函数\ndef learning_rate_decay(current_iter, total_iter, init_lr=0.1, decay_power=0.99):\n    &quot;&quot;&quot;\n    学习率衰减\n \n    参数:\n        current_iter: 当前迭代次数\n        total_iter: 总迭代次数\n        init_lr: 初始学习率\n        decay_power: 衰减因子\n    &quot;&quot;&quot;\n    return init_lr * (decay_power ** current_iter)\n \n# 在训练中使用\nnum_iterations = 1000\ncallbacks = [\n    lgb.early_stopping(stopping_rounds=50, verbose=True),\n    lgb.log_evaluation(period=100),\n    lgb.reset_parameter(\n        learning_rate=lambda iter: learning_rate_decay(iter, num_iterations, init_lr=0.1, decay_power=0.99)\n    )\n]\n \nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=num_iterations,\n    valid_sets=[train_data, val_data],\n    callbacks=callbacks\n)\n余弦退火\ndef cosine_annealing_lr(current_iter, total_iter, init_lr=0.1, min_lr=0.001):\n    &quot;&quot;&quot;\n    余弦退火学习率\n    &quot;&quot;&quot;\n    cosine = (1 + np.cos(np.pi * current_iter / total_iter)) / 2\n    return min_lr + (init_lr - min_lr) * cosine\n \ncallbacks = [\n    lgb.early_stopping(stopping_rounds=50, verbose=True),\n    lgb.log_evaluation(period=100),\n    lgb.reset_parameter(\n        learning_rate=lambda iter: cosine_annealing_lr(iter, num_iterations, init_lr=0.1, min_lr=0.001)\n    )\n]\n3.2 特征采样策略\n动态特征采样\nclass DynamicFeatureSampler:\n    &quot;&quot;&quot;\n    动态特征采样器\n \n    策略：\n    1. 早期：使用所有特征，快速学习\n    2. 中期：根据特征重要性采样\n    3. 后期：只使用重要特征，精细调优\n    &quot;&quot;&quot;\n \n    def __init__(self, n_features, importance=None):\n        self.n_features = n_features\n        self.importance = importance\n        self.stage = &#039;early&#039;\n \n    def get_feature_fraction(self, iteration, total_iterations):\n        &quot;&quot;&quot;\n        根据迭代阶段返回特征采样比例\n        &quot;&quot;&quot;\n        early_ratio = iteration / total_iterations\n \n        if early_ratio &lt; 0.3:\n            # 早期：使用100%特征\n            return 1.0\n        elif early_ratio &lt; 0.7:\n            # 中期：根据重要性采样\n            return 0.8\n        else:\n            # 后期：只使用重要特征\n            return 0.5\n \n# 使用动态特征采样\nsampler = DynamicFeatureSampler(X.shape[1])\ncallbacks = [\n    lgb.early_stopping(stopping_rounds=50, verbose=True),\n    lgb.log_evaluation(period=100),\n    lgb.reset_parameter(\n        feature_fraction=lambda iter: sampler.get_feature_fraction(iter, num_iterations)\n    )\n]\n3.3 类别不平衡处理\n权重调整\n# 计算正负样本比例\npos_samples = np.sum(y &gt; 0)\nneg_samples = np.sum(y &lt;= 0)\nscale_pos_weight = neg_samples / pos_samples\n \nprint(f&quot;正样本: {pos_samples}, 负样本: {neg_samples}&quot;)\nprint(f&quot;权重比例: {scale_pos_weight:.2f}&quot;)\n \n# 设置参数\nparams = {\n    &#039;objective&#039;: &#039;regression&#039;,\n    &#039;metric&#039;: &#039;rmse&#039;,\n    &#039;scale_pos_weight&#039;: scale_pos_weight,  # 正负样本权重\n    &#039;num_leaves&#039;: 31,\n    &#039;learning_rate&#039;: 0.05,\n    &#039;min_data_in_leaf&#039;: 20,\n    &#039;verbosity&#039;: -1,\n}\n自定义样本权重\n# 计算样本权重\nsample_weights = np.ones_like(y)\n \n# 对难预测样本给予更高权重\ntrain_residuals = y_train - model.predict(X_train)\nresidual_std = np.std(train_residuals)\n \n# 对残差大的样本给予更高权重\nsample_weights = 1 + np.abs(train_residuals) / residual_std\n \n# 创建带权重的数据集\ntrain_data_weighted = lgb.Dataset(X_train, label=y_train, weight=sample_weights)\n \n# 训练\nmodel = lgb.train(\n    params,\n    train_data_weighted,\n    num_boost_round=1000,\n    valid_sets=[train_data_weighted, val_data],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50, verbose=True),\n        lgb.log_evaluation(period=100)\n    ]\n)\n3.4 早停机制详解\n早停 (Early Stopping) 的作用：\n训练曲线示意:\n\nMSE ↑\n    │   训练集\n    │     ╲\n    │      ╲─────────────→  继续下降 (记忆数据)\n    │       验证集\n    │         ╲\n    │          ╲___          最佳点\n    │              ╲_______↗\n    │                 ╱ 过拟合开始!\n    │               ╱\n    └────────────────────────→ 迭代次数\n\n工作原理：\n\n每轮计算验证集 MSE\n如果连续 N 轮没有改进 → 停止训练\n返回最佳迭代的模型\n\n代码：\nlgb.early_stopping(stopping_rounds=30)  # 30轮无改进则停止\n完整示例：\n# 训练模型，带早停\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=500,                # 最多500棵树\n    valid_sets=[train_data, val_data],\n    valid_names=[&#039;train&#039;, &#039;valid&#039;],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=30),  # 30轮无改进则停止\n        lgb.log_evaluation(period=50)            # 每50轮打印\n    ]\n)\n \n# 示例输出:\n# Training until validation scores don&#039;t improve for 30 rounds\n# [50]    train&#039;s l2: 0.00347577    valid&#039;s l2: 0.00422996\n# Early stopping, best iteration is:\n# [57]    train&#039;s l2: 0.00345507    valid&#039;s l2: 0.00422585\n \nprint(f&quot;✅ 训练完成!&quot;)\nprint(f&quot;   训练时间: 0:00:00.584752&quot;)\nprint(f&quot;   最佳迭代: {model.best_iteration}&quot;)\nprint(f&quot;   树的数量: {model.num_trees()}&quot;)\n3.5 预测与简单评估\n# 预测\ny_pred_train = model.predict(X_train)\ny_pred_valid = model.predict(X_valid)\ny_pred_test = model.predict(X_test)\n \n# 简单评估\nfrom sklearn.metrics import mean_squared_error, r2_score\n \nmse_train = mean_squared_error(y_train, y_pred_train)\nmse_valid = mean_squared_error(y_valid, y_pred_valid)\nmse_test = mean_squared_error(y_test, y_pred_test)\n \nprint(f&quot;MSE 评估:&quot;)\nprint(f&quot;  训练集: {mse_train:.6f}&quot;)\nprint(f&quot;  验证集: {mse_valid:.6f}&quot;)\nprint(f&quot;  测试集: {mse_test:.6f}&quot;)\n \n# 检查过拟合\nif mse_train &lt; mse_valid * 0.5:\n    print(f&quot;⚠️ 警告: 训练集 MSE 远低于验证集，可能过拟合!&quot;)\nelse:\n    print(f&quot;✅ 过拟合检查通过&quot;)\n3.6 早停策略优化\n多层早停\nclass MultiLevelEarlyStopping:\n    &quot;&quot;&quot;\n    多层早停策略\n \n    策略：\n    1. 训练集指标：监控过拟合\n    2. 验证集IC：监控预测能力\n    3. IC衰减：监控性能衰减\n    &quot;&quot;&quot;\n \n    def __init__(self, stopping_rounds=100, min_delta=0.001):\n        self.stopping_rounds = stopping_rounds\n        self.min_delta = min_delta\n        self.best_ic = -np.inf\n        self.best_round = 0\n        self.ic_history = []\n \n    def __call__(self, env):\n        &quot;&quot;&quot;\n        回调函数\n        &quot;&quot;&quot;\n        # 获取验证集IC\n        val_data = env.validation_data\n        y_pred = env.model.predict(val_data.data)\n        y_true = val_data.label\n        ic = np.corrcoef(y_pred, y_true)[0, 1]\n \n        self.ic_history.append(ic)\n \n        # 检查是否提升\n        if ic &gt; self.best_ic + self.min_delta:\n            self.best_ic = ic\n            self.best_round = env.iteration\n        elif env.iteration - self.best_round &gt;= self.stopping_rounds:\n            # 早停\n            raise StopIteration\n \n        print(f&quot;Round {env.iteration}: IC = {ic:.4f}, Best IC = {self.best_ic:.4f}&quot;)\n \n# 使用多层早停\ncallbacks = [\n    lgb.log_evaluation(period=100),\n    MultiLevelEarlyStopping(stopping_rounds=100, min_delta=0.001)\n]\n4. 分布式训练\n4.1 多GPU训练\n# 使用多GPU训练\nparams = {\n    &#039;objective&#039;: &#039;regression&#039;,\n    &#039;metric&#039;: &#039;rmse&#039;,\n    &#039;num_leaves&#039;: 31,\n    &#039;learning_rate&#039;: 0.05,\n    &#039;device&#039;: &#039;gpu&#039;,  # 使用GPU\n    &#039;gpu_platform_id&#039;: 0,\n    &#039;gpu_device_id&#039;: 0,\n    &#039;num_gpu&#039;: 2,  # 使用2个GPU\n    &#039;verbosity&#039;: -1,\n}\n \nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=1000,\n    valid_sets=[train_data, val_data],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50, verbose=True),\n        lgb.log_evaluation(period=100)\n    ]\n)\n4.2 多机训练\n# 假设有多个机器，每台机器处理一部分数据\n \n# 主机（master）\nparams = {\n    &#039;objective&#039;: &#039;regression&#039;,\n    &#039;metric&#039;: &#039;rmse&#039;,\n    &#039;num_leaves&#039;: 31,\n    &#039;learning_rate&#039;: 0.05,\n    &#039;tree_learner&#039;: &#039;data_parallel&#039;,  # 数据并行\n    &#039;num_machines&#039;: 4,  # 4台机器\n    &#039;machines&#039;: &#039;192.168.1.1:12345,192.168.1.2:12345,192.168.1.3:12345,192.168.1.4:12345&#039;,\n    &#039;verbosity&#039;: -1,\n}\n \nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=1000,\n    valid_sets=[train_data, val_data],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50, verbose=True),\n        lgb.log_evaluation(period=100)\n    ]\n)\n5. 模型保存与加载\n5.1 保存模型\n# 保存模型\nmodel.save_model(&#039;lightgbm_model.txt&#039;)\n \n# 保存为JSON格式（更易读）\nmodel.save_model(&#039;lightgbm_model.json&#039;)\n \n# 保存模型和数据\nimport joblib\njoblib.dump(model, &#039;lightgbm_model.pkl&#039;)\n5.2 加载模型\n# 加载模型\nmodel = lgb.Booster(model_file=&#039;lightgbm_model.txt&#039;)\n \n# 加载并继续训练\nmodel = lgb.Booster(model_file=&#039;lightgbm_model.txt&#039;)\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=100,\n    init_model=model,  # 继续训练\n    valid_sets=[train_data, val_data],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50, verbose=True),\n        lgb.log_evaluation(period=100)\n    ]\n)\n5.3 模型版本管理\nimport os\nimport json\nfrom datetime import datetime\n \nclass ModelVersioning:\n    &quot;&quot;&quot;\n    模型版本管理\n \n    功能：\n    1. 保存模型及其元数据\n    2. 版本控制\n    3. 模型比较\n    &quot;&quot;&quot;\n \n    def __init__(self, model_dir=&#039;models&#039;):\n        self.model_dir = model_dir\n        os.makedirs(model_dir, exist_ok=True)\n \n    def save_model(self, model, metadata):\n        &quot;&quot;&quot;\n        保存模型及元数据\n \n        metadata: {\n            &#039;train_ic&#039;: 0.05,\n            &#039;val_ic&#039;: 0.03,\n            &#039;params&#039;: {...},\n            &#039;train_date&#039;: &#039;2024-01-01&#039;,\n            ...\n        }\n        &quot;&quot;&quot;\n        version = datetime.now().strftime(&#039;%Y%m%d_%H%M%S&#039;)\n        model_name = f&quot;model_{version}&quot;\n \n        # 保存模型\n        model_path = os.path.join(self.model_dir, f&quot;{model_name}.txt&quot;)\n        model.save_model(model_path)\n \n        # 保存元数据\n        metadata_path = os.path.join(self.model_dir, f&quot;{model_name}_metadata.json&quot;)\n        with open(metadata_path, &#039;w&#039;) as f:\n            json.dump(metadata, f, indent=2)\n \n        print(f&quot;模型已保存: {model_name}&quot;)\n \n        return model_name\n \n    def load_model(self, model_name):\n        &quot;&quot;&quot;加载模型&quot;&quot;&quot;\n        model_path = os.path.join(self.model_dir, f&quot;{model_name}.txt&quot;)\n        metadata_path = os.path.join(self.model_dir, f&quot;{model_name}_metadata.json&quot;)\n \n        # 加载模型\n        model = lgb.Booster(model_file=model_path)\n \n        # 加载元数据\n        with open(metadata_path, &#039;r&#039;) as f:\n            metadata = json.load(f)\n \n        return model, metadata\n \n    def list_models(self):\n        &quot;&quot;&quot;列出所有模型&quot;&quot;&quot;\n        models = []\n        for file in os.listdir(self.model_dir):\n            if file.endswith(&#039;.txt&#039;):\n                model_name = file.replace(&#039;.txt&#039;, &#039;&#039;)\n                metadata_path = os.path.join(self.model_dir, f&quot;{model_name}_metadata.json&quot;)\n \n                if os.path.exists(metadata_path):\n                    with open(metadata_path, &#039;r&#039;) as f:\n                        metadata = json.load(f)\n                    models.append((model_name, metadata))\n \n        return models\n使用示例\n# 创建版本管理器\nversion_manager = ModelVersioning()\n \n# 保存模型\nmetadata = {\n    &#039;train_ic&#039;: train_ic,\n    &#039;val_ic&#039;: val_ic,\n    &#039;params&#039;: params,\n    &#039;train_date&#039;: datetime.now().strftime(&#039;%Y-%m-%d&#039;),\n    &#039;train_samples&#039;: len(X_train),\n    &#039;val_samples&#039;: len(X_val),\n}\n \nmodel_name = version_manager.save_model(model, metadata)\n \n# 列出所有模型\nmodels = version_manager.list_models()\nfor name, meta in models:\n    print(f&quot;{name}: IC={meta[&#039;val_ic&#039;]:.4f}, Date={meta[&#039;train_date&#039;]}&quot;)\n6. 总结\nLightGBM模型训练在量化场景中需要特别关注：\n\n训练流程：数据准备、参数设置、模型训练、评估\n特殊训练策略：针对IC优化、分组训练、在线学习\n进阶技巧：学习率调度、特征采样、类别不平衡处理\n分布式训练：多GPU、多机训练\n模型管理：保存、加载、版本控制\n\n正确的训练策略是提升模型性能的关键，需要根据具体场景灵活调整。"},"quant/qlib/week2/04-IC-Rank-IC评估指标":{"slug":"quant/qlib/week2/04-IC-Rank-IC评估指标","filePath":"quant/qlib/week2/04-IC-Rank-IC评估指标.md","title":"04-IC-Rank-IC评估指标","links":[],"tags":[],"content":"IC/Rank IC 评估指标\n1. IC（Information Coefficient）基础\n1.1 IC的定义\n数学定义\nIC（Information Coefficient，信息系数）是预测值与实际值的Pearson相关系数：\nIC = \\rho(\\hat{y}, y) = \\frac{Cov(\\hat{y}, y)}{\\sigma_{\\hat{y}} \\sigma_y}\n其中：\n\n\\hat{y} 是预测值（预测收益率）\ny 是实际值（实际收益率）\n\\rho 是Pearson相关系数\nCov 是协方差\n\\sigma 是标准差\n\n取值范围\n\nIC ∈ [-1, 1]\nIC = 1：完全正相关，预测完全准确\nIC = 0：无相关性，预测无效\nIC = -1：完全负相关，预测完全相反\n\n计算示例\nimport numpy as np\nfrom scipy.stats import pearsonr\n \n# 假设我们有预测值和实际值\ny_pred = np.array([0.01, 0.02, -0.01, 0.03, 0.005])\ny_true = np.array([0.015, 0.025, -0.005, 0.035, 0.01])\n \n# 计算IC\nic = pearsonr(y_pred, y_true)[0]\nprint(f&quot;IC = {ic:.4f}&quot;)\n \n# 手动计算\ndef calculate_ic(y_pred, y_true):\n    mean_pred = np.mean(y_pred)\n    mean_true = np.mean(y_true)\n \n    std_pred = np.std(y_pred)\n    std_true = np.std(y_true)\n \n    covariance = np.mean((y_pred - mean_pred) * (y_true - mean_true))\n    ic = covariance / (std_pred * std_true)\n \n    return ic\n \nic_manual = calculate_ic(y_pred, y_true)\nprint(f&quot;手动计算 IC = {ic_manual:.4f}&quot;)\n1.2 IC在量化中的意义\n量化投资的核心问题\n量化投资的核心是预测股票的相对强弱，而非精确的收益率预测。\nIC的优势\n\n非线性不敏感：只关心排序，不关心绝对值\n稳健性：对异常值不敏感\n可解释性：直接衡量预测能力\n\nIC与收益的关系\n假设我们根据预测值构建多空组合：\n\\text{Return}_{t} = \\frac{1}{N_{\\text{long}}} \\sum_{i \\in \\text{long}} r_{i,t} - \\frac{1}{N_{\\text{short}}} \\sum_{i \\in \\text{short}} r_{i,t}\n其中多空组合根据预测值排序选择。\n理论上，IC越高，组合收益越高：\nE[\\text{Return}] \\propto IC \\times \\sigma_r\n其中 \\sigma_r 是收益率的标准差。\n2. Rank IC\n2.1 Rank IC的定义\n数学定义\nRank IC是预测值排序与实际值排序的Spearman秩相关系数：\n\\text{Rank IC} = \\rho(\\text{rank}(\\hat{y}), \\text{rank}(y))\n其中 \\text{rank}(\\cdot) 是排名函数。\n计算示例\nfrom scipy.stats import spearmanr\n \n# 假设我们有预测值和实际值\ny_pred = np.array([0.01, 0.02, -0.01, 0.03, 0.005])\ny_true = np.array([0.015, 0.025, -0.005, 0.035, 0.01])\n \n# 计算Rank IC\nrank_ic = spearmanr(y_pred, y_true)[0]\nprint(f&quot;Rank IC = {rank_ic:.4f}&quot;)\n \n# 手动计算\ndef calculate_rank_ic(y_pred, y_true):\n    rank_pred = pd.Series(y_pred).rank()\n    rank_true = pd.Series(y_true).rank()\n \n    # 使用Pearson相关系数计算排名的相关性\n    ic = pearsonr(rank_pred, rank_true)[0]\n \n    return ic\n \nrank_ic_manual = calculate_rank_ic(y_pred, y_true)\nprint(f&quot;手动计算 Rank IC = {rank_ic_manual:.4f}&quot;)\n2.2 IC与Rank IC的区别\n区别对比\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n特性ICRank IC相关系数类型PearsonSpearman敏感性对数值敏感只对排序敏感异常值敏感不敏感适用场景线性关系单调关系计算复杂度O(N)O(N log N)\n数值示例\n# 示例：异常值的影响\ny_pred = np.array([0.01, 0.02, -0.01, 0.03, 0.005])\ny_true = np.array([0.015, 0.025, -0.005, 0.035, 0.01])\n \n# 加入异常值\ny_pred_outlier = np.array([0.01, 0.02, -0.01, 0.03, 10.0])  # 第5个值异常大\ny_true_outlier = np.array([0.015, 0.025, -0.005, 0.035, 0.01])\n \n# 计算IC\nic_normal = pearsonr(y_pred, y_true)[0]\nic_outlier = pearsonr(y_pred_outlier, y_true_outlier)[0]\n \n# 计算Rank IC\nrank_ic_normal = spearmanr(y_pred, y_true)[0]\nrank_ic_outlier = spearmanr(y_pred_outlier, y_true_outlier)[0]\n \nprint(f&quot;正常数据: IC = {ic_normal:.4f}, Rank IC = {rank_ic_normal:.4f}&quot;)\nprint(f&quot;异常数据: IC = {ic_outlier:.4f}, Rank IC = {rank_ic_outlier:.4f}&quot;)\n \n# 结果：IC受异常值影响大，Rank IC几乎不变\n量化场景的选择\n在量化投资中，通常更关注Rank IC，因为：\n\n我们关注的是股票的相对排名\n收益率存在异常值（涨跌停、停牌等）\n排序比精确值更稳定\n\n3. IC的统计显著性检验\n3.1 t检验\n假设检验\n\nH0（原假设）：IC = 0（预测无能力）\nH1（备择假设）：IC ≠ 0（预测有能力）\n\n检验统计量\nt = \\frac{IC \\times \\sqrt{N - 2}}{\\sqrt{1 - IC^2}}\n其中 N 是样本数量。\n代码实现\nfrom scipy.stats import t\n \ndef ic_t_test(ic, n_samples, alpha=0.05):\n    &quot;&quot;&quot;\n    IC的t检验\n \n    参数:\n        ic: IC值\n        n_samples: 样本数量\n        alpha: 显著性水平\n \n    返回:\n        t_statistic: t统计量\n        p_value: p值\n        is_significant: 是否显著\n    &quot;&quot;&quot;\n    # 计算t统计量\n    t_statistic = ic * np.sqrt(n_samples - 2) / np.sqrt(1 - ic ** 2)\n \n    # 计算p值（双尾检验）\n    p_value = 2 * (1 - t.cdf(abs(t_statistic), df=n_samples - 2))\n \n    # 判断显著性\n    is_significant = p_value &lt; alpha\n \n    return t_statistic, p_value, is_significant\n \n# 示例\nic = 0.05\nn_samples = 252  # 一年的交易日\n \nt_stat, p_val, sig = ic_t_test(ic, n_samples)\n \nprint(f&quot;IC = {ic:.4f}&quot;)\nprint(f&quot;t统计量 = {t_stat:.4f}&quot;)\nprint(f&quot;p值 = {p_val:.4f}&quot;)\nprint(f&quot;是否显著: {sig}&quot;)\n3.2 IC的置信区间\n置信区间计算\nIC的置信区间可以通过Fisher变换计算：\nz = \\frac{1}{2} \\ln\\left(\\frac{1 + IC}{1 - IC}\\right)\nz的标准误差：\nSE_z = \\frac{1}{\\sqrt{N - 3}}\n置信区间：\nCI_{IC} = \\tanh\\left(z \\pm z_{1-\\alpha/2} \\times SE_z\\right)\n代码实现\nfrom scipy.stats import norm\n \ndef ic_confidence_interval(ic, n_samples, alpha=0.05):\n    &quot;&quot;&quot;\n    IC的置信区间\n \n    参数:\n        ic: IC值\n        n_samples: 样本数量\n        alpha: 显著性水平\n \n    返回:\n        (lower, upper): 置信区间\n    &quot;&quot;&quot;\n    # Fisher变换\n    z = 0.5 * np.log((1 + ic) / (1 - ic))\n \n    # 计算标准误差\n    se_z = 1 / np.sqrt(n_samples - 3)\n \n    # 计算置信区间\n    z_critical = norm.ppf(1 - alpha / 2)\n    z_lower = z - z_critical * se_z\n    z_upper = z + z_critical * se_z\n \n    # 反Fisher变换\n    ic_lower = np.tanh(z_lower)\n    ic_upper = np.tanh(z_upper)\n \n    return ic_lower, ic_upper\n \n# 示例\nic = 0.05\nn_samples = 252\n \nlower, upper = ic_confidence_interval(ic, n_samples)\nprint(f&quot;IC = {ic:.4f}&quot;)\nprint(f&quot;95%置信区间: [{lower:.4f}, {upper:.4f}]&quot;)\n4. IC的时序分析\n4.1 滚动IC\n滚动IC的定义\n滚动IC是在固定时间窗口内计算的IC序列，用于分析IC的稳定性。\n代码实现\ndef rolling_ic(y_pred, y_true, window=20):\n    &quot;&quot;&quot;\n    滚动IC计算\n \n    参数:\n        y_pred: 预测值序列，shape=[n_samples]\n        y_true: 实际值序列，shape=[n_samples]\n        window: 滚动窗口大小\n \n    返回:\n        ic_series: IC序列\n    &quot;&quot;&quot;\n    n_samples = len(y_pred)\n    ic_series = []\n \n    for i in range(window, n_samples + 1):\n        window_pred = y_pred[i-window:i]\n        window_true = y_true[i-window:i]\n \n        ic = pearsonr(window_pred, window_true)[0]\n        ic_series.append(ic)\n \n    return np.array(ic_series)\n \n# 示例\ny_pred = np.random.randn(500)\ny_true = y_pred * 0.5 + np.random.randn(500) * 0.5\n \nic_series = rolling_ic(y_pred, y_true, window=20)\n \nprint(f&quot;平均IC: {np.mean(ic_series):.4f}&quot;)\nprint(f&quot;IC标准差: {np.std(ic_series):.4f}&quot;)\nprint(f&quot;IC最大值: {np.max(ic_series):.4f}&quot;)\nprint(f&quot;IC最小值: {np.min(ic_series):.4f}&quot;)\n滚动IC可视化\nimport matplotlib.pyplot as plt\n \ndef plot_rolling_ic(y_pred, y_true, window=20):\n    &quot;&quot;&quot;\n    绘制滚动IC\n    &quot;&quot;&quot;\n    ic_series = rolling_ic(y_pred, y_true, window)\n \n    plt.figure(figsize=(12, 6))\n    plt.plot(ic_series, label=&#039;Rolling IC&#039;)\n    plt.axhline(y=0, color=&#039;r&#039;, linestyle=&#039;--&#039;, label=&#039;Zero Line&#039;)\n    plt.axhline(y=np.mean(ic_series), color=&#039;g&#039;, linestyle=&#039;--&#039;, label=&#039;Mean IC&#039;)\n    plt.xlabel(&#039;Time&#039;)\n    plt.ylabel(&#039;IC&#039;)\n    plt.title(f&#039;Rolling IC (Window={window})&#039;)\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n \n# 使用示例\nplot_rolling_ic(y_pred, y_true, window=20)\n4.2 IC衰减分析\nIC衰减的定义\nIC衰减是指预测值与未来不同周期实际值的IC，用于分析预测的时效性。\n代码实现\ndef ic_decay(y_pred, y_true, max_lag=10):\n    &quot;&quot;&quot;\n    IC衰减分析\n \n    参数:\n        y_pred: 预测值序列\n        y_true: 实际值序列\n        max_lag: 最大滞后阶数\n \n    返回:\n        ic_decay_series: IC衰减序列\n    &quot;&quot;&quot;\n    ic_decay_series = []\n \n    for lag in range(max_lag + 1):\n        # 对齐数据\n        pred = y_pred[:len(y_pred) - lag]\n        true = y_true[lag:len(y_true)]\n \n        # 计算IC\n        ic = pearsonr(pred, true)[0]\n        ic_decay_series.append(ic)\n \n    return np.array(ic_decay_series)\n \n# 示例\ny_pred = np.random.randn(500)\ny_true = np.random.randn(500)\n \nic_decay_series = ic_decay(y_pred, y_true, max_lag=10)\n \nfor lag, ic in enumerate(ic_decay_series):\n    print(f&quot;Lag {lag}: IC = {ic:.4f}&quot;)\nIC衰减可视化\ndef plot_ic_decay(y_pred, y_true, max_lag=10):\n    &quot;&quot;&quot;\n    绘制IC衰减曲线\n    &quot;&quot;&quot;\n    ic_decay_series = ic_decay(y_pred, y_true, max_lag)\n \n    plt.figure(figsize=(10, 6))\n    plt.bar(range(max_lag + 1), ic_decay_series)\n    plt.xlabel(&#039;Lag&#039;)\n    plt.ylabel(&#039;IC&#039;)\n    plt.title(&#039;IC Decay Analysis&#039;)\n    plt.grid(True, axis=&#039;y&#039;)\n    plt.show()\n \n# 使用示例\nplot_ic_decay(y_pred, y_true, max_lag=10)\n5. IR（Information Ratio）\n5.1 IR的定义\n数学定义\nIR（Information Ratio，信息比率）是IC的均值除以IC的标准差：\nIR = \\frac{E[IC]}{\\sigma_{IC}}\n其中：\n\nE[IC] 是IC的期望（均值）\n\\sigma_{IC} 是IC的标准差\n\n意义\nIR衡量预测能力的稳定性：\n\nIR高：IC均值高且稳定\nIR低：IC均值低或不稳定\n\n代码实现\ndef calculate_ir(ic_series):\n    &quot;&quot;&quot;\n    计算IR\n \n    参数:\n        ic_series: IC序列\n \n    返回:\n        ir: 信息比率\n        ic_mean: IC均值\n        ic_std: IC标准差\n    &quot;&quot;&quot;\n    ic_mean = np.mean(ic_series)\n    ic_std = np.std(ic_series, ddof=1)  # 使用样本标准差\n \n    if ic_std == 0:\n        ir = 0\n    else:\n        ir = ic_mean / ic_std\n \n    return ir, ic_mean, ic_std\n \n# 示例\nic_series = np.array([0.05, 0.03, 0.07, 0.04, 0.06])\n \nir, ic_mean, ic_std = calculate_ir(ic_series)\n \nprint(f&quot;IC均值: {ic_mean:.4f}&quot;)\nprint(f&quot;IC标准差: {ic_std:.4f}&quot;)\nprint(f&quot;IR: {ir:.4f}&quot;)\n5.2 IR与IC的关系\n关系分析\nIR = \\frac{E[IC]}{\\sigma_{IC}}\n\nIC高，IR高：预测能力强且稳定\nIC高，IR低：预测能力强但不稳定\nIC低，IR高：预测能力弱但稳定\nIC低，IR低：预测能力弱且不稳定\n\n示例对比\n# 场景1：IC高且稳定\nic_series_1 = np.array([0.05, 0.05, 0.05, 0.05, 0.05])\nir_1, mean_1, std_1 = calculate_ir(ic_series_1)\nprint(f&quot;场景1 - IC={mean_1:.4f}, IR={ir_1:.4f} (高且稳定)&quot;)\n \n# 场景2：IC高但不稳定\nic_series_2 = np.array([0.10, 0.00, 0.10, 0.00, 0.10])\nir_2, mean_2, std_2 = calculate_ir(ic_series_2)\nprint(f&quot;场景2 - IC={mean_2:.4f}, IR={ir_2:.4f} (高但不稳定)&quot;)\n \n# 场景3：IC低但稳定\nic_series_3 = np.array([0.02, 0.02, 0.02, 0.02, 0.02])\nir_3, mean_3, std_3 = calculate_ir(ic_series_3)\nprint(f&quot;场景3 - IC={mean_3:.4f}, IR={ir_3:.4f} (低但稳定)&quot;)\n5.3 年化IR\n年化IR的计算\n如果IC按日计算，年化IR为：\nIR_{annual} = IR_{daily} \\times \\sqrt{252}\n其中252是每年的交易日数量。\n代码实现\ndef annualized_ir(ic_series, periods_per_year=252):\n    &quot;&quot;&quot;\n    年化IR\n \n    参数:\n        ic_series: IC序列\n        periods_per_year: 每年周期数\n \n    返回:\n        ir_annual: 年化IR\n    &quot;&quot;&quot;\n    ir, ic_mean, ic_std = calculate_ir(ic_series)\n    ir_annual = ir * np.sqrt(periods_per_year)\n \n    return ir_annual\n \n# 示例：每日IC\ndaily_ic_series = np.random.randn(252) * 0.01 + 0.03\n \nir_annual = annualized_ir(daily_ic_series)\nprint(f&quot;年化IR: {ir_annual:.4f}&quot;)\n6. IC在不同子集上的表现\n6.1 按市场状态分析IC\n市场状态分类\ndef analyze_ic_by_market_regime(y_pred, y_true, regimes):\n    &quot;&quot;&quot;\n    按市场状态分析IC\n \n    参数:\n        y_pred: 预测值\n        y_true: 实际值\n        regimes: 市场状态（-1=熊市, 0=震荡, 1=牛市）\n \n    返回:\n        dict: {regime: {&#039;ic&#039;: ic, &#039;n_samples&#039;: n_samples}}\n    &quot;&quot;&quot;\n    results = {}\n \n    for regime in [-1, 0, 1]:\n        mask = regimes == regime\n        y_pred_regime = y_pred[mask]\n        y_true_regime = y_true[mask]\n \n        if len(y_pred_regime) &gt; 0:\n            ic = pearsonr(y_pred_regime, y_true_regime)[0]\n            results[regime] = {\n                &#039;ic&#039;: ic,\n                &#039;n_samples&#039;: len(y_pred_regime)\n            }\n \n    return results\n \n# 示例\nregimes = np.random.choice([-1, 0, 1], size=len(y_pred), p=[0.2, 0.6, 0.2])\n \nresults = analyze_ic_by_market_regime(y_pred, y_true, regimes)\n \nfor regime in [-1, 0, 1]:\n    regime_name = {-1: &#039;熊市&#039;, 0: &#039;震荡市&#039;, 1: &#039;牛市&#039;}[regime]\n    if regime in results:\n        print(f&quot;{regime_name}: IC={results[regime][&#039;ic&#039;]:.4f}, 样本数={results[regime][&#039;n_samples&#039;]}&quot;)\n6.2 按行业分析IC\ndef analyze_ic_by_industry(y_pred, y_true, industry_codes):\n    &quot;&quot;&quot;\n    按行业分析IC\n \n    参数:\n        y_pred: 预测值\n        y_true: 实际值\n        industry_codes: 行业代码\n \n    返回:\n        dict: {industry: {&#039;ic&#039;: ic, &#039;n_samples&#039;: n_samples}}\n    &quot;&quot;&quot;\n    results = {}\n \n    for code in np.unique(industry_codes):\n        mask = industry_codes == code\n        y_pred_industry = y_pred[mask]\n        y_true_industry = y_true[mask]\n \n        if len(y_pred_industry) &gt; 0:\n            ic = pearsonr(y_pred_industry, y_true_industry)[0]\n            results[code] = {\n                &#039;ic&#039;: ic,\n                &#039;n_samples&#039;: len(y_pred_industry)\n            }\n \n    return results\n \n# 示例\nindustry_codes = np.random.choice([1, 2, 3, 4, 5], size=len(y_pred))\n \nresults = analyze_ic_by_industry(y_pred, y_true, industry_codes)\n \nfor code, result in results.items():\n    print(f&quot;行业{code}: IC={result[&#039;ic&#039;]:.4f}, 样本数={result[&#039;n_samples&#039;]}&quot;)\n7. IC 胜率\n7.1 IC 胜率的定义\n定义：\nIC 胜率 = IC &gt; 0 的天数比例\n标准：\n\n胜率 &gt; 50%: 预测整体有效\n胜率 &gt; 55%: 较为理想\n胜率 &gt; 60%: 非常好\n\n注意：\n即使平均 IC 很高，如果胜率低，说明模型可能只在少数天有效，风险较大。\n实现代码：\ndef calculate_ic_win_rate(ic_series):\n    &quot;&quot;&quot;\n    计算 IC 胜率\n \n    参数:\n        ic_series: IC 序列\n \n    返回:\n        win_rate: 胜率 (0~1)\n    &quot;&quot;&quot;\n    return (ic_series &gt; 0).mean()\n \n# 使用示例\nwin_rate = calculate_ic_win_rate(ic_series)\nprint(f&quot;IC 胜率: {win_rate * 100:.1f}%&quot;)\n7.2 IC 胜率与均值的关系\n场景分析：\n# 场景1: 高均值，低胜率（不稳定）\nic_scenario1 = np.array([0.15, 0.12, -0.05, 0.18, -0.08, 0.20])\nwin_rate1 = (ic_scenario1 &gt; 0).mean()\nmean_ic1 = ic_scenario1.mean()\nprint(f&quot;场景1 - IC均值={mean_ic1:.4f}, 胜率={win_rate1*100:.1f}% (高均值低胜率)&quot;)\n \n# 场景2: 中等均值，高胜率（稳定）\nic_scenario2 = np.array([0.04, 0.05, 0.03, 0.06, 0.04, 0.05])\nwin_rate2 = (ic_scenario2 &gt; 0).mean()\nmean_ic2 = ic_scenario2.mean()\nprint(f&quot;场景2 - IC均值={mean_ic2:.4f}, 胜率={win_rate2*100:.1f}% (中等均值高胜率)&quot;)\n \n# 场景3: 低均值，高胜率（稳定但信号弱）\nic_scenario3 = np.array([0.02, 0.01, 0.02, 0.01, 0.02, 0.01])\nwin_rate3 = (ic_scenario3 &gt; 0).mean()\nmean_ic3 = ic_scenario3.mean()\nprint(f&quot;场景3 - IC均值={mean_ic3:.4f}, 胜率={win_rate3*100:.1f}% (低均值高胜率)&quot;)\n建议：\n\n优先选择高胜率模型（稳定性好）\n在高胜率基础上，追求更高IC均值\n避免低胜率但高均值的模型（风险大）\n\n8. 完整评估函数\n8.1 评估报告生成\ndef calculate_daily_ic(pred, true):\n    &quot;&quot;&quot;\n    计算每日 IC\n \n    参数:\n        pred: 预测值 (Series 或 DataFrame)\n        true: 真实值 (Series 或 DataFrame)\n \n    返回:\n        ic: IC 值\n        pvalue: 显著性 p 值\n    &quot;&quot;&quot;\n    ic, pvalue = pearsonr(pred, true)\n    return ic, pvalue\n \ndef calculate_ic_series(pred_df, true_df):\n    &quot;&quot;&quot;\n    计算时间序列 IC\n \n    参数:\n        pred_df: 预测值 DataFrame (index: [date, instrument])\n        true_df: 真实值 DataFrame (同上)\n \n    返回:\n        ic_series: IC 序列\n    &quot;&quot;&quot;\n    # 按日期分组\n    dates = pred_df.index.get_level_values(0).unique()\n \n    ic_values = []\n    for date in dates:\n        pred = pred_df.loc[date]\n        true = true_df.loc[date]\n \n        ic, _ = calculate_daily_ic(pred.values, true.values)\n        ic_values.append(ic)\n \n    return pd.Series(ic_values, index=dates)\n \ndef evaluate_model(pred_df, true_df):\n    &quot;&quot;&quot;\n    完整的模型评估函数\n \n    参数:\n        pred_df: 预测值 DataFrame (index: [date, instrument])\n        true_df: 真实值 DataFrame (同上)\n \n    返回:\n        evaluation_report: 评估报告\n    &quot;&quot;&quot;\n    # 计算 IC 系列和 Rank IC 系列\n    ic_series = calculate_ic_series(pred_df, true_df)\n    rank_ic_series = calculate_rank_ic_series(pred_df, true_df)\n \n    # 计算指标\n    metrics = {\n        &#039;IC_mean&#039;: ic_series.mean(),\n        &#039;IC_std&#039;: ic_series.std(),\n        &#039;ICIR&#039;: ic_series.mean() / ic_series.std() if ic_series.std() &gt; 0 else 0,\n        &#039;IC_positive_ratio&#039;: (ic_series &gt; 0).mean(),\n        &#039;Rank_IC_mean&#039;: rank_ic_series.mean(),\n        &#039;Rank_IC_std&#039;: rank_ic_series.std(),\n        &#039;n_days&#039;: len(ic_series),\n    }\n \n    # 打印报告\n    print(&quot;=&quot; * 60)\n    print(&quot;📊 模型评估报告&quot;)\n    print(&quot;=&quot; * 60)\n \n    print(&quot;\\nIC 指标:&quot;)\n    print(f&quot;  IC 均值: {metrics[&#039;IC_mean&#039;]:.4f}&quot;)\n    print(f&quot;  IC 标准差: {metrics[&#039;IC_std&#039;]:.4f}&quot;)\n    print(f&quot;  ICIR: {metrics[&#039;ICIR&#039;]:.4f}&quot;)\n    print(f&quot;  IC 胜率: {metrics[&#039;IC_positive_ratio&#039;] * 100:.2f}%&quot;)\n \n    print(&quot;\\nRank IC 指标:&quot;)\n    print(f&quot;  Rank IC 均值: {metrics[&#039;Rank_IC_mean&#039;]:.4f}&quot;)\n    print(f&quot;  Rank IC 标准差: {metrics[&#039;Rank_IC_std&#039;]:.4f}&quot;)\n \n    print(&quot;\\n模型质量评估:&quot;)\n    if metrics[&#039;IC_mean&#039;] &gt; 0.05:\n        print(&quot;  ✅ IC 均值优秀&quot;)\n    elif metrics[&#039;IC_mean&#039;] &gt; 0.03:\n        print(&quot;  ✅ IC 均值有效&quot;)\n    else:\n        print(&quot;  ⚠️ IC 均值较弱&quot;)\n \n    if metrics[&#039;ICIR&#039;] &gt; 0.5:\n        print(&quot;  🌟 ICIR 非常稳定&quot;)\n    elif metrics[&#039;ICIR&#039;] &gt; 0.3:\n        print(&quot;  ✅ ICIR 较稳定&quot;)\n    else:\n        print(&quot;  ⚠️ ICIR 稳定性一般&quot;)\n \n    if metrics[&#039;IC_positive_ratio&#039;] &gt; 0.55:\n        print(&quot;  ✅ IC 胜率良好&quot;)\n    else:\n        print(&quot;  ⚠️ IC 胜率一般&quot;)\n \n    print(&quot;=&quot; * 60)\n \n    return metrics\n \ndef calculate_rank_ic_series(pred_df, true_df):\n    &quot;&quot;&quot;\n    计算时间序列 Rank IC\n \n    参数:\n        pred_df: 预测值 DataFrame\n        true_df: 真实值 DataFrame\n \n    返回:\n        rank_ic_series: Rank IC 序列\n    &quot;&quot;&quot;\n    dates = pred_df.index.get_level_values(0).unique()\n    \n    rank_ic_values = []\n    for date in dates:\n        pred = pred_df.loc[date]\n        true = true_df.loc[date]\n        \n        rank_ic, _ = spearmanr(pred.values, true.values)\n        rank_ic_values.append(rank_ic)\n    \n    return pd.Series(rank_ic_values, index=dates)\n \n# 使用示例\nimport pandas as pd\n \n# 创建示例数据\nn_days = 100\nn_stocks = 300\n \ndates = pd.date_range(&#039;2020-01-01&#039;, periods=n_days, freq=&#039;D&#039;)\nstocks = [f&#039;stock_{i}&#039; for i in range(n_stocks)]\nindex = pd.MultiIndex.from_product([dates, stocks], names=[&#039;date&#039;, &#039;instrument&#039;])\n \npred_df = pd.DataFrame(np.random.randn(len(index)), index=index, columns=[&#039;pred&#039;])\ntrue_df = pd.DataFrame(np.random.randn(len(index)), index=index, columns=[&#039;true&#039;])\n \n# 评估模型\nmetrics = evaluate_model(pred_df, true_df)\n8.2 模型质量判断标准\nIC 均值标准：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIC 值模型质量&gt; 0.10🌟 顶级 (非常罕见)&gt; 0.05✅ 优秀&gt; 0.03✅ 有效0.02~0.03⚠️ 一般&lt; 0.02❌ 较弱\nICIR 标准：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nICIR 值稳定性&gt; 0.5🌟 非常稳定&gt; 0.3✅ 较稳定&gt; 0.2⚠️ 一般&lt; 0.2❌ 不稳定\nIC 胜率标准：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n胜率评价&gt; 60%🌟 优秀&gt; 55%✅ 良好&gt; 50%⚠️ 一般&lt; 50%❌ 较差\n8.3 可视化评估结果\nimport matplotlib.pyplot as plt\n \ndef plot_ic_evaluation(ic_series, rank_ic_series):\n    &quot;&quot;&quot;\n    绘制IC评估结果\n    \n    参数:\n        ic_series: IC 序列\n        rank_ic_series: Rank IC 序列\n    &quot;&quot;&quot;\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    # IC 时间序列\n    axes[0, 0].plot(ic_series, linewidth=1, alpha=0.7)\n    axes[0, 0].axhline(y=0, color=&#039;r&#039;, linestyle=&#039;--&#039;, alpha=0.5)\n    axes[0, 0].axhline(y=ic_series.mean(), color=&#039;g&#039;, linestyle=&#039;--&#039;, \n                       label=f&#039;Mean: {ic_series.mean():.4f}&#039;)\n    axes[0, 0].set_xlabel(&#039;Date&#039;)\n    axes[0, 0].set_ylabel(&#039;IC&#039;)\n    axes[0, 0].set_title(&#039;IC Time Series&#039;)\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # IC 分布\n    axes[0, 1].hist(ic_series, bins=30, alpha=0.7, edgecolor=&#039;black&#039;)\n    axes[0, 1].axvline(x=ic_series.mean(), color=&#039;r&#039;, linestyle=&#039;--&#039;,\n                       label=f&#039;Mean: {ic_series.mean():.4f}&#039;)\n    axes[0, 1].axvline(x=0, color=&#039;gray&#039;, linestyle=&#039;-&#039;, alpha=0.5)\n    axes[0, 1].set_xlabel(&#039;IC&#039;)\n    axes[0, 1].set_ylabel(&#039;Frequency&#039;)\n    axes[0, 1].set_title(&#039;IC Distribution&#039;)\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Rank IC 时间序列\n    axes[1, 0].plot(rank_ic_series, linewidth=1, alpha=0.7, color=&#039;orange&#039;)\n    axes[1, 0].axhline(y=0, color=&#039;r&#039;, linestyle=&#039;--&#039;, alpha=0.5)\n    axes[1, 0].axhline(y=rank_ic_series.mean(), color=&#039;g&#039;, linestyle=&#039;--&#039;,\n                       label=f&#039;Mean: {rank_ic_series.mean():.4f}&#039;)\n    axes[1, 0].set_xlabel(&#039;Date&#039;)\n    axes[1, 0].set_ylabel(&#039;Rank IC&#039;)\n    axes[1, 0].set_title(&#039;Rank IC Time Series&#039;)\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    # IC vs Rank IC 散点图\n    axes[1, 1].scatter(ic_series, rank_ic_series, alpha=0.6, s=20)\n    axes[1, 1].axhline(y=0, color=&#039;r&#039;, linestyle=&#039;--&#039;, alpha=0.3)\n    axes[1, 1].axvline(x=0, color=&#039;r&#039;, linestyle=&#039;--&#039;, alpha=0.3)\n    axes[1, 1].plot([min(ic_series), max(ic_series)], \n                     [min(ic_series), max(ic_series)], \n                     &#039;r--&#039;, alpha=0.5, label=&#039;y=x&#039;)\n    axes[1, 1].set_xlabel(&#039;IC&#039;)\n    axes[1, 1].set_ylabel(&#039;Rank IC&#039;)\n    axes[1, 1].set_title(&#039;IC vs Rank IC&#039;)\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n \n# 使用示例\nplot_ic_evaluation(ic_series, rank_ic_series)\n9. IC在模型评估中的应用\n9.1 交叉验证中的IC评估\nfrom sklearn.model_selection import TimeSeriesSplit\n \ndef cross_validate_ic(X, y, params, model, n_splits=5):\n    &quot;&quot;&quot;\n    时间序列交叉验证，评估IC\n \n    参数:\n        X: 特征矩阵\n        y: 目标变量\n        params: 模型参数\n        model: 模型对象\n        n_splits: 折数\n \n    返回:\n        ic_scores: 每折的IC得分\n        models: 训练的模型列表\n    &quot;&quot;&quot;\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    ic_scores = []\n    models = []\n \n    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n        print(f&quot;Fold {fold + 1}/{n_splits}&quot;)\n \n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n \n        # 训练模型\n        model.fit(X_train, y_train)\n \n        # 预测\n        y_pred = model.predict(X_val)\n \n        # 计算IC\n        ic = pearsonr(y_pred, y_val)[0]\n        ic_scores.append(ic)\n        models.append(model)\n \n        print(f&quot;  Val IC: {ic:.4f}&quot;)\n \n    return ic_scores, models\n \n# 示例\nfrom lightgbm import LGBMRegressor\n \nparams = {\n    &#039;objective&#039;: &#039;regression&#039;,\n    &#039;num_leaves&#039;: 31,\n    &#039;learning_rate&#039;: 0.05,\n}\n \nmodel = LGBMRegressor(**params)\nic_scores, models = cross_validate_ic(X, y, params, model, n_splits=5)\n \nprint(f&quot;\\n平均IC: {np.mean(ic_scores):.4f}&quot;)\nprint(f&quot;IC标准差: {np.std(ic_scores):.4f}&quot;)\n7.2 IC与模型选择\ndef select_model_by_ic(X_train, y_train, X_val, y_val, param_grid, model_class):\n    &quot;&quot;&quot;\n    基于IC选择最佳模型\n \n    参数:\n        X_train, y_train: 训练数据\n        X_val, y_val: 验证数据\n        param_grid: 参数网格\n        model_class: 模型类\n \n    返回:\n        best_model: 最佳模型\n        best_params: 最佳参数\n        best_ic: 最佳IC\n    &quot;&quot;&quot;\n    best_model = None\n    best_params = None\n    best_ic = -np.inf\n \n    from itertools import product\n \n    keys = param_grid.keys()\n    values = param_grid.values()\n \n    for combination in product(*values):\n        params = dict(zip(keys, combination))\n \n        # 训练模型\n        model = model_class(**params)\n        model.fit(X_train, y_train)\n \n        # 预测\n        y_pred = model.predict(X_val)\n \n        # 计算IC\n        ic = pearsonr(y_pred, y_val)[0]\n \n        if ic &gt; best_ic:\n            best_ic = ic\n            best_model = model\n            best_params = params\n \n        print(f&quot;Params: {params}, IC: {ic:.4f}&quot;)\n \n    return best_model, best_params, best_ic\n \n# 示例\nparam_grid = {\n    &#039;num_leaves&#039;: [31, 63],\n    &#039;learning_rate&#039;: [0.01, 0.05, 0.1],\n    &#039;min_data_in_leaf&#039;: [10, 20],\n}\n \nbest_model, best_params, best_ic = select_model_by_ic(\n    X_train, y_train, X_val, y_val, param_grid, LGBMRegressor\n)\n \nprint(f&quot;\\n最佳参数: {best_params}&quot;)\nprint(f&quot;最佳IC: {best_ic:.4f}&quot;)\n8. 总结\nIC和Rank IC是量化投资中最重要的评估指标：\n\nIC定义：预测值与实际值的Pearson相关系数\nRank IC：预测值排序与实际值排序的Spearman秩相关系数\n统计显著性：通过t检验和置信区间验证IC的显著性\n时序分析：滚动IC和IC衰减分析预测的稳定性和时效性\nIR指标：衡量IC的稳定性，IC均值除以标准差\n多维度分析：按市场状态、行业等子集分析IC表现\n\nIC是量化模型评估的核心，正确的IC分析是构建有效量化策略的基础。"},"quant/qlib/week2/05-特征重要性分析":{"slug":"quant/qlib/week2/05-特征重要性分析","filePath":"quant/qlib/week2/05-特征重要性分析.md","title":"05-特征重要性分析","links":[],"tags":[],"content":"特征重要性分析\n1. 特征重要性基础\n1.1 特征重要性的意义\n为什么重要？\n在量化投资中，特征重要性分析有三大价值：\n\n模型解释性：理解模型如何做决策\n特征筛选：识别有效因子，剔除冗余因子\n风险控制：避免模型依赖无效特征\n\n量化场景的特殊性\n量化数据的特点：\n\n高维特征：数百到数千个因子\n低信噪比：大量噪声特征\n强相关性：因子间存在多重共线性\n非平稳性：因子重要性随时间变化\n\n1.2 LightGBM的特征重要性类型\nLightGBM提供两种特征重要性：\n\n\nSplit重要性（分裂重要性）\n\n基于特征作为分裂节点的次数\n衡量特征在分裂决策中的使用频率\n\n\n\nGain重要性（增益重要性）\n\n基于分裂带来的信息增益\n衡量特征对模型性能的贡献度\n\n\n\n代码示例\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n \n# 训练模型\nmodel = lgb.train(params, train_data, num_boost_round=1000,\n                  valid_sets=[train_data, val_data],\n                  callbacks=[lgb.early_stopping(stopping_rounds=50)])\n \n# 获取特征重要性\nsplit_importance = model.feature_importance(importance_type=&#039;split&#039;)\ngain_importance = model.feature_importance(importance_type=&#039;gain&#039;)\n \n# 打印\nprint(&quot;Split重要性（前10）：&quot;)\nfor i, (idx, imp) in enumerate(sorted(enumerate(split_importance),\n                                      key=lambda x: x[1], reverse=True)[:10]):\n    print(f&quot;  {idx+1}. 特征{idx}: {imp}&quot;)\n \nprint(&quot;\\nGain重要性（前10）：&quot;)\nfor i, (idx, imp) in enumerate(sorted(enumerate(gain_importance),\n                                      key=lambda x: x[1], reverse=True)[:10]):\n    print(f&quot;  {idx+1}. 特征{idx}: {imp:.2f}&quot;)\n2. 特征重要性可视化\n2.1 条形图可视化\ndef plot_feature_importance(model, feature_names, importance_type=&#039;gain&#039;, top_n=20):\n    &quot;&quot;&quot;\n    绘制特征重要性条形图\n \n    参数:\n        model: LightGBM模型\n        feature_names: 特征名称列表\n        importance_type: &#039;split&#039; 或 &#039;gain&#039;\n        top_n: 显示前n个特征\n    &quot;&quot;&quot;\n    # 获取重要性\n    importance = model.feature_importance(importance_type=importance_type)\n \n    # 排序\n    indices = np.argsort(importance)[::-1][:top_n]\n    importance = importance[indices]\n    feature_names = np.array(feature_names)[indices]\n \n    # 绘制\n    plt.figure(figsize=(12, 8))\n    plt.barh(range(len(importance)), importance[::-1])\n    plt.yticks(range(len(importance)), feature_names[::-1])\n    plt.xlabel(f&#039;Feature Importance ({importance_type})&#039;)\n    plt.title(f&#039;Top {top_n} Feature Importance&#039;)\n    plt.tight_layout()\n    plt.show()\n \n# 使用示例\nfeature_names = [f&#039;factor_{i}&#039; for i in range(X.shape[1])]\nplot_feature_importance(model, feature_names, importance_type=&#039;gain&#039;, top_n=20)\n2.2 对数重要性图\ndef plot_feature_importance_log(model, feature_names, importance_type=&#039;gain&#039;):\n    &quot;&quot;&quot;\n    绘制对数特征重要性图\n \n    适用于特征重要性差异巨大的情况\n    &quot;&quot;&quot;\n    importance = model.feature_importance(importance_type=importance_type)\n    importance = np.maximum(importance, 1e-6)  # 避免log(0)\n \n    indices = np.argsort(importance)[::-1]\n    importance_log = np.log(importance[indices])\n \n    plt.figure(figsize=(12, 8))\n    plt.plot(range(len(importance_log)), importance_log, marker=&#039;o&#039;)\n    plt.xlabel(&#039;Feature Rank&#039;)\n    plt.ylabel(&#039;Log Feature Importance&#039;)\n    plt.title(f&#039;Feature Importance Distribution (Log Scale) - {importance_type}&#039;)\n    plt.grid(True)\n    plt.show()\n \nplot_feature_importance_log(model, feature_names, importance_type=&#039;gain&#039;)\n3. 基于Permutation的特征重要性\n3.1 Permutation Importance原理\n核心思想\nPermutation Importance通过打乱特征的值来评估其重要性：\n\n计算基准模型性能\n打乱某个特征的值\n重新计算模型性能\n性能下降越大，特征越重要\n\n优势\n\n不依赖于模型类型\n考虑特征间的交互作用\n更真实反映特征重要性\n\n代码实现\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import pearsonr\n \ndef permutation_importance(model, X, y, metric=&#039;ic&#039;, n_repeats=5, random_state=42):\n    &quot;&quot;&quot;\n    计算Permutation Importance\n \n    参数:\n        model: 训练好的模型\n        X: 特征矩阵\n        y: 目标变量\n        metric: 评估指标 (&#039;rmse&#039;, &#039;r2&#039;, &#039;ic&#039;, &#039;rank_ic&#039;)\n        n_repeats: 重复次数\n        random_state: 随机种子\n \n    返回:\n        importances: 特征重要性数组，shape=[n_features, n_repeats]\n    &quot;&quot;&quot;\n    np.random.seed(random_state)\n    n_features = X.shape[1]\n    importances = np.zeros((n_features, n_repeats))\n \n    # 计算基准性能\n    y_pred = model.predict(X)\n    if metric == &#039;rmse&#039;:\n        baseline_score = np.sqrt(mean_squared_error(y, y_pred))\n    elif metric == &#039;r2&#039;:\n        from sklearn.metrics import r2_score\n        baseline_score = r2_score(y, y_pred)\n    elif metric == &#039;ic&#039;:\n        baseline_score = pearsonr(y_pred, y)[0]\n    elif metric == &#039;rank_ic&#039;:\n        from scipy.stats import spearmanr\n        baseline_score = spearmanr(y_pred, y)[0]\n    else:\n        raise ValueError(f&quot;Unknown metric: {metric}&quot;)\n \n    print(f&quot;Baseline {metric}: {baseline_score:.4f}&quot;)\n \n    # 对每个特征进行Permutation\n    for feature_idx in range(n_features):\n        print(f&quot;Processing feature {feature_idx + 1}/{n_features}&quot;)\n \n        for repeat in range(n_repeats):\n            # 打乱特征\n            X_permuted = X.copy()\n            np.random.shuffle(X_permuted[:, feature_idx])\n \n            # 重新预测\n            y_pred_permuted = model.predict(X_permuted)\n \n            # 计算性能\n            if metric == &#039;rmse&#039;:\n                score = np.sqrt(mean_squared_error(y, y_pred_permuted))\n                importance = score - baseline_score  # RMSE增加，重要\n            elif metric == &#039;r2&#039;:\n                score = r2_score(y, y_pred_permuted)\n                importance = baseline_score - score  # R2减少，重要\n            elif metric == &#039;ic&#039;:\n                score = pearsonr(y_pred_permuted, y)[0]\n                importance = baseline_score - score  # IC减少，重要\n            elif metric == &#039;rank_ic&#039;:\n                score = spearmanr(y_pred_permuted, y)[0]\n                importance = baseline_score - score  # Rank IC减少，重要\n \n            importances[feature_idx, repeat] = importance\n \n    return importances\n \n# 使用示例\nimportances = permutation_importance(model, X_val, y_val, metric=&#039;ic&#039;, n_repeats=5)\n \n# 计算平均重要性\nmean_importance = importances.mean(axis=1)\nstd_importance = importances.std(axis=1)\n \n# 排序\nsorted_indices = np.argsort(mean_importance)[::-1]\n \nprint(&quot;\\nPermutation Importance (IC):&quot;)\nfor i, idx in enumerate(sorted_indices[:10]):\n    print(f&quot;  {i+1}. 特征{idx}: {mean_importance[idx]:.4f} ± {std_importance[idx]:.4f}&quot;)\n3.2 可视化Permutation Importance\ndef plot_permutation_importance(importances, feature_names, top_n=20):\n    &quot;&quot;&quot;\n    绘制Permutation Importance\n \n    参数:\n        importances: 特征重要性数组，shape=[n_features, n_repeats]\n        feature_names: 特征名称\n        top_n: 显示前n个特征\n    &quot;&quot;&quot;\n    # 计算平均重要性\n    mean_imp = importances.mean(axis=1)\n    std_imp = importances.std(axis=1)\n \n    # 排序\n    indices = np.argsort(mean_imp)[::-1][:top_n]\n    mean_imp = mean_imp[indices]\n    std_imp = std_imp[indices]\n    feature_names = np.array(feature_names)[indices]\n \n    # 绘制\n    plt.figure(figsize=(12, 8))\n    plt.barh(range(len(mean_imp)), mean_imp[::-1], xerr=std_imp[::-1],\n             color=&#039;steelblue&#039;, alpha=0.7, capsize=3)\n    plt.yticks(range(len(mean_imp)), feature_names[::-1])\n    plt.xlabel(&#039;Permutation Importance&#039;)\n    plt.title(f&#039;Top {top_n} Permutation Feature Importance&#039;)\n    plt.tight_layout()\n    plt.show()\n \nplot_permutation_importance(importances, feature_names, top_n=20)\n4. SHAP值分析\n4.1 SHAP原理\nSHAP（SHapley Additive exPlanations）\nSHAP值基于博弈论中的Shapley值，提供一致的局部解释。\n核心思想\n每个特征对预测的贡献：\n\\hat{y}_i = \\text{Base Value} + \\sum_{j=1}^M \\text{SHAP}_{i,j}\n其中：\n\n\\hat{y}_i 是第i个样本的预测值\nBase Value是所有样本的均值预测\n\\text{SHAP}_{i,j} 是第j个特征对第i个样本的贡献\n\n代码实现\nimport shap\n \n# 计算SHAP值\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\n \n# SHAP Summary Plot\nshap.summary_plot(shap_values, X, feature_names=feature_names, plot_type=&#039;bar&#039;)\n \n# SHAP Summary Plot (详细)\nshap.summary_plot(shap_values, X, feature_names=feature_names)\n4.2 特征重要性排序\ndef shap_feature_importance(shap_values, feature_names, top_n=20):\n    &quot;&quot;&quot;\n    基于SHAP值的特征重要性\n \n    参数:\n        shap_values: SHAP值数组\n        feature_names: 特征名称\n        top_n: 显示前n个特征\n \n    返回:\n        重要性排序结果\n    &quot;&quot;&quot;\n    # 计算每个特征的平均绝对SHAP值\n    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n \n    # 排序\n    indices = np.argsort(mean_abs_shap)[::-1][:top_n]\n    importance = mean_abs_shap[indices]\n    names = np.array(feature_names)[indices]\n \n    # 打印\n    print(&quot;SHAP Feature Importance:&quot;)\n    for i, (name, imp) in enumerate(zip(names, importance)):\n        print(f&quot;  {i+1}. {name}: {imp:.4f}&quot;)\n \n    return indices, importance, names\n \n# 使用示例\nindices, importance, names = shap_feature_importance(shap_values, feature_names, top_n=20)\n4.3 个体解释\ndef plot_shap_force(explainer, X, sample_idx, feature_names):\n    &quot;&quot;&quot;\n    绘制单个样本的SHAP Force Plot\n \n    参数:\n        explainer: SHAP解释器\n        X: 特征矩阵\n        sample_idx: 样本索引\n        feature_names: 特征名称\n    &quot;&quot;&quot;\n    # 计算SHAP值\n    shap_values = explainer.shap_values(X[[sample_idx]])\n \n    # 绘制Force Plot\n    shap.force_plot(explainer.expected_value[0],\n                   shap_values[0],\n                   X[sample_idx],\n                   feature_names=feature_names)\n \n# 使用示例\nplot_shap_force(explainer, X_val, sample_idx=0, feature_names=feature_names)\n5. 特征相关性分析\n5.1 为什么特征相关性重要？\n高相关的特征：\n\n提供冗余信息\n可能导致重要性”分散”\n增加模型复杂度但不增加价值\n\n建议：\n\n同类特征保留最重要的 1-2 个\n相关性 &gt; 0.9 的特征考虑合并或剔除\n\n5.2 相关性计算\ndef analyze_feature_correlation(X, features, threshold=0.7):\n    &quot;&quot;&quot;\n    分析特征相关性\n \n    参数:\n        X: 特征 DataFrame\n        features: 特征列表\n        threshold: 高相关阈值\n \n    返回:\n        high_corr_pairs: 高相关特征对\n    &quot;&quot;&quot;\n    # 计算相关矩阵\n    corr_matrix = X[features].corr()\n \n    # 找高相关对\n    high_corr_pairs = []\n    for i, feat1 in enumerate(features):\n        for feat2 in features[i+1:]:\n            corr = corr_matrix.loc[feat1, feat2]\n            if abs(corr) &gt; threshold:\n                high_corr_pairs.append((feat1, feat2, corr))\n \n    # 排序\n    high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n \n    # 打印\n    print(f&quot;\\n📊 高相关特征对 (|corr| &gt; {threshold}):&quot;)\n    print(&quot;-&quot; * 50)\n    for feat1, feat2, corr in high_corr_pairs:\n        print(f&quot;  {feat1:12s} ↔ {feat2:12s} : {corr:.3f}&quot;)\n \n    return high_corr_pairs\n \n# 使用示例\nhigh_corr_pairs = analyze_feature_correlation(X_train, list(FEATURES.keys()), threshold=0.7)\n示例输出：\n高相关特征对 (|corr| &gt; 0.7):\n--------------------------------------------------\nRET_5D       ↔ BIAS_10     :  0.931\nRET_10D      ↔ BIAS_20     :  0.924\nBIAS_10      ↔ BIAS_20     :  0.873\nBIAS_5       ↔ BIAS_10     :  0.860\nRET_10D      ↔ BIAS_10     :  0.834\nRET_20D      ↔ BIAS_20     :  0.829\nRET_5D       ↔ BIAS_5      :  0.818\nRET_1D       ↔ BODY        :  0.814\nRET_5D       ↔ BIAS_20     :  0.769\nVOL_10       ↔ VOL_20      :  0.745\n\n5.3 相关性热力图可视化\ndef plot_correlation_heatmap(X, features, top_n=30):\n    &quot;&quot;&quot;\n    绘制特征相关性热力图\n \n    参数:\n        X: 特征 DataFrame\n        features: 特征列表\n        top_n: 显示前n个特征\n    &quot;&quot;&quot;\n    # 计算相关性矩阵\n    corr_matrix = X[features[:top_n]].corr()\n \n    # 绘制热力图\n    import seaborn as sns\n    plt.figure(figsize=(12, 10))\n \n    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    sns.heatmap(corr_matrix,\n                mask=mask,\n                cmap=&#039;coolwarm&#039;,\n                center=0,\n                square=True,\n                linewidths=1,\n                cbar_kws={&quot;shrink&quot;: 0.8},\n                annot=False,\n                fmt=&#039;.2f&#039;,\n                xticklabels=1,\n                yticklabels=1)\n \n    plt.title(&#039;Feature Correlation Heatmap&#039;, fontsize=14, fontweight=&#039;bold&#039;)\n    plt.xlabel(&#039;Features&#039;, fontsize=12)\n    plt.ylabel(&#039;Features&#039;, fontsize=12)\n    plt.xticks(rotation=45, ha=&#039;right&#039;)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.show()\n \n# 使用示例\nplot_correlation_heatmap(X_train, list(FEATURES.keys()), top_n=30)\n5.4 基于相关性的特征筛选\ndef filter_features_by_correlation(X, features, threshold=0.9, importance=None):\n    &quot;&quot;&quot;\n    基于相关性筛选特征\n \n    策略：\n    1. 找到高相关特征对\n    2. 如果两个特征相关性 &gt; threshold\n    3. 保留重要性更高的特征\n \n    参数:\n        X: 特征 DataFrame\n        features: 特征列表\n        threshold: 相关性阈值\n        importance: 特征重要性字典 {feature: importance}\n \n    返回:\n        selected_features: 筛选后的特征列表\n    &quot;&quot;&quot;\n    # 计算相关矩阵\n    corr_matrix = X[features].corr()\n \n    # 如果没有提供重要性，使用默认值\n    if importance is None:\n        importance = {feat: 1.0 for feat in features}\n \n    # 标记要删除的特征\n    to_remove = set()\n \n    for i, feat1 in enumerate(features):\n        for feat2 in features[i+1:]:\n            corr = abs(corr_matrix.loc[feat1, feat2])\n \n            if corr &gt; threshold:\n                # 保留重要性更高的特征\n                if importance[feat1] &gt;= importance[feat2]:\n                    to_remove.add(feat2)\n                    print(f&quot;移除 {feat2} (与 {feat1} 相关性={corr:.3f})&quot;)\n                else:\n                    to_remove.add(feat1)\n                    print(f&quot;移除 {feat1} (与 {feat2} 相关性={corr:.3f})&quot;)\n \n    # 筛选特征\n    selected_features = [f for f in features if f not in to_remove]\n \n    print(f&quot;\\n原始特征数: {len(features)}&quot;)\n    print(f&quot;筛选后特征数: {len(selected_features)}&quot;)\n    print(f&quot;删除特征数: {len(to_remove)}&quot;)\n \n    return selected_features\n \n# 使用示例\nimportance_dict = dict(zip(feature_names, gain_importance))\nselected_features = filter_features_by_correlation(\n    X_train, \n    list(FEATURES.keys()),\n    threshold=0.9,\n    importance=importance_dict\n)\n6. 重训练验证\n6.1 验证特征选择的效果\ndef validate_feature_selection(X_train, y_train, X_valid, y_valid, X_test, y_test,\n                         full_features, selected_features, params):\n    &quot;&quot;&quot;\n    验证特征选择的效果\n \n    参数:\n        X_train, y_train: 训练数据\n        X_valid, y_valid: 验证数据\n        X_test, y_test: 测试数据\n        full_features: 所有特征\n        selected_features: 选中的特征\n        params: 模型参数\n \n    返回:\n        comparison: 对比结果\n    &quot;&quot;&quot;\n    import lightgbm as lgb\n    from scipy.stats import pearsonr\n \n    # 训练全特征模型\n    print(&quot;\\n训练全特征模型...&quot;)\n    train_data_full = lgb.Dataset(X_train[full_features], label=y_train)\n    val_data_full = lgb.Dataset(X_valid[full_features], label=y_valid, reference=train_data_full)\n \n    model_full = lgb.train(\n        params,\n        train_data_full,\n        num_boost_round=1000,\n        valid_sets=[train_data_full, val_data_full],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=50, verbose=False),\n            lgb.log_evaluation(period=100)\n        ]\n    )\n \n    # 训练精选特征模型\n    print(&quot;\\n训练精选特征模型...&quot;)\n    train_data_selected = lgb.Dataset(X_train[selected_features], label=y_train)\n    val_data_selected = lgb.Dataset(X_valid[selected_features], label=y_valid, reference=train_data_selected)\n \n    model_selected = lgb.train(\n        params,\n        train_data_selected,\n        num_boost_round=1000,\n        valid_sets=[train_data_selected, val_data_selected],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=50, verbose=False),\n            lgb.log_evaluation(period=100)\n        ]\n    )\n \n    # 预测\n    pred_full = model_full.predict(X_test[full_features])\n    pred_selected = model_selected.predict(X_test[selected_features])\n \n    # 计算IC\n    ic_full = pearsonr(pred_full, y_test)[0]\n    ic_selected = pearsonr(pred_selected, y_test)[0]\n \n    # 打印对比\n    print(&quot;\\n🔬 特征选择验证:&quot;)\n    print(&quot;-&quot; * 50)\n    print(f&quot;{&#039;模型&#039;:&lt;15s} {&#039;特征数&#039;:&lt;10s} {&#039;测试集 IC&#039;:&lt;10s}&quot;)\n    print(&quot;-&quot; * 50)\n    print(f&quot;{&#039;全部特征&#039;:&lt;15s} {len(full_features):&lt;10d} {ic_full:&lt;10.4f}&quot;)\n    print(f&quot;{&#039;精选特征&#039;:&lt;15s} {len(selected_features):&lt;10d} {ic_selected:&lt;10.4f}&quot;)\n    print(&quot;-&quot; * 50)\n \n    # 评估\n    ic_ratio = ic_selected / ic_full if ic_full != 0 else 0\n    print(f&quot;\\nIC保持率: {ic_ratio:.2%}&quot;)\n \n    if ic_selected &gt;= ic_full * 0.95:\n        print(&quot;✅ 精选特征表现接近全特征，可以简化模型!&quot;)\n    elif ic_selected &gt;= ic_full * 0.90:\n        print(&quot;⚠️ 精选特征性能略有下降，但简化模型可能值得&quot;)\n    else:\n        print(&quot;❌ 精选特征性能下降较多，需要调整&quot;)\n \n    return {\n        &#039;full_features_ic&#039;: ic_full,\n        &#039;selected_features_ic&#039;: ic_selected,\n        &#039;n_full&#039;: len(full_features),\n        &#039;n_selected&#039;: len(selected_features),\n        &#039;ic_ratio&#039;: ic_ratio,\n        &#039;model_full&#039;: model_full,\n        &#039;model_selected&#039;: model_selected\n    }\n \n# 使用示例\nparams = {\n    &#039;objective&#039;: &#039;regression&#039;,\n    &#039;metric&#039;: &#039;rmse&#039;,\n    &#039;num_leaves&#039;: 31,\n    &#039;learning_rate&#039;: 0.05,\n    &#039;verbose&#039;: -1,\n}\n \ncomparison = validate_feature_selection(\n    X_train, y_train, X_valid, y_valid, X_test, y_test,\n    full_features=list(FEATURES.keys()),\n    selected_features=selected_features,\n    params=params\n)\n6.2 特征选择效果可视化\ndef plot_feature_selection_comparison(comparison):\n    &quot;&quot;&quot;\n    可视化特征选择对比结果\n \n    参数:\n        comparison: validate_feature_selection 的返回结果\n    &quot;&quot;&quot;\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n \n    # 特征数量对比\n    models = [&#039;全特征&#039;, &#039;精选特征&#039;]\n    n_features = [comparison[&#039;n_full&#039;], comparison[&#039;n_selected&#039;]]\n    ics = [comparison[&#039;full_features_ic&#039;], comparison[&#039;selected_features_ic&#039;]]\n \n    # 特征数 vs IC\n    axes[0].scatter(n_features, ics, s=200, alpha=0.6)\n    axes[0].plot(n_features, ics, &#039;r-&#039;, linewidth=2, alpha=0.5)\n \n    # 添加标注\n    for i, (n, ic, model) in enumerate(zip(n_features, ics, models)):\n        axes[0].annotate(f&#039;{model}\\n{n}特征\\nIC={ic:.4f}&#039;,\n                        (n, ic),\n                        textcoords=&quot;offset points&quot;,\n                        xytext=(0, 10),\n                        ha=&#039;center&#039;)\n \n    axes[0].set_xlabel(&#039;特征数量&#039;)\n    axes[0].set_ylabel(&#039;测试集 IC&#039;)\n    axes[0].set_title(&#039;特征数量 vs 性能&#039;)\n    axes[0].grid(True, alpha=0.3)\n \n    # IC 柱状图\n    bars = axes[1].bar(models, ics, color=[&#039;steelblue&#039;, &#039;coral&#039;], alpha=0.7, edgecolor=&#039;black&#039;)\n    axes[1].set_ylabel(&#039;测试集 IC&#039;)\n    axes[1].set_title(&#039;模型性能对比&#039;)\n    axes[1].grid(True, axis=&#039;y&#039;, alpha=0.3)\n \n    # 添加数值标签\n    for bar, ic in zip(bars, ics):\n        height = bar.get_height()\n        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n                     f&#039;{ic:.4f}&#039;,\n                     ha=&#039;center&#039;, va=&#039;bottom&#039;, fontweight=&#039;bold&#039;)\n \n    # 添加IC保持率\n    axes[1].axhline(y=comparison[&#039;full_features_ic&#039;] * 0.95,\n                   color=&#039;green&#039;, linestyle=&#039;--&#039;, alpha=0.5,\n                   label=&#039;95% 阈值&#039;)\n    axes[1].legend()\n \n    plt.tight_layout()\n    plt.show()\n \n# 使用示例\nplot_feature_selection_comparison(comparison)\n6.3 时序特征重要性\n5. 时序特征重要性\n5.1 滚动窗口特征重要性\n核心思想\n在不同时间窗口内计算特征重要性，分析重要性的稳定性。\n代码实现\ndef rolling_feature_importance(X, y, model_class, params,\n                               window_size=252, step_size=21):\n    &quot;&quot;&quot;\n    滚动窗口特征重要性\n \n    参数:\n        X: 特征矩阵，shape=[n_samples, n_features]\n        y: 目标变量\n        model_class: 模型类\n        params: 模型参数\n        window_size: 窗口大小\n        step_size: 步长\n \n    返回:\n        importance_history: 特征重要性历史\n    &quot;&quot;&quot;\n    n_samples = len(X)\n    importance_history = []\n \n    start_idx = window_size\n    while start_idx + step_size &lt;= n_samples:\n        print(f&quot;Processing window: {start_idx - window_size} - {start_idx}&quot;)\n \n        # 划分数据\n        X_window = X[start_idx - window_size:start_idx]\n        y_window = y[start_idx - window_size:start_idx]\n \n        # 训练模型\n        model = model_class(**params)\n        model.fit(X_window, y_window)\n \n        # 计算特征重要性\n        importance = model.feature_importance(importance_type=&#039;gain&#039;)\n        importance_history.append(importance)\n \n        # 滚动窗口\n        start_idx += step_size\n \n    return np.array(importance_history)\n \n# 使用示例\nimportance_history = rolling_feature_importance(\n    X, y, LGBMRegressor, params,\n    window_size=252,  # 1年\n    step_size=21      # 1个月\n)\n \nprint(f&quot;重要性历史: {importance_history.shape}&quot;)\n5.2 特征重要性稳定性分析\ndef analyze_importance_stability(importance_history, feature_names, top_n=10):\n    &quot;&quot;&quot;\n    分析特征重要性的稳定性\n \n    参数:\n        importance_history: 特征重要性历史，shape=[n_windows, n_features]\n        feature_names: 特征名称\n        top_n: 分析前n个特征\n    &quot;&quot;&quot;\n    # 计算统计量\n    mean_importance = importance_history.mean(axis=0)\n    std_importance = importance_history.std(axis=0)\n    cv_importance = std_importance / (mean_importance + 1e-6)  # 变异系数\n \n    # 排序\n    sorted_indices = np.argsort(mean_importance)[::-1][:top_n]\n \n    # 打印\n    print(f&quot;{&#039;特征&#039;:&lt;20} {&#039;均值&#039;:&gt;10} {&#039;标准差&#039;:&gt;10} {&#039;变异系数&#039;:&gt;10}&quot;)\n    print(&quot;-&quot; * 60)\n    for i, idx in enumerate(sorted_indices):\n        print(f&quot;{feature_names[idx]:&lt;20} &quot;\n              f&quot;{mean_importance[idx]:&gt;10.4f} &quot;\n              f&quot;{std_importance[idx]:&gt;10.4f} &quot;\n              f&quot;{cv_importance[idx]:&gt;10.4f}&quot;)\n \n    # 可视化\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n \n    # 均值vs标准差\n    axes[0].scatter(mean_importance, std_importance, alpha=0.6)\n    for idx in sorted_indices:\n        axes[0].annotate(feature_names[idx],\n                        (mean_importance[idx], std_importance[idx]))\n    axes[0].set_xlabel(&#039;Mean Importance&#039;)\n    axes[0].set_ylabel(&#039;Std Importance&#039;)\n    axes[0].set_title(&#039;Importance Stability&#039;)\n    axes[0].grid(True)\n \n    # 时间序列\n    for idx in sorted_indices:\n        axes[1].plot(importance_history[:, idx], label=feature_names[idx])\n    axes[1].set_xlabel(&#039;Time Window&#039;)\n    axes[1].set_ylabel(&#039;Importance&#039;)\n    axes[1].set_title(f&#039;Top {top_n} Features Importance Over Time&#039;)\n    axes[1].legend()\n    axes[1].grid(True)\n \n    plt.tight_layout()\n    plt.show()\n \n# 使用示例\nanalyze_importance_stability(importance_history, feature_names, top_n=10)\n6. 特征选择策略\n6.1 基于重要性的特征选择\ndef select_features_by_importance(model, X, feature_names,\n                                  importance_type=&#039;gain&#039;, threshold=0.01):\n    &quot;&quot;&quot;\n    基于特征重要性选择特征\n \n    参数:\n        model: LightGBM模型\n        X: 特征矩阵\n        feature_names: 特征名称\n        importance_type: &#039;split&#039; 或 &#039;gain&#039;\n        threshold: 重要性阈值（比例）\n \n    返回:\n        X_selected: 选择后的特征矩阵\n        selected_features: 选择的特征名称\n        selected_indices: 选择的特征索引\n    &quot;&quot;&quot;\n    # 获取特征重要性\n    importance = model.feature_importance(importance_type=importance_type)\n \n    # 归一化\n    importance_normalized = importance / importance.sum()\n \n    # 选择重要性超过阈值的特征\n    selected_indices = np.where(importance_normalized &gt;= threshold)[0]\n \n    # 提取数据\n    X_selected = X[:, selected_indices]\n    selected_features = np.array(feature_names)[selected_indices]\n \n    print(f&quot;选择 {len(selected_indices)}/{len(feature_names)} 个特征&quot;)\n    print(f&quot;累计重要性: {importance_normalized[selected_indices].sum():.2%}&quot;)\n \n    return X_selected, selected_features, selected_indices\n \n# 使用示例\nX_selected, selected_features, selected_indices = select_features_by_importance(\n    model, X, feature_names, importance_type=&#039;gain&#039;, threshold=0.01\n)\n6.2 递归特征消除（RFE）\nfrom sklearn.feature_selection import RFE\n \ndef recursive_feature_elimination(X, y, estimator, n_features_to_select=None,\n                                 step=1, cv=5):\n    &quot;&quot;&quot;\n    递归特征消除\n \n    参数:\n        X: 特征矩阵\n        y: 目标变量\n        estimator: 评估器\n        n_features_to_select: 目标特征数\n        step: 每次消除的特征数\n        cv: 交叉验证折数\n \n    返回:\n        X_selected: 选择后的特征矩阵\n        selected_indices: 选择的特征索引\n        rfe: RFE对象\n    &quot;&quot;&quot;\n    # 创建RFE\n    rfe = RFE(estimator=estimator,\n              n_features_to_select=n_features_to_select,\n              step=step,\n              importance_getter=&#039;auto&#039;)\n \n    # 拟合\n    rfe.fit(X, y)\n \n    # 提取结果\n    selected_indices = np.where(rfe.support_)[0]\n    X_selected = X[:, selected_indices]\n \n    print(f&quot;选择 {len(selected_indices)}/{X.shape[1]} 个特征&quot;)\n \n    return X_selected, selected_indices, rfe\n \n# 使用示例\nestimator = LGBMRegressor(**params)\nX_selected, selected_indices, rfe = recursive_feature_elimination(\n    X, y, estimator, n_features_to_select=50, step=5\n)\n6.3 稳定性特征选择\ndef stable_feature_selection(importance_history, feature_names,\n                             stability_threshold=0.7, top_n=None):\n    &quot;&quot;&quot;\n    稳定性特征选择\n \n    参数:\n        importance_history: 特征重要性历史\n        feature_names: 特征名称\n        stability_threshold: 稳定性阈值\n        top_n: 选择前n个稳定特征\n \n    返回:\n        selected_features: 选择的特征名称\n        stability_scores: 稳定性得分\n    &quot;&quot;&quot;\n    # 计算每个特征的排名\n    rankings = []\n    for importance in importance_history:\n        ranking = np.argsort(importance)[::-1]\n        rankings.append(ranking)\n \n    rankings = np.array(rankings)\n \n    # 计算稳定性得分（基于排名的方差）\n    stability_scores = []\n    for feature_idx in range(len(feature_names)):\n        feature_rankings = np.where(rankings == feature_idx)[1]\n        stability_score = 1 / (1 + np.var(feature_rankings))\n        stability_scores.append(stability_score)\n \n    stability_scores = np.array(stability_scores)\n \n    # 排序\n    sorted_indices = np.argsort(stability_scores)[::-1]\n \n    # 选择稳定特征\n    if top_n is None:\n        selected_indices = sorted_indices[stability_scores[sorted_indices] &gt;= stability_threshold]\n    else:\n        selected_indices = sorted_indices[:top_n]\n \n    selected_features = np.array(feature_names)[selected_indices]\n \n    print(f&quot;选择 {len(selected_indices)} 个稳定特征&quot;)\n \n    return selected_features, stability_scores\n \n# 使用示例\nselected_features, stability_scores = stable_feature_selection(\n    importance_history, feature_names,\n    stability_threshold=0.7, top_n=20\n)\n7. 特征重要性分析的最佳实践\n7.1 综合分析流程\nclass FeatureImportanceAnalyzer:\n    &quot;&quot;&quot;\n    特征重要性分析器\n \n    功能：\n    1. 计算多种特征重要性\n    2. 可视化分析结果\n    3. 时序稳定性分析\n    4. 特征选择建议\n    &quot;&quot;&quot;\n \n    def __init__(self, model, X, y, feature_names):\n        self.model = model\n        self.X = X\n        self.y = y\n        self.feature_names = feature_names\n \n        self.split_importance = None\n        self.gain_importance = None\n        self.permutation_importance = None\n        self.shap_values = None\n \n    def calculate_lightgbm_importance(self):\n        &quot;&quot;&quot;计算LightGBM内置重要性&quot;&quot;&quot;\n        self.split_importance = self.model.feature_importance(importance_type=&#039;split&#039;)\n        self.gain_importance = self.model.feature_importance(importance_type=&#039;gain&#039;)\n \n    def calculate_permutation_importance(self, metric=&#039;ic&#039;, n_repeats=5):\n        &quot;&quot;&quot;计算Permutation Importance&quot;&quot;&quot;\n        self.permutation_importance = permutation_importance(\n            self.model, self.X, self.y,\n            metric=metric, n_repeats=n_repeats\n        )\n \n    def calculate_shap_values(self):\n        &quot;&quot;&quot;计算SHAP值&quot;&quot;&quot;\n        explainer = shap.TreeExplainer(self.model)\n        self.shap_values = explainer.shap_values(self.X)\n \n    def analyze_all(self):\n        &quot;&quot;&quot;执行所有分析&quot;&quot;&quot;\n        print(&quot;计算LightGBM重要性...&quot;)\n        self.calculate_lightgbm_importance()\n \n        print(&quot;计算Permutation Importance...&quot;)\n        self.calculate_permutation_importance()\n \n        print(&quot;计算SHAP值...&quot;)\n        self.calculate_shap_values()\n \n    def plot_summary(self, top_n=20):\n        &quot;&quot;&quot;绘制汇总图&quot;&quot;&quot;\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n \n        # Split重要性\n        indices = np.argsort(self.split_importance)[::-1][:top_n]\n        axes[0, 0].barh(range(len(indices)), self.split_importance[indices][::-1])\n        axes[0, 0].set_yticks(range(len(indices)), np.array(self.feature_names)[indices][::-1])\n        axes[0, 0].set_title(&#039;Split Importance&#039;)\n        axes[0, 0].invert_yaxis()\n \n        # Gain重要性\n        indices = np.argsort(self.gain_importance)[::-1][:top_n]\n        axes[0, 1].barh(range(len(indices)), self.gain_importance[indices][::-1])\n        axes[0, 1].set_yticks(range(len(indices)), np.array(self.feature_names)[indices][::-1])\n        axes[0, 1].set_title(&#039;Gain Importance&#039;)\n        axes[0, 1].invert_yaxis()\n \n        # Permutation Importance\n        if self.permutation_importance is not None:\n            mean_imp = self.permutation_importance.mean(axis=1)\n            indices = np.argsort(mean_imp)[::-1][:top_n]\n            axes[1, 0].barh(range(len(indices)), mean_imp[indices][::-1])\n            axes[1, 0].set_yticks(range(len(indices)), np.array(self.feature_names)[indices][::-1])\n            axes[1, 0].set_title(&#039;Permutation Importance&#039;)\n            axes[1, 0].invert_yaxis()\n \n        # SHAP重要性\n        if self.shap_values is not None:\n            mean_shap = np.abs(self.shap_values).mean(axis=0)\n            indices = np.argsort(mean_shap)[::-1][:top_n]\n            axes[1, 1].barh(range(len(indices)), mean_shap[indices][::-1])\n            axes[1, 1].set_yticks(range(len(indices)), np.array(self.feature_names)[indices][::-1])\n            axes[1, 1].set_title(&#039;SHAP Importance&#039;)\n            axes[1, 1].invert_yaxis()\n \n        plt.tight_layout()\n        plt.show()\n \n# 使用示例\nanalyzer = FeatureImportanceAnalyzer(model, X_val, y_val, feature_names)\nanalyzer.analyze_all()\nanalyzer.plot_summary(top_n=15)\n7.2 检查清单\n特征重要性分析检查清单\n\n 计算至少两种不同的特征重要性\n 可视化特征重要性分布\n 检查特征重要性的稳定性\n 分析特征间的相关性\n 验证特征选择的合理性\n 记录分析过程和结论\n\n时序特征重要性分析检查清单\n\n 使用滚动窗口分析重要性变化\n 识别稳定和不稳定特征\n 分析不同市场状态下的重要性\n 检查重要性与市场周期的关系\n 制定特征更新策略\n\n8. 总结\n特征重要性分析是量化模型开发中的关键环节：\n\n基础重要性：LightGBM内置的Split和Gain重要性\nPermutation Importance：更真实的重要性评估方法\nSHAP分析：提供个体和全局解释\n时序分析：分析重要性的稳定性\n特征选择：基于重要性的特征筛选策略\n\n正确的特征重要性分析能够帮助我们：\n\n理解模型决策逻辑\n识别有效因子\n提升模型性能\n控制模型风险\n"},"quant/qlib/week2/06-学习检查清单":{"slug":"quant/qlib/week2/06-学习检查清单","filePath":"quant/qlib/week2/06-学习检查清单.md","title":"06-学习检查清单","links":[],"tags":[],"content":"学习检查清单\n📋 学习目标检查\n✅ Module 3.1: Gradient Boosting 原理\n\n\n 理解 Boosting 的迭代纠错机制\n\n能解释 Tree₁ → Tree₂ → Tree₃ 的迭代过程\n理解残差学习 r₁ = y - ŷ₁ 的含义\n知道最终预测 = ŷ₁ + ŷ₂ + ŷ₃ + …\n\n\n\n 知道 Bagging vs Boosting 的区别\n\n并行 vs 串行\n降低方差 vs 降低偏差\n不容易过拟合 vs 需要早停控制\n\n\n\n 了解 LightGBM 的三大创新\n\nGOSS (梯度单边采样)：保留大梯度，采样小梯度\nEFB (互斥特征捆绑)：合并稀疏特征\nLeaf-wise 生长：每次分裂增益最大的叶子\n\n\n\n 理解为什么量化偏爱 Boosting\n\n信号很弱（IC 只有 0.03~0.08）\n特征稀疏（不是所有特征都有用）\n需要快速迭代（每天都有新数据）\n\n\n\n✅ Module 3.2: 时序数据划分\n\n\n 知道为什么不能随机划分时序数据\n\n理解数据泄露的概念\n能举例说明随机划分的问题\n知道因果性约束的重要性\n\n\n\n 能正确实现时序划分\ndates = X.index.get_level_values(0)\nunique_dates = dates.unique().sort_values()\ntrain_mask = dates &lt;= unique_dates[int(n_dates * 0.7)]\nvalid_mask = (dates &gt; unique_dates[int(n_dates * 0.7)]) &amp; (dates &lt;= unique_dates[int(n_dates * 0.85)])\ntest_mask = dates &gt; unique_dates[int(n_dates * 0.85)]\n\n\n 了解 Purging 和 Embargo 的作用\n\nPurging: 删除训练集末尾 N 天\nEmbargo: 验证集开头额外空出几天\n能实现完整的 Purging + Embargo\n\n\n\n 能实现 Walk-Forward 验证\n\n理解滚动窗口验证的原理\n能实现 Walk-Forward 验证代码\n能分析多个窗口的 IC 变化\n\n\n\n✅ Module 3.3: LightGBM 训练\n\n\n 了解关键参数的含义\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数推荐值作用learning_rate0.01~0.1控制收敛速度num_leaves31~127控制模型复杂度max_depth6~10防止过拟合feature_fraction0.8特征采样bagging_fraction0.8样本采样lambda_l10.1L1 正则lambda_l20.1L2 正则\n\n\n 能正确创建 Dataset\ntrain_data = lgb.Dataset(X_train.values, label=y_train.values,\n                         feature_name=list(FEATURES.keys()),\n                         categorical_feature=[])\nvalid_data = lgb.Dataset(X_valid.values, label=y_valid.values,\n                         reference=train_data)\n\n\n 能训练模型并使用早停\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=500,\n    valid_sets=[train_data, valid_data],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=30),\n        lgb.log_evaluation(period=50)\n    ]\n)\n\n\n 能保存和加载模型\nmodel.save_model(&#039;model.txt&#039;)\nmodel = lgb.Booster(model_file=&#039;model.txt&#039;)\n\n\n✅ Module 3.4: IC/ICIR 评估\n\n\n 理解为什么用 IC 而不是 MSE\n\n量化只关心排序，不关心绝对值\nIC 衡量排序能力，MSE 衡量精确度\n预测 A=0.05, B=0.03 vs A=0.02, B=0.01\n→ MSE 不同，但排序相同，IC 相同\n\n\n\n 能计算 IC 和 Rank IC\nfrom scipy.stats import pearsonr, spearmanr\n \nic, _ = pearsonr(y_pred, y_true)\nrank_ic, _ = spearmanr(y_pred, y_true)\n\n\n 能计算 ICIR 和 IC 胜率\nicir = ic_series.mean() / ic_series.std()\nwin_rate = (ic_series &gt; 0).mean()\n\n\n 能评估模型质量\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIC 值模型质量&gt; 0.10🌟 顶级 (非常罕见)&gt; 0.05✅ 优秀&gt; 0.03✅ 有效&gt; 0.02⚠️ 一般&lt; 0.02❌ 较弱\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nICIR 值稳定性&gt; 0.5🌟 非常稳定&gt; 0.3✅ 较稳定&gt; 0.2⚠️ 一般&lt; 0.2❌ 不稳定\n\n\n✅ Module 3.5: 特征重要性分析\n\n\n 知道三种重要性计算方式\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n方式速度可靠性推荐度Split快中等⚠️ 一般Gain快高✅ 推荐Permutation慢最高✅ 最可靠\n\n\n 能获取和解读特征重要性\nimportance = model.feature_importance(importance_type=&#039;gain&#039;)\nimportance_df = pd.DataFrame({\n    &#039;feature&#039;: feature_names,\n    &#039;importance&#039;: importance\n}).sort_values(&#039;importance&#039;, ascending=False)\n\n\n 能实现特征选择策略\n\n阈值选择\nTop-K 选择\n递归特征消除 (RFE)\n\n\n\n 能分析特征相关性\ncorr_matrix = X[features].corr()\nhigh_corr_pairs = []\nfor i, feat1 in enumerate(features):\n    for feat2 in features[i+1:]:\n        corr = corr_matrix.loc[feat1, feat2]\n        if abs(corr) &gt; threshold:\n            high_corr_pairs.append((feat1, feat2, corr))\n\n\n🎯 实践能力检查\n✅ 能独立完成的项目\n\n\n 项目1: 基础模型训练\n\n加载数据，正确划分训练集/验证集/测试集\n训练 LightGBM 模型\n评估模型 IC 和 ICIR\n打印完整评估报告\n\n\n\n 项目2: 时序数据划分\n\n实现 Walk-Forward 验证\n实现 Purging 和 Embargo\n分析多个窗口的 IC 变化\n识别模型是否需要重训练\n\n\n\n 项目3: 特征工程\n\n计算多种特征重要性（Split, Gain, Permutation）\n分析特征相关性\n实现特征选择\n验证特征选择的效果\n\n\n\n 项目4: 模型优化\n\n调优关键参数\n实现早停机制\n防止过拟合\n提升模型 IC 和 ICIR\n\n\n\n✅ 能回答的问题\n\n\n为什么量化投资偏爱 Boosting 而不是 Bagging？\n\nBoosting 更适合捕捉微弱信号\n能逐步降低偏差\n在量化场景中表现更好\n\n\n\n为什么不能用随机划分时序数据？\n\n会导致数据泄露（未来信息）\n不符合实际投资场景\n模型性能虚高\n\n\n\nPurging 和 Embargo 的作用是什么？\n\nPurging: 删除训练集末尾，避免标签包含未来信息\nEmbargo: 验证集开头空出几天，作为缓冲\n两者结合，更严格地防止信息泄露\n\n\n\nIC 和 MSE 哪个更重要？\n\nIC 更重要，因为量化只关心排序\nMSE 衡量精确度，不适合量化场景\nIC 能直接反映预测能力\n\n\n\n如何判断模型是否过拟合？\n\n训练集 IC 远高于验证集 IC\n使用早停机制\n增加正则化\n减少特征数量\n\n\n\n特征重要性分析的意义是什么？\n\n理解模型决策逻辑\n识别有效因子\n剔除冗余特征\n提升模型性能\n\n\n\n📚 推荐学习路径\n🟢 初学者路径\n1. 理解 Gradient Boosting 原理\n   ↓\n2. 学习时序数据划分\n   ↓\n3. 训练第一个 LightGBM 模型\n   ↓\n4. 计算 IC 和 ICIR\n   ↓\n5. 分析特征重要性\n\n🟡 进阶路径\n1. 掌握 IC 优化训练\n   ↓\n2. 学习在线学习\n   ↓\n3. 实现 Walk-Forward 验证\n   ↓\n4. 掌握高级评估方法\n   ↓\n5. 进行稳定性分析\n\n🔴 实战路径\n1. 从实际项目出发\n   ↓\n2. 遇到问题查文档\n   ↓\n3. 理论原理学习\n   ↓\n4. 实践应用\n   ↓\n5. 持续优化迭代\n\n🔧 工具箱\n必备 Python 库\nimport lightgbm as lgb  # LightGBM\nimport pandas as pd       # 数据处理\nimport numpy as np        # 数值计算\nfrom scipy.stats import pearsonr, spearmanr  # IC 计算\nimport matplotlib.pyplot as plt  # 可视化\nimport shap  # SHAP 解释\n常用代码片段\n1. 数据划分\ndef train_val_test_split(X, y, dates, train_ratio=0.7, val_ratio=0.15):\n    unique_dates = np.unique(dates)\n    n_dates = len(unique_dates)\n    train_end_idx = int(n_dates * train_ratio)\n    val_end_idx = int(n_dates * (train_ratio + val_ratio))\n \n    train_mask = dates &lt;= unique_dates[train_end_idx]\n    valid_mask = (dates &gt; unique_dates[train_end_idx]) &amp; (dates &lt;= unique_dates[val_end_idx])\n    test_mask = dates &gt; unique_dates[val_end_idx]\n \n    return (X[train_mask], X[valid_mask], X[test_mask]), (y[train_mask], y[valid_mask], y[test_mask])\n2. 模型训练\ndef train_lightgbm(X_train, y_train, X_valid, y_valid, params):\n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n \n    model = lgb.train(\n        params,\n        train_data,\n        num_boost_round=500,\n        valid_sets=[train_data, valid_data],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=30),\n            lgb.log_evaluation(period=50)\n        ]\n    )\n \n    return model\n3. IC 评估\ndef evaluate_ic(y_pred, y_true):\n    ic, _ = pearsonr(y_pred, y_true)\n    rank_ic, _ = spearmanr(y_pred, y_true)\n    return ic, rank_ic\n4. 特征重要性\ndef get_feature_importance(model, feature_names, importance_type=&#039;gain&#039;):\n    importance = model.feature_importance(importance_type=importance_type)\n    importance_df = pd.DataFrame({\n        &#039;feature&#039;: feature_names,\n        &#039;importance&#039;: importance\n    }).sort_values(&#039;importance&#039;, ascending=False)\n    return importance_df\n🚀 下一步学习\n完成 Week 3 后，你已经掌握了：\n✅ Gradient Boosting 原理\n✅ 时序数据正确划分\n✅ LightGBM 模型训练\n✅ 量化模型评估 (IC/ICIR)\n✅ 特征重要性分析与选择\n下一步：Week 4 - 策略回测\n你将学习：\n\n交易策略原理 (Top-K、等权重、IC加权)\n投资组合构建方法\n回测框架使用\n风险指标计算 (夏普比率、最大回撤等)\n\n\n祝学习顺利！ 🎉"},"quant/qlib/week2/index":{"slug":"quant/qlib/week2/index","filePath":"quant/qlib/week2/index.md","title":"index","links":["quant/qlib/week2/01-Gradient-Boosting原理","quant/qlib/week2/02-时序数据划分","quant/qlib/week2/03-模型训练","quant/qlib/week2/04-IC-Rank-IC评估指标","quant/qlib/week2/05-特征重要性分析","/"],"tags":[],"content":"LightGBM 量化投资应用指南\nLightGBM 是微软开源的高性能梯度提升框架，在量化投资领域有着广泛的应用。本文档系统讲解了 LightGBM 在量化场景下的应用方法。\n\n📖 文档目录\n1️⃣ Gradient Boosting 原理\n→ 阅读完整文档\n核心内容：\n\nBoosting算法基础与数学推导\nLightGBM三大创新：GOSS、EFB、Leaf-wise\n量化场景下的优势分析\n核心参数详解\n\n适合人群：想要深入理解LightGBM原理的开发者\n\n2️⃣ 时序数据划分\n→ 阅读完整文档\n核心内容：\n\n量化时序数据的因果性约束\n时间序列交叉验证（TimeSeriesSplit）\n滚动窗口验证与步进验证\n多市场周期的数据划分策略\n\n适合人群：需要处理时序数据的量化开发者\n\n3️⃣ 模型训练\n→ 阅读完整文档\n核心内容：\n\nLightGBM基础训练流程\n针对IC优化的训练策略\n在线学习（Online Learning）\n学习率调度与特征采样\n分布式训练与模型管理\n\n适合人群：需要构建量化模型的开发者\n\n4️⃣ IC/Rank IC 评估指标\n→ 阅读完整文档\n核心内容：\n\nIC与Rank IC的定义与区别\nIC的统计显著性检验\n滚动IC与IC衰减分析\nIR（Information Ratio）指标\n多维度IC表现分析\n\n适合人群：需要评估模型性能的量化研究者\n\n5️⃣ 特征重要性分析\n→ 阅读完整文档\n核心内容：\n\nSplit与Gain特征重要性\nPermutation Importance实现\nSHAP值分析与应用\n时序特征重要性分析\n稳定性特征选择策略\n\n适合人群：需要进行特征工程的开发者\n\n🎯 学习路径\n🟢 初学者路径\n梯度提升原理 → 时序数据划分 → 模型训练基础 → IC评估 → 特征重要性\n\n🟡 进阶路径\nIC优化训练 → 在线学习 → 分布式训练 → 高级评估方法 → 稳定性分析\n\n🔴 实战路径\n从实际项目出发 → 遇到问题查文档 → 理论原理 → 实践应用\n\n\n💡 实用提示\n\n文档示例：所有代码示例均可直接运行\n量化场景：内容针对量化投资特点设计\n最佳实践：包含大量实战经验总结\n持续更新：跟随最新技术发展\n\n🔍 最佳实践\n时序划分\n# ✅ 正确做法\ndates = X.index.get_level_values(0)\nunique_dates = dates.unique().sort_values()\ntrain_mask = dates &lt;= unique_dates[int(n_dates * 0.7)]\nvalid_mask = (dates &gt; unique_dates[int(n_dates * 0.7)]) &amp; (dates &lt;= unique_dates[int(n_dates * 0.85)])\ntest_mask = dates &gt; unique_dates[int(n_dates * 0.85)]\n \n# ❌ 错误做法\nfrom sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(X, test_size=0.3)  # 数据泄露!\n模型训练\n# ✅ 推荐做法\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=500,\n    valid_sets=[train_data, valid_data],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=30),  # 使用早停\n        lgb.log_evaluation(period=50)\n    ]\n)\n \n# ⚠️ 不推荐做法\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=100,  # 固定轮数，可能过拟合或欠拟合\n    valid_sets=[train_data, valid_data],\n)\n模型评估\n# ✅ 推荐做法 - 使用 IC/ICIR\nmetrics = calculate_ic_metrics(pred_df, true_df)\nprint(f&quot;IC: {metrics[&#039;IC_mean&#039;]:.4f}&quot;)\nprint(f&quot;ICIR: {metrics[&#039;ICIR&#039;]:.4f}&quot;)\n \n# ⚠️ 不推荐做法 - 只看 MSE\nmse = mean_squared_error(y_true, y_pred)\nprint(f&quot;MSE: {mse:.6f}&quot;)  # 不能反映排序能力\n特征选择\n# ✅ 推荐做法 - 结合重要性和相关性\nselected = select_features_by_threshold(importance_df, threshold=0.2)\nhigh_corr = analyze_feature_correlation(X, selected, threshold=0.9)\n# 手动去除冗余特征\n \n# ⚠️ 不推荐做法 - 只看重要性，不考虑相关性\nselected = importance_df.head(10)[&#039;feature&#039;].tolist()  # 可能包含高度相关的特征\n❓ 常见问题\nQ1: 为什么要用 IC 而不是 MSE？\nA: 量化投资只关心排序，不关心预测值的绝对准确性。\n示例：\n股票   预测收益   真实收益   MSE   IC\nA      0.05       0.06      ...   ✓ 排序正确\nB      0.04       0.04      ...   ✓ 排序正确\nC      0.03       0.02      ...   ✓ 排序正确\n\n模型1: 预测 [0.05, 0.04, 0.03]\n模型2: 预测 [0.50, 0.40, 0.30]\n\n→ 两种预测的 MSE 不同，但排序相同\n→ IC 相同，IC 均能反映预测质量\n\nQ2: 早停轮数如何选择？\nA: 一般选择 20-50，取决于：\n\n数据量大小：大数据可以用更大的轮数\n学习率：小学习率需要更多轮数\n稳定性要求：高风险场景用更保守的轮数\n\n推荐：\nlgb.early_stopping(stopping_rounds=30)  # 常用默认值\nQ3: 如何防止过拟合？\nA: 多种方法组合使用：\n\n\n早停机制\nlgb.early_stopping(stopping_rounds=30)\n\n\n正则化\nparams = {\n    &#039;lambda_l1&#039;: 0.1,  # L1 正则\n    &#039;lambda_l2&#039;: 0.1,  # L2 正则\n}\n\n\n采样\nparams = {\n    &#039;feature_fraction&#039;: 0.8,  # 特征采样\n    &#039;bagging_fraction&#039;: 0.8,  # 样本采样\n}\n\n\n交叉验证\n\n使用 Walk-Forward 验证\n多个窗口测试稳定性\n\n\n\nQ4: IC 和 Rank IC 哪个更好？\nA: Rank IC 更稳定。\n原因：\n\n抗极端值\n不受异常值影响\n更适合量化场景\n\n推荐：\n# 主要看 Rank IC\nprint(f&quot;Rank IC: {rank_ic:.4f}&quot;)\n \n# IC 作为参考\nprint(f&quot;IC: {ic:.4f}&quot;)\nQ5: 特征重要性下降该怎么办？\nA: 分情况处理：\n\n\n重要性普遍低 (&lt; 5)\n\n可能特征质量差\n需要重新构造特征\n\n\n\n某些特征重要性低\n\n可能与其他特征相关\n检查特征相关性\n考虑剔除冗余特征\n\n\n\n重要性随时间变化\n\n可能市场环境变化\n考虑滚动窗口训练\n\n\n\nQ6: 模型 IC 算好吗？\nA: 参考以下标准：\nIC &gt; 0.10: 🌟 顶级 (非常罕见)\nIC &gt; 0.05: ✅ 优秀\nIC &gt; 0.03: ✅ 有效\nIC &gt; 0.02: ⚠️ 一般\nIC &lt; 0.02: ❌ 较弱\n\n注意：\n\n测试集 IC 可能比验证集低 20-30%\n实盘 IC 可能比回测低 30-50%\n需要结合 ICIR 判断稳定性\n\n\n← 返回首页"},"quant/qlib/week3/01-交易策略理论":{"slug":"quant/qlib/week3/01-交易策略理论","filePath":"quant/qlib/week3/01-交易策略理论.md","title":"01-交易策略理论","links":[],"tags":[],"content":"交易策略理论\n1. 交易策略定义\n1.1 策略的核心组成\n交易策略 = 信号生成 + 仓位管理 + 风险控制 + 执行规则\n市场数据 → 特征工程 → 模型预测 → 信号生成\n                            ↓\n                        投资组合构建 → 交易执行 → 绩效评估\n\n1.2 四个关键组成部分\n1. 信号生成\n定义：决定买什么、卖什么\n常见方法：\n\nTop-K 策略：选择预测最好的 K 只股票\n回归预测：预测每只股票的收益率\n分类预测：预测股票的涨跌\n\n输出：股票选择或买卖信号\ndef generate_signals(predictions, method=&#039;topk&#039;, k=20):\n    &quot;&quot;&quot;\n    生成交易信号\n    \n    参数:\n        predictions: 股票预测分数\n        method: 策略方法 (&#039;topk&#039;, &#039;regression&#039;, &#039;classification&#039;)\n        k: 选择股票数量（Top-K策略）\n    \n    返回:\n        signals: 交易信号\n    &quot;&quot;&quot;\n    if method == &#039;topk&#039;:\n        # 选择预测分数最高的K只股票\n        topk = predictions.nlargest(k)\n        signals = (predictions &gt;= topk.min()).astype(int)\n    elif method == &#039;regression&#039;:\n        # 使用回归预测值作为信号\n        signals = predictions\n    elif method == &#039;classification&#039;:\n        # 使用分类预测结果（1=买入, -1=卖出）\n        signals = predictions\n    \n    return signals\n2. 仓位管理\n定义：决定买多少、卖多少\n常见方法：\n\n等权重：每只股票分配相同资金\nIC加权：根据历史IC值分配权重\n风险平价：等风险贡献\nMV优化：均值-方差优化\n\n输出：每只股票的权重分配\n3. 风险控制\n定义：限制投资风险\n常见方法：\n\n最大单股权重限制\n行业分散要求\n止损策略\n风险平价\n\n输出：仓位调整或平仓信号\n4. 执行规则\n定义：何时调仓、如何执行\n常见方法：\n\n定期调仓：每日、每周、每月调仓\n触发调仓：达到某个条件时调仓\n\n输出：交易指令\n\n2. 量化投资基础概念\n2.1 信息系数（IC）\n定义：预测分数与实际收益的秩相关系数\n计算公式：\nIC = RankCorrelation(预测分数, 实际收益)\n\nPython 实现：\nfrom scipy.stats import spearmanr\n \ndef calculate_ic(predictions, returns):\n    &quot;&quot;&quot;\n    计算 IC（信息系数）\n    \n    参数:\n        predictions: 预测分数\n        returns: 实际收益率\n    \n    返回:\n        ic: IC 值\n    &quot;&quot;&quot;\n    ic, _ = spearmanr(predictions, returns)\n    return ic\n \n# 示例\npredictions = pd.Series([0.05, 0.03, 0.01, -0.02, -0.05])\nreturns = pd.Series([0.06, 0.04, 0.02, -0.01, -0.04])\n \nic = calculate_ic(predictions, returns)\nprint(f&quot;IC = {ic:.4f}&quot;)\nIC 含义：\n\nIC &gt; 0.05：模型有一定预测能力\nIC &gt; 0.1：模型预测能力较强\nIC &lt; 0：预测方向错误\n\nICIR（信息比率）：\nICIR = mean(IC) / std(IC)\n\ndef calculate_icir(ic_series):\n    &quot;&quot;&quot;\n    计算 ICIR（信息比率）\n    \n    参数:\n        ic_series: IC 序列\n    \n    返回:\n        icir: ICIR 值\n    &quot;&quot;&quot;\n    icir = ic_series.mean() / ic_series.std()\n    return icir\n2.2 换手率\n定义：衡量交易活跃程度的指标\n计算公式：\n换手率 = 交易额 / 平均资产\n\nPython 实现：\ndef calculate_turnover(portfolio_changes, avg_value):\n    &quot;&quot;&quot;\n    计算换手率\n    \n    参数:\n        portfolio_changes: 持仓变化（绝对值）\n        avg_value: 平均资产价值\n    \n    返回:\n        turnover: 换手率\n    &quot;&quot;&quot;\n    turnover = portfolio_changes.abs().sum() / avg_value\n    return turnover\n \n# 示例\n# 假设某日持仓变化\nportfolio_changes = pd.Series({\n    &#039;600000&#039;: 100000,   # 买入10万\n    &#039;600001&#039;: -50000,   # 卖出5万\n    &#039;600002&#039;: 80000,    # 买入8万\n})\n \navg_value = 1000000  # 平均资产100万\nturnover = calculate_turnover(portfolio_changes, avg_value)\nprint(f&quot;换手率 = {turnover:.2%}&quot;)\n换手率含义：\n\n高换手率：交易频繁，成本高\n低换手率：交易少，成本低\n过高换手可能损害收益\n\n换手率范围：\n\n低频策略：50%-100%\n中频策略：100%-300%\n高频策略：300%-1000%\n\n2.3 回撤\n定义：从历史最高点到当前点的跌幅\n计算公式：\n回撤 = (当前净值 - 历史最高净值) / 历史最高净值\n\n最大回撤：\n最大回撤 = min(回撤)  # 回撤的最小值\n\nPython 实现：\ndef calculate_drawdown(cumulative_returns):\n    &quot;&quot;&quot;\n    计算回撤\n    \n    参数:\n        cumulative_returns: 累计收益率序列\n    \n    返回:\n        drawdown: 回撤序列\n        max_drawdown: 最大回撤\n    &quot;&quot;&quot;\n    # 计算累计净值\n    cumulative_value = (1 + cumulative_returns).cumprod()\n    \n    # 计算历史最高点\n    running_max = cumulative_value.expanding().max()\n    \n    # 计算回撤\n    drawdown = (cumulative_value - running_max) / running_max\n    \n    # 最大回撤\n    max_drawdown = drawdown.min()\n    \n    return drawdown, max_drawdown\n \n# 示例\nreturns = pd.Series([0.01, 0.02, -0.01, 0.03, -0.02, 0.01, -0.03])\ndrawdown, max_drawdown = calculate_drawdown(returns)\n \nprint(f&quot;最大回撤 = {max_drawdown:.2%}&quot;)\nprint(f&quot;当前回撤 = {drawdown.iloc[-1]:.2%}&quot;)\n\n3. Top-K 策略\n3.1 核心思想\n\n对所有股票的预测分数排序\n选择预测最好的 K 只股票\n按某种权重分配资金（通常等权重）\n定期调仓\n\n3.2 算法步骤\ndef top_k_strategy(predictions, k=20):\n    &quot;&quot;&quot;\n    Top-K 投资组合策略\n    \n    参数:\n        predictions: 股票预测分数\n        k: 选择股票数量\n    \n    返回:\n        weights: 股票权重，权重和为1\n    &quot;&quot;&quot;\n    # 1. 按预测分数排序（降序）\n    sorted_predictions = predictions.sort_values(ascending=False)\n    \n    # 2. 选择 Top-K\n    top_k = sorted_predictions[:k]\n    \n    # 3. 计算等权重\n    weight = 1.0 / k\n    \n    # 4. 分配权重\n    weights = pd.Series(0, index=predictions.index)\n    weights[top_k.index] = weight\n    \n    return weights\n \n# 示例\npredictions = pd.Series({\n    &#039;600000&#039;: 0.025,\n    &#039;600001&#039;: 0.023,\n    &#039;600002&#039;: 0.010,\n    &#039;600003&#039;: 0.009,\n    &#039;600004&#039;: -0.015,\n})\n \nweights = top_k_strategy(predictions, k=3)\nprint(&quot;Top-K 权重分配:&quot;)\nprint(weights)\n示例输出：\nTop-K 权重分配:\n600000    0.333333\n600001    0.333333\n600002    0.333333\n600003    0.000000\n600004    0.000000\ndtype: float64\n\n3.3 优势与风险\n优势：\n\n✅ 简单直观，易于实现\n✅ 集中投资在最有把握的股票\n✅ 自动分散风险（持有多只股票）\n✅ 管理成本低，易于维护\n\n风险：\n\n⚠️ 对预测质量敏感\n⚠️ 可能集中投资相似股票\n⚠️ 换手率可能较高\n⚠️ 没有考虑风险和相关性\n\n3.4 参数选择\nK 值的影响：\n\nK 小（如10）：集中度高，风险大，潜在收益高\nK 大（如50）：分散度高，风险低，收益可能降低\nK = 全市场：等权重市场组合，接近指数投资\n\n实践建议：\n\nK 通常选择 20-50 之间\n根据流动性调整 K 值\n考虑市场环境调整 K 值\n\n\n4. IC 权重策略\n4.1 核心思想\n根据历史 IC 值分配权重，预测能力强的股票获得更高权重\n4.2 IC 计算方法\n单期 IC：\nIC_t = RankCorrelation(预测分数_t, 实际收益_t)\n滚动 IC：\n# 计算过去 N 期的平均 IC\nIC_rolling = mean(IC_{t-N+1}, ..., IC_t)\nPython 实现：\ndef calculate_rolling_ic(predictions, returns, window=20):\n    &quot;&quot;&quot;\n    计算滚动 IC\n    \n    参数:\n        predictions: 预测分数 DataFrame (index: date, columns: stock)\n        returns: 实际收益率 DataFrame (index: date, columns: stock)\n        window: 滚动窗口大小\n    \n    返回:\n        rolling_ic: 滚动 IC DataFrame\n    &quot;&quot;&quot;\n    rolling_ic = pd.DataFrame(index=predictions.index, columns=predictions.columns)\n    \n    for stock in predictions.columns:\n        for date in predictions.index[window:]:\n            # 获取窗口期数据\n            pred_window = predictions.loc[date - window + 1: date, stock]\n            ret_window = returns.loc[date - window + 1: date, stock]\n            \n            # 计算 IC\n            ic, _ = spearmanr(pred_window, ret_window)\n            rolling_ic.loc[date, stock] = ic\n    \n    return rolling_ic\n4.3 权重分配方法\n方法 1：直接 IC 权重\ndef ic_weight_direct(rolling_ic, min_ic=0.02):\n    &quot;&quot;&quot;\n    直接 IC 权重分配\n    \n    参数:\n        rolling_ic: 滚动 IC\n        min_ic: 最小 IC 阈值\n    \n    返回:\n        weights: 股票权重\n    &quot;&quot;&quot;\n    # 只选择 IC &gt; min_ic 的股票\n    valid_stocks = rolling_ic[rolling_ic &gt; min_ic]\n    \n    # 根据 IC 分配权重\n    weights = valid_stocks / valid_stocks.sum()\n    \n    return weights\n方法 2：IC 绝对值权重\ndef ic_weight_absolute(rolling_ic, min_ic=0.02):\n    &quot;&quot;&quot;\n    IC 绝对值权重分配\n    \n    参数:\n        rolling_ic: 滚动 IC\n        min_ic: 最小 IC 阈值\n    \n    返回:\n        weights: 股票权重\n    &quot;&quot;&quot;\n    # 取 IC 的绝对值\n    abs_ic = rolling_ic.abs()\n    \n    # 只选择 IC &gt; min_ic 的股票\n    valid_stocks = abs_ic[abs_ic &gt; min_ic]\n    \n    # 根据 IC 绝对值分配权重\n    weights = valid_stocks / valid_stocks.sum()\n    \n    return weights\n方法 3：平滑 IC 权重\ndef ic_weight_smooth(rolling_ic, min_ic=0.02, alpha=1.0):\n    &quot;&quot;&quot;\n    平滑 IC 权重分配（softmax）\n    \n    参数:\n        rolling_ic: 滚动 IC\n        min_ic: 最小 IC 阈值\n        alpha: 平滑系数\n    \n    返回:\n        weights: 股票权重\n    &quot;&quot;&quot;\n    # 只选择 IC &gt; min_ic 的股票\n    valid_stocks = rolling_ic[rolling_ic &gt; min_ic]\n    \n    # Softmax 归一化\n    weights = np.exp(alpha * valid_stocks) / np.sum(np.exp(alpha * valid_stocks))\n    \n    return weights\n4.4 优势与风险\n优势：\n\n✅ 考虑预测能力差异\n✅ 动态调整权重\n✅ 对历史表现好的股票给予更高权重\n\n风险：\n\n⚠️ 需要足够历史数据\n⚠️ IC 可能不稳定\n⚠️ 换手率可能较高\n⚠️ 过度拟合历史数据\n\n4.5 实践建议\n\n使用滚动窗口计算 IC（如 20 期）\n设置最小 IC 阈值\n对 IC 进行平滑处理\n限制单股权重最大值\n\n\n5. 均值-方差优化（MVO）\n5.1 理论基础\n马科维茨现代投资组合理论：在给定风险水平下最大化收益\n5.2 数学模型\n优化问题：\nmax: μ^T w - λ w^T Σ w\n\ns.t.:\n    Σw_i = 1        (权重和为1)\n    w_i ≥ 0         (不允许做空)\n    w_i ≤ w_max     (最大单股权重)\n\n变量说明：\n\nw：权重向量\nμ：期望收益向量\nΣ：协方差矩阵\nλ：风险厌恶系数\n\n5.3 有效前沿\n定义：在不同风险水平下获得最大期望收益的投资组合集合\n意义：\n\n有效前沿上的组合都是最优的\n投资者根据风险偏好选择组合\n有效前沿是凸的\n\n5.4 参数估计\n期望收益估计：\nmu = returns.mean()\n协方差矩阵估计：\nsigma = returns.cov()\n正则化（防止过拟合）：\nsigma_reg = sigma + lambda_reg * np.eye(len(returns.columns))\n5.5 CVXPY 求解\nimport cvxpy as cp\n \ndef mv_optimization(returns, risk_aversion=1.0, max_weight=0.1):\n    &quot;&quot;&quot;\n    均值-方差优化\n    \n    参数:\n        returns: 历史收益率\n        risk_aversion: 风险厌恶系数\n        max_weight: 最大单股权重\n    \n    返回:\n        weights: 最优权重\n    &quot;&quot;&quot;\n    n = len(returns.columns)\n    \n    # 定义变量\n    w = cp.Variable(n)\n    \n    # 定义参数\n    mu = returns.mean().values\n    sigma = returns.cov().values\n    \n    # 正则化\n    sigma_reg = sigma + 1e-6 * np.eye(n)\n    \n    # 定义目标函数\n    objective = cp.Maximize(mu @ w - risk_aversion * cp.quad_form(w, sigma_reg))\n    \n    # 定义约束\n    constraints = [\n        cp.sum(w) == 1,  # 权重和为1\n        w &gt;= 0,         # 不允许做空\n        w &lt;= max_weight # 最大单股权重\n    ]\n    \n    # 求解\n    problem = cp.Problem(objective, constraints)\n    problem.solve()\n    \n    # 获取解\n    optimal_weights = pd.Series(w.value, index=returns.columns)\n    \n    return optimal_weights\n \n# 示例\nimport numpy as np\nimport pandas as pd\n \n# 生成示例数据\nnp.random.seed(42)\nn_stocks = 10\nn_days = 252\nstocks = [f&#039;stock_{i}&#039; for i in range(n_stocks)]\ndates = pd.date_range(&#039;2020-01-01&#039;, periods=n_days, freq=&#039;D&#039;)\n \nreturns = pd.DataFrame(np.random.randn(n_days, n_stocks) * 0.02, \n                      index=dates, columns=stocks)\n \n# MV 优化\nweights = mv_optimization(returns, risk_aversion=1.0, max_weight=0.2)\n \nprint(&quot;MV 优化权重:&quot;)\nprint(weights)\n5.6 优势与风险\n优势：\n\n✅ 理论基础扎实（诺贝尔奖理论）\n✅ 考虑风险和相关性\n✅ 科学化资产配置\n✅ 可以量化风险\n\n风险：\n\n⚠️ 依赖参数估计（μ 和 Σ）\n⚠️ 估计误差影响大\n⚠️ 可能集中投资少数股票\n⚠️ 对异常值敏感\n\n5.7 实践建议\n\n使用稳健的估计方法\n对协方差矩阵进行正则化\n添加约束条件（行业、流动性等）\n定期重新估计参数\n\n\n6. 三种方法对比\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n维度Top-K 等权IC 权重MV 优化复杂度低中高数据需求低中高理论支撑经验统计理论计算速度快中慢换手率中高中风险控制弱中强适合新手✅⚠️❌\n\n7. 实践建议\n7.1 策略选择原则\n新手建议：\n\n从 Top-K 策权开始\n使用较小的 K 值（20-30）\n定期调仓（周频或月频）\n严格风险控制\n\n进阶建议：\n\n尝试 IC 权重策略\n学习 MV 优化方法\n进行参数敏感性分析\n做样本外验证\n\n7.2 风险控制建议\n\n\n分散化\n\n持有多只股票（至少 10 只）\n考虑行业分散\n避免过度集中\n\n\n\n仓位控制\n\n限制单股权重（&lt; 20%）\n限制行业权重（&lt; 30%）\n设置止损机制\n\n\n\n成本控制\n\n控制换手率（&lt; 300%）\n使用低佣金券商\n选择高流动性股票\n\n\n\n7.3 参数优化建议\n\n\n避免过拟合\n\n使用样本外验证\n限制参数数量\n参数有经济学含义\n\n\n\n参数稳定性\n\n在不同时间段表现稳定\n对数据变化不过敏\n定期重新评估参数\n\n\n\n\n总结\n交易策略的核心是：\n\n信号生成：选择股票\n仓位管理：分配权重\n风险控制：限制风险\n执行规则：何时调仓\n\n三种主要方法：\n\nTop-K 等权：简单可靠，适合新手\nIC 权重：动态调整，适合有历史数据\nMV 优化：科学配置，适合追求风险调整后收益\n\n建议从简单的 Top-K 策略开始，逐步学习更复杂的方法。"},"quant/qlib/week3/02-投资组合构建方法":{"slug":"quant/qlib/week3/02-投资组合构建方法","filePath":"quant/qlib/week3/02-投资组合构建方法.md","title":"02-投资组合构建方法","links":[],"tags":[],"content":"投资组合构建方法\n1. Top-K 均等权重\n1.1 算法步骤\ndef top_k_equal_weight(predictions, k=20):\n    &quot;&quot;&quot;\n    Top-K 均等权重策略\n    \n    参数:\n        predictions: 股票预测分数\n        k: 选择股票数量\n    \n    返回:\n        weights: 股票权重，权重和为1\n    &quot;&quot;&quot;\n    # 1. 按预测分数排序\n    sorted_stocks = predictions.sort_values(ascending=False)\n    \n    # 2. 选择 Top-K\n    top_k = sorted_stocks[:k]\n    \n    # 3. 计算等权重\n    weight = 1.0 / k\n    \n    # 4. 分配权重\n    weights = pd.Series(0, index=predictions.index)\n    weights[top_k.index] = weight\n    \n    return weights\n \n# 示例\npredictions = pd.Series({\n    &#039;600000&#039;: 0.025,\n    &#039;600001&#039;: 0.023,\n    &#039;600002&#039;: 0.010,\n    &#039;600003&#039;: 0.009,\n    &#039;600004&#039;: -0.015,\n    &#039;600005&#039;: -0.020,\n})\n \nweights = top_k_equal_weight(predictions, k=3)\nprint(&quot;Top-K 权重分配:&quot;)\nprint(weights)\n输出：\nTop-K 权重分配:\n600000    0.333333\n600001    0.333333\n600002    0.333333\n600003    0.000000\n600004    0.000000\n600005    0.000000\ndtype: float64\n\n1.2 关键参数\nk（选择股票数量）\n\n推荐范围：20-50\n小 k（10）：集中度高，风险大，潜在收益高\n大 k（50）：分散度高，风险低，收益可能降低\nk = 全市场：等权重市场组合，接近指数投资\n\n调仓频率\n\n日频：每天调仓，成本高，反应及时\n周频：每周调仓，平衡成本和及时性\n月频：每月调仓，成本低，反应滞后\n\n权重方式\n\n等权重：简单，适合新手\n按分数加权：考虑预测差异，但增加复杂度\n\n1.3 适用场景\n\n✅ 预测质量高且稳定\n✅ 追求简单策略\n✅ 流动性要求高\n✅ 交易成本敏感\n✅ 新手入门\n\n1.4 参数敏感性分析\ndef analyze_k_sensitivity(predictions, k_values=[10, 20, 30, 40, 50]):\n    &quot;&quot;&quot;\n    分析 K 值敏感性\n    \n    参数:\n        predictions: 股票预测分数\n        k_values: K 值列表\n    \n    返回:\n        results: 不同 K 值的结果\n    &quot;&quot;&quot;\n    results = []\n    \n    for k in k_values:\n        weights = top_k_equal_weight(predictions, k=k)\n        \n        # 计算指标\n        n_selected = (weights &gt; 0).sum()\n        avg_weight = weights[weights &gt; 0].mean()\n        max_weight = weights.max()\n        \n        results.append({\n            &#039;k&#039;: k,\n            &#039;n_selected&#039;: n_selected,\n            &#039;avg_weight&#039;: avg_weight,\n            &#039;max_weight&#039;: max_weight\n        })\n    \n    return pd.DataFrame(results)\n \n# 示例\nsensitivity_results = analyze_k_sensitivity(predictions)\nprint(&quot;K 值敏感性分析:&quot;)\nprint(sensitivity_results)\n\n2. IC 权重分配\n2.1 算法步骤\ndef ic_weight_strategy(predictions, ic_history, window=20, min_ic=0.02):\n    &quot;&quot;&quot;\n    IC 权重策略\n    \n    参数:\n        predictions: 股票预测分数\n        ic_history: IC 历史数据\n        window: IC 计算窗口\n        min_ic: 最小 IC 阈值\n    \n    返回:\n        weights: 股票权重，权重和为1\n    &quot;&quot;&quot;\n    # 1. 计算滚动 IC\n    rolling_ic = ic_history.rolling(window=window).mean()\n    \n    # 2. 只选择 IC &gt; 0 的股票\n    valid_stocks = rolling_ic[rolling_ic &gt; min_ic]\n    \n    # 3. 根据 IC 分配权重\n    if len(valid_stocks) == 0:\n        # 如果没有符合条件的股票，使用等权重\n        weights = pd.Series(1.0 / len(predictions), index=predictions.index)\n    else:\n        weights = valid_stocks / valid_stocks.sum()\n    \n    # 4. 填充未选中的股票\n    final_weights = pd.Series(0, index=predictions.index)\n    final_weights[weights.index] = weights\n    \n    return final_weights\n \n# 示例\nic_history = pd.DataFrame({\n    &#039;600000&#039;: [0.03, 0.04, 0.05, 0.04, 0.06],\n    &#039;600001&#039;: [0.02, 0.03, 0.02, 0.03, 0.02],\n    &#039;600002&#039;: [0.01, 0.01, 0.02, 0.01, 0.01],\n    &#039;600003&#039;: [0.00, 0.01, 0.00, 0.01, 0.00],\n    &#039;600004&#039;: [-0.01, -0.02, -0.01, -0.02, -0.01],\n})\n \nweights = ic_weight_strategy(predictions, ic_history, window=5, min_ic=0.02)\nprint(&quot;IC 权重分配:&quot;)\nprint(weights)\n2.2 关键参数\nwindow（IC 计算窗口）\n\n推荐范围：20-60\n窗口小：反应快，但不稳定\n窗口大：反应慢，但更稳定\n\nmin_ic（最小 IC 阈值）\n\n推荐范围：0.02-0.05\n阈值高：选股更严格，可能仓位不足\n阈值低：选股宽松，可能包含低质量股票\n\nsmooth_factor（IC 平滑系数）\n\n推荐范围：0.5-2.0\n用于平滑 IC 变动\n\nweight_cap（最大单股权重）\n\n推荐范围：0.05-0.2\n限制单股权重，控制风险\n\n2.3 适用场景\n\n✅ 有足够历史数据\n✅ 模型预测能力差异明显\n✅ 追求超额收益\n✅ 可以承受较高换手率\n\n2.4 权重平滑处理\ndef ic_weight_smooth(predictions, ic_history, window=20, alpha=1.0):\n    &quot;&quot;&quot;\n    平滑 IC 权重（Softmax）\n    \n    参数:\n        predictions: 股票预测分数\n        ic_history: IC 历史数据\n        window: IC 计算窗口\n        alpha: Softmax 系数\n    \n    返回:\n        weights: 股票权重，权重和为1\n    &quot;&quot;&quot;\n    # 1. 计算滚动 IC\n    rolling_ic = ic_history.rolling(window=window).mean()\n    \n    # 2. Softmax 归一化\n    exp_ic = np.exp(alpha * rolling_ic)\n    weights = exp_ic / exp_ic.sum()\n    \n    return weights\n \n# 示例\nweights_smooth = ic_weight_smooth(predictions, ic_history, window=5, alpha=1.0)\nprint(&quot;平滑 IC 权重:&quot;)\nprint(weights_smooth)\n\n3. 均值-方差优化\n3.1 算法步骤\nimport cvxpy as cp\n \ndef mv_optimization(returns, risk_aversion=1.0, max_weight=0.1, min_weight=0.0):\n    &quot;&quot;&quot;\n    均值-方差优化\n    \n    参数:\n        returns: 历史收益率\n        risk_aversion: 风险厌恶系数\n        max_weight: 最大单股权重\n        min_weight: 最小单股权重\n    \n    返回:\n        weights: 最优权重\n    &quot;&quot;&quot;\n    n = len(returns.columns)\n    \n    # 1. 估计期望收益\n    mu = returns.mean().values\n    \n    # 2. 估计协方差矩阵\n    sigma = returns.cov().values\n    \n    # 3. 正则化（防止过拟合）\n    sigma_reg = sigma + 1e-6 * np.eye(n)\n    \n    # 4. 定义变量\n    w = cp.Variable(n)\n    \n    # 5. 定义目标函数\n    objective = cp.Maximize(mu @ w - risk_aversion * cp.quad_form(w, sigma_reg))\n    \n    # 6. 定义约束\n    constraints = [\n        cp.sum(w) == 1,        # 权重和为1\n        w &gt;= min_weight,        # 最小权重\n        w &lt;= max_weight         # 最大权重\n    ]\n    \n    # 7. 求解\n    problem = cp.Problem(objective, constraints)\n    problem.solve()\n    \n    # 8. 获取解\n    optimal_weights = pd.Series(w.value, index=returns.columns)\n    \n    return optimal_weights\n \n# 示例\nimport numpy as np\n \n# 生成示例数据\nnp.random.seed(42)\nn_stocks = 10\nn_days = 252\nstocks = [f&#039;stock_{i}&#039; for i in range(n_stocks)]\ndates = pd.date_range(&#039;2020-01-01&#039;, periods=n_days, freq=&#039;D&#039;)\n \nreturns = pd.DataFrame(\n    np.random.randn(n_days, n_stocks) * 0.02,\n    index=dates,\n    columns=stocks\n)\n \n# MV 优化\nweights_mv = mv_optimization(\n    returns, \n    risk_aversion=1.0,\n    max_weight=0.2,\n    min_weight=0.0\n)\n \nprint(&quot;MV 优化权重:&quot;)\nprint(weights_mv)\n3.2 关键参数\nrisk_aversion（风险厌恶系数）\n\n推荐范围：0.1-10.0\n小值：更注重收益，风险大\n大值：更注重风险，收益可能低\n\nmax_weight（最大单股权重）\n\n推荐范围：0.05-0.2\n限制单股权重，控制风险\n\nlambda_reg（正则化系数）\n\n推荐范围：1e-6-1e-3\n正则化协方差矩阵，防止过拟合\n\nestimation_window（参数估计窗口）\n\n推荐范围：126-504（半年到两年）\n窗口越大，估计越稳定，但可能滞后\n\n3.3 适用场景\n\n✅ 追求风险调整后收益\n✅ 有良好的数据质量\n✅ 需要科学的资产配置\n✅ 理解量化投资理论\n❌ 不适合新手\n\n3.4 风险厌恶系数优化\ndef optimize_risk_aversion(returns, ra_values=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]):\n    &quot;&quot;&quot;\n    优化风险厌恶系数\n    \n    参数:\n        returns: 历史收益率\n        ra_values: 风险厌恶系数候选值\n    \n    返回:\n        results: 不同风险厌恶系数的结果\n    &quot;&quot;&quot;\n    results = []\n    \n    for ra in ra_values:\n        # 计算最优权重\n        weights = mv_optimization(returns, risk_aversion=ra, max_weight=0.2)\n        \n        # 计算组合收益和风险\n        portfolio_return = (returns * weights).sum(axis=1).mean()\n        portfolio_std = (returns * weights).sum(axis=1).std()\n        sharpe = portfolio_return / portfolio_std\n        \n        results.append({\n            &#039;risk_aversion&#039;: ra,\n            &#039;return&#039;: portfolio_return,\n            &#039;std&#039;: portfolio_std,\n            &#039;sharpe&#039;: sharpe,\n            &#039;n_stocks&#039;: (weights &gt; 0.01).sum()\n        })\n    \n    return pd.DataFrame(results)\n \n# 示例\nra_results = optimize_risk_aversion(returns)\nprint(&quot;风险厌恶系数优化:&quot;)\nprint(ra_results)\n\n4. 三种方法对比\n4.1 性能对比\ndef compare_strategies(predictions, returns, k=20, risk_aversion=1.0):\n    &quot;&quot;&quot;\n    对比三种策略\n    \n    参数:\n        predictions: 股票预测分数\n        returns: 历史收益率\n        k: Top-K 的 K 值\n        risk_aversion: 风险厌恶系数\n    \n    返回:\n        comparison: 对比结果\n    &quot;&quot;&quot;\n    # 1. Top-K 等权\n    weights_topk = top_k_equal_weight(predictions, k=k)\n    return_topk = (returns * weights_topk).sum(axis=1)\n    sharpe_topk = return_topk.mean() / return_topk.std()\n    \n    # 2. MV 优化\n    weights_mv = mv_optimization(returns, risk_aversion=risk_aversion)\n    return_mv = (returns * weights_mv).sum(axis=1)\n    sharpe_mv = return_mv.mean() / return_mv.std()\n    \n    # 3. 对比结果\n    comparison = pd.DataFrame({\n        &#039;Strategy&#039;: [&#039;Top-K&#039;, &#039;MV&#039;],\n        &#039;Mean Return&#039;: [return_topk.mean(), return_mv.mean()],\n        &#039;Std&#039;: [return_topk.std(), return_mv.std()],\n        &#039;Sharpe&#039;: [sharpe_topk, sharpe_mv],\n        &#039;N Stocks&#039;: [(weights_topk &gt; 0).sum(), (weights_mv &gt; 0.01).sum()],\n        &#039;Max Weight&#039;: [weights_topk.max(), weights_mv.max()]\n    })\n    \n    return comparison\n \n# 示例\ncomparison = compare_strategies(predictions, returns, k=3, risk_aversion=1.0)\nprint(&quot;策略对比:&quot;)\nprint(comparison)\n4.2 综合对比表\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n维度Top-K 等权IC 权重MV 优化复杂度低中高数据需求低中高理论支撑经验统计理论计算速度快中慢换手率中高中风险控制弱中强适合新手✅⚠️❌适合实战✅✅✅\n\n5. 实践建议\n5.1 策略选择\n新手建议：\n\n从 Top-K 等权开始\n使用较小的 K 值（20-30）\n定期调仓（周频或月频）\n严格风险控制\n\n进阶建议：\n\n尝试 IC 权重策略\n学习 MV 优化方法\n进行参数敏感性分析\n做样本外验证\n\n专家建议：\n\n结合多种方法\n动态调整策略\n考虑市场环境\n持续优化改进\n\n5.2 参数优化建议\n1. 避免过拟合\n\n使用样本外验证\n限制参数数量\n参数有经济学含义\n\n2. 参数稳定性\n\n在不同时间段表现稳定\n对数据变化不过敏\n定期重新评估参数\n\n3. 实践经验\n# ❌ 错误做法\n# 过度优化，参数过多\nparams = {\n    &#039;k&#039;: 23,\n    &#039;lookback&#039;: 17,\n    &#039;threshold&#039;: 0.032,\n    &#039;smoothing&#039;: 0.7,\n    # ... 太多参数\n}\n \n# ✅ 正确做法\n# 少量关键参数\nparams = {\n    &#039;k&#039;: 20,  # 清晰的含义\n    &#039;lookback&#039;: 20  # 基于经济逻辑\n}\n5.3 风险控制建议\n1. 分散化\n\n持有多只股票（至少 10 只）\n考虑行业分散\n避免过度集中\n\n2. 仓位控制\ndef position_sizing(weights, max_position=0.1):\n    &quot;&quot;&quot;\n    仓位控制\n    \n    参数:\n        weights: 原始权重\n        max_position: 最大单股权重\n    \n    返回:\n        controlled_weights: 控制后的权重\n    &quot;&quot;&quot;\n    # 限制单股权重\n    controlled_weights = weights.clip(upper=max_position)\n    \n    # 重新归一化\n    controlled_weights = controlled_weights / controlled_weights.sum()\n    \n    return controlled_weights\n3. 行业中性化\ndef industry_neutralize(weights, industry_weights, target=0.0):\n    &quot;&quot;&quot;\n    行业中性化\n    \n    参数:\n        weights: 股票权重\n        industry_weights: 行业权重\n        target: 目标行业权重\n    \n    返回:\n        neutralized_weights: 中性化后的权重\n    &quot;&quot;&quot;\n    # 计算行业偏离\n    industry_exposure = (weights * industry_weights).sum()\n    \n    # 调整权重\n    neutralized_weights = weights * (target / industry_exposure)\n    \n    return neutralized_weights\n\n总结\n三种投资组合构建方法各有优劣：\n\nTop-K 等权：简单可靠，适合新手\nIC 权重：动态调整，适合有历史数据\nMV 优化：科学配置，适合追求风险调整后收益\n\n建议：\n\n新手从 Top-K 等权开始\n进阶后尝试 IC 权重\n专家可以使用 MV 优化\n结合多种方法，取长补短\n"},"quant/qlib/week3/03-Executor与成本模型":{"slug":"quant/qlib/week3/03-Executor与成本模型","filePath":"quant/qlib/week3/03-Executor与成本模型.md","title":"03-Executor与成本模型","links":[],"tags":[],"content":"Executor 与成本模型\n1. Executor 机制\n1.1 Executor 定义\nExecutor：负责执行交易信号的组件，模拟真实交易环境\nclass Executor:\n    &quot;&quot;&quot;\n    交易执行器\n    \n    功能：\n    1. 接收交易信号\n    2. 执行交易\n    3. 计算成本\n    4. 更新持仓\n    5. 记录绩效\n    &quot;&quot;&quot;\n    \n    def __init__(self, config):\n        self.account = config[&#039;initial_account&#039;]\n        self.positions = {}\n        self.cash = self.account\n        self.trade_history = []\n        \n    def execute(self, signals, prices):\n        &quot;&quot;&quot;\n        执行交易信号\n        \n        参数:\n            signals: 交易信号\n            prices: 价格数据\n        \n        返回:\n            trades: 执行的交易\n        &quot;&quot;&quot;\n        trades = []\n        \n        for stock, target_weight in signals.items():\n            if target_weight == 0:\n                # 卖出\n                trade = self.sell(stock, prices[stock])\n            else:\n                # 买入\n                trade = self.buy(stock, target_weight, prices[stock])\n            \n            if trade:\n                trades.append(trade)\n        \n        return trades\n    \n    def buy(self, stock, target_weight, price):\n        &quot;&quot;&quot;\n        买入股票\n        \n        参数:\n            stock: 股票代码\n            target_weight: 目标权重\n            price: 价格\n        \n        返回:\n            trade: 交易记录\n        &quot;&quot;&quot;\n        # 计算目标持仓价值\n        target_value = self.account * target_weight\n        \n        # 计算需要买入的股数\n        shares = int(target_value / price)\n        \n        if shares &lt;= 0:\n            return None\n        \n        # 计算交易金额\n        trade_value = shares * price\n        \n        # 检查现金是否足够\n        if trade_value &gt; self.cash:\n            shares = int(self.cash / price)\n            trade_value = shares * price\n        \n        # 更新持仓和现金\n        if stock not in self.positions:\n            self.positions[stock] = 0\n        \n        self.positions[stock] += shares\n        self.cash -= trade_value\n        \n        # 记录交易\n        trade = {\n            &#039;stock&#039;: stock,\n            &#039;action&#039;: &#039;buy&#039;,\n            &#039;shares&#039;: shares,\n            &#039;price&#039;: price,\n            &#039;value&#039;: trade_value\n        }\n        self.trade_history.append(trade)\n        \n        return trade\n    \n    def sell(self, stock, price):\n        &quot;&quot;&quot;\n        卖出股票\n        \n        参数:\n            stock: 股票代码\n            price: 价格\n        \n        返回:\n            trade: 交易记录\n        &quot;&quot;&quot;\n        # 检查持仓\n        if stock not in self.positions or self.positions[stock] &lt;= 0:\n            return None\n        \n        # 全部卖出\n        shares = self.positions[stock]\n        trade_value = shares * price\n        \n        # 更新持仓和现金\n        self.positions[stock] = 0\n        self.cash += trade_value\n        \n        # 记录交易\n        trade = {\n            &#039;stock&#039;: stock,\n            &#039;action&#039;: &#039;sell&#039;,\n            &#039;shares&#039;: shares,\n            &#039;price&#039;: price,\n            &#039;value&#039;: trade_value\n        }\n        self.trade_history.append(trade)\n        \n        return trade\n1.2 Qlib Executor 架构\nExecutor {\n    execute(signals) → trades           # 执行交易信号\n    apply_costs(trades) → adj_returns  # 应用交易成本\n    generate_portfolio_metrics() → metrics  # 生成组合指标\n}\n1.3 Executor 功能\n1. 信号执行\n\n接收交易信号\n模拟订单提交\n计算成交价格\n\n2. 成本应用\n\n计算手续费\n计算滑点成本\n计算市场冲击\n\n3. 持仓管理\n\n更新当前持仓\n计算持仓价值\n管理现金余额\n\n4. 绩效记录\n\n记录每笔交易\n计算实时绩效\n生成绩效报告\n\n\n2. 交易成本模型\n2.1 成本构成\n1. 手续费\n\n按交易金额的固定比例收取\n买入费率和卖出费率可能不同\n通常有最低费用限制\n\n2. 滑点\n\n实际成交价格与理想价格的差异\n与交易规模和流动性相关\n通常按价格百分比计算\n\n3. 市场冲击\n\n大额交易对价格的冲击\n与交易金额和市场流动性相关\n通常按交易量的平方根计算\n\n2.2 手续费计算\n计算公式：\n手续费 = 交易金额 × 费率\n手续费 = max(手续费, 最低费用)\n\nPython 实现：\ndef calculate_commission(trade_value, commission_rate, min_commission=5):\n    &quot;&quot;&quot;\n    计算手续费\n    \n    参数:\n        trade_value: 交易金额\n        commission_rate: 手续费率\n        min_commission: 最低手续费\n    \n    返回:\n        commission: 手续费\n    &quot;&quot;&quot;\n    commission = trade_value * commission_rate\n    commission = max(commission, min_commission)\n    \n    return commission\n \n# 示例\ntrade_value = 100000  # 交易10万元\ncommission_rate = 0.0003  # 万分之三\nmin_commission = 5  # 最低5元\n \ncommission = calculate_commission(trade_value, commission_rate, min_commission)\nprint(f&quot;手续费 = {commission:.2f} 元&quot;)\n2.3 滑点计算\n计算公式：\n滑点成本 = 交易金额 × 滑点率\n成交价格 = 理想价格 × (1 ± 滑点率)\n\nPython 实现：\ndef calculate_slippage(trade_value, slippage_rate, is_buy=True):\n    &quot;&quot;&quot;\n    计算滑点成本\n    \n    参数:\n        trade_value: 交易金额\n        slippage_rate: 滑点率\n        is_buy: 是否买入\n    \n    返回:\n        slippage_cost: 滑点成本\n        execution_price: 执行价格\n    &quot;&quot;&quot;\n    # 买入时滑点增加成本，卖出时滑点减少收益\n    if is_buy:\n        slippage_cost = trade_value * slippage_rate\n        execution_price = 1 + slippage_rate\n    else:\n        slippage_cost = trade_value * slippage_rate\n        execution_price = 1 - slippage_rate\n    \n    return slippage_cost, execution_price\n \n# 示例\ntrade_value = 100000  # 交易10万元\nslippage_rate = 0.001  # 千分之一\n \n# 买入\nslippage_buy, price_buy = calculate_slippage(trade_value, slippage_rate, is_buy=True)\nprint(f&quot;买入滑点 = {slippage_buy:.2f} 元&quot;)\n \n# 卖出\nslippage_sell, price_sell = calculate_slippage(trade_value, slippage_rate, is_buy=False)\nprint(f&quot;卖出滑点 = {slippage_sell:.2f} 元&quot;)\n2.4 市场冲击计算\n计算公式：\n市场冲击 = k × sqrt(交易量 / 日均成交量)\n\nPython 实现：\ndef calculate_market_impact(trade_value, avg_daily_value, impact_factor=0.1):\n    &quot;&quot;&quot;\n    计算市场冲击\n    \n    参数:\n        trade_value: 交易金额\n        avg_daily_value: 日均交易金额\n        impact_factor: 冲击系数\n    \n    返回:\n        market_impact: 市场冲击成本\n    &quot;&quot;&quot;\n    if avg_daily_value == 0:\n        return 0\n    \n    # 计算交易比例\n    trade_ratio = trade_value / avg_daily_value\n    \n    # 计算市场冲击\n    market_impact = trade_value * impact_factor * np.sqrt(trade_ratio)\n    \n    return market_impact\n \n# 示例\ntrade_value = 1000000  # 交易100万元\navg_daily_value = 50000000  # 日均交易5000万元\n \nmarket_impact = calculate_market_impact(trade_value, avg_daily_value)\nprint(f&quot;市场冲击 = {market_impact:.2f} 元&quot;)\n2.5 总成本模型\ndef calculate_total_cost(trade_value, commission_rate, slippage_rate, \n                           min_commission=5, avg_daily_value=None, impact_factor=0.1):\n    &quot;&quot;&quot;\n    计算总成本\n    \n    参数:\n        trade_value: 交易金额\n        commission_rate: 手续费率\n        slippage_rate: 滑点率\n        min_commission: 最低手续费\n        avg_daily_value: 日均交易金额\n        impact_factor: 冲击系数\n    \n    返回:\n        total_cost: 总成本\n        cost_breakdown: 成本明细\n    &quot;&quot;&quot;\n    # 1. 手续费\n    commission = calculate_commission(trade_value, commission_rate, min_commission)\n    \n    # 2. 滑点\n    slippage = trade_value * slippage_rate\n    \n    # 3. 市场冲击\n    if avg_daily_value and impact_factor:\n        market_impact = calculate_market_impact(trade_value, avg_daily_value, impact_factor)\n    else:\n        market_impact = 0\n    \n    # 4. 总成本\n    total_cost = commission + slippage + market_impact\n    \n    # 5. 成本明细\n    cost_breakdown = {\n        &#039;commission&#039;: commission,\n        &#039;slippage&#039;: slippage,\n        &#039;market_impact&#039;: market_impact,\n        &#039;total&#039;: total_cost\n    }\n    \n    return total_cost, cost_breakdown\n \n# 示例\ntrade_value = 100000  # 交易10万元\ncommission_rate = 0.0003  # 万分之三\nslippage_rate = 0.001  # 千分之一\navg_daily_value = 5000000  # 日均交易500万元\nimpact_factor = 0.1\n \ntotal_cost, breakdown = calculate_total_cost(\n    trade_value, commission_rate, slippage_rate,\n    min_commission=5,\n    avg_daily_value=avg_daily_value,\n    impact_factor=impact_factor\n)\n \nprint(&quot;成本明细:&quot;)\nfor key, value in breakdown.items():\n    print(f&quot;  {key}: {value:.2f} 元&quot;)\nprint(f&quot;  总成本: {total_cost:.2f} 元&quot;)\nprint(f&quot;  成本率: {total_cost / trade_value:.4f}&quot;)\n\n3. 成本敏感性分析\n3.1 成本参数范围\n手续费率：\n\n低：0.0001（万分之一）\n中：0.001（千分之一）\n高：0.003（千分之三）\n\n滑点率：\n\n低：0.0005（万分之一）\n中：0.001（千分之一）\n高：0.005（千分之五）\n\n3.2 成本影响分析\n对收益率的影响：\n净值收益 = 原始收益 - 总成本\n\n对换手率的影响：\n换手成本 = 换手率 × 平均资产 × 成本率\n\n对策略选择的影响：\n\n高换手率策略对成本更敏感\n低成本环境适合高频策略\n高成本环境适合低换手率策略\n\n3.3 成本敏感性分析\ndef cost_sensitivity_analysis(trade_value, commission_rates, slippage_rates):\n    &quot;&quot;&quot;\n    成本敏感性分析\n    \n    参数:\n        trade_value: 交易金额\n        commission_rates: 手续费率列表\n        slippage_rates: 滑点率列表\n    \n    返回:\n        sensitivity: 敏感性分析结果\n    &quot;&quot;&quot;\n    sensitivity = []\n    \n    for comm_rate in commission_rates:\n        for slip_rate in slippage_rates:\n            total_cost, _ = calculate_total_cost(\n                trade_value, comm_rate, slip_rate\n            )\n            \n            sensitivity.append({\n                &#039;commission_rate&#039;: comm_rate,\n                &#039;slippage_rate&#039;: slip_rate,\n                &#039;total_cost&#039;: total_cost,\n                &#039;cost_rate&#039;: total_cost / trade_value\n            })\n    \n    return pd.DataFrame(sensitivity)\n \n# 示例\ncommission_rates = [0.0001, 0.0003, 0.001, 0.003]\nslippage_rates = [0.0005, 0.001, 0.002, 0.005]\n \nsensitivity_df = cost_sensitivity_analysis(100000, commission_rates, slippage_rates)\n \nprint(&quot;成本敏感性分析:&quot;)\nprint(sensitivity_df)\n3.4 换手率与成本的关系\ndef turnover_cost_analysis(avg_value, turnover_rates, cost_rate=0.003):\n    &quot;&quot;&quot;\n    换手率与成本关系分析\n    \n    参数:\n        avg_value: 平均资产\n        turnover_rates: 换手率列表\n        cost_rate: 成本率\n    \n    返回:\n        analysis: 分析结果\n    &quot;&quot;&quot;\n    analysis = []\n    \n    for turnover in turnover_rates:\n        # 年化换手成本\n        annual_cost = avg_value * turnover * cost_rate\n        \n        analysis.append({\n            &#039;turnover&#039;: turnover,\n            &#039;annual_cost&#039;: annual_cost,\n            &#039;cost_ratio&#039;: annual_cost / avg_value\n        })\n    \n    return pd.DataFrame(analysis)\n \n# 示例\navg_value = 1000000  # 平均资产100万\nturnover_rates = [0.5, 1.0, 2.0, 3.0, 5.0, 10.0]  # 换手率\n \ncost_df = turnover_cost_analysis(avg_value, turnover_rates, cost_rate=0.003)\n \nprint(&quot;换手率与成本关系:&quot;)\nprint(cost_df)\n\n4. 实践建议\n4.1 成本控制策略\n1. 降低换手率\n\n优化调仓频率\n增加持仓周期\n使用止损止盈\n\n2. 降低交易成本\n\n选择低佣金券商\n优化下单时间\n分批建仓\n\n3. 选择流动性好的股票\n\n避免小市值股票\n关注日均成交量\n避免停牌股票\n\n4.2 成本参数配置\nQlib 配置示例：\nexecutor_config = {\n    &#039;class&#039;: &#039;SimulatorExecutor&#039;,\n    &#039;module_path&#039;: &#039;qlib.backtest.executor&#039;,\n    &#039;kwargs&#039;: {\n        &#039;time_per_step&#039;: &#039;day&#039;,\n        &#039;generate_portfolio_metrics&#039;: True\n    }\n}\n \nexchange_config = {\n    &#039;freq&#039;: &#039;day&#039;,\n    &#039;limit_threshold&#039;: 0.095,\n    &#039;deal_price&#039;: &#039;close&#039;,\n    &#039;open_cost&#039;: 0.0005,   # 买入费率万分之五\n    &#039;close_cost&#039;: 0.0015,  # 卖出费率千分之1.5\n    &#039;min_cost&#039;: 5           # 最低手续费5元\n}\n4.3 成本敏感性测试\n测试流程：\n\n使用低成本参数回测\n逐步增加成本参数\n观察策略表现变化\n确定策略的成本敏感性\n\nPython 实现：\ndef test_cost_sensitivity(strategy, returns, base_config, \n                         cost_multipliers=[1.0, 1.5, 2.0, 3.0]):\n    &quot;&quot;&quot;\n    成本敏感性测试\n    \n    参数:\n        strategy: 交易策略\n        returns: 收益率数据\n        base_config: 基础配置\n        cost_multipliers: 成本倍数列表\n    \n    返回:\n        results: 测试结果\n    &quot;&quot;&quot;\n    results = []\n    \n    for multiplier in cost_multipliers:\n        # 调整成本参数\n        config = base_config.copy()\n        config[&#039;open_cost&#039;] *= multiplier\n        config[&#039;close_cost&#039;] *= multiplier\n        \n        # 回测\n        performance = backtest(strategy, returns, config)\n        \n        results.append({\n            &#039;cost_multiplier&#039;: multiplier,\n            &#039;return&#039;: performance[&#039;return&#039;],\n            &#039;sharpe&#039;: performance[&#039;sharpe&#039;],\n            &#039;max_drawdown&#039;: performance[&#039;max_drawdown&#039;]\n        })\n    \n    return pd.DataFrame(results)\n \n# 示例\nbase_config = {\n    &#039;open_cost&#039;: 0.0005,\n    &#039;close_cost&#039;: 0.0015,\n    &#039;min_cost&#039;: 5\n}\n \nresults = test_cost_sensitivity(strategy, returns, base_config)\nprint(&quot;成本敏感性测试:&quot;)\nprint(results)\n\n总结\n交易成本是量化投资中不可忽视的因素：\n\n成本构成：手续费、滑点、市场冲击\n成本计算：建立准确的成本模型\n成本敏感性：分析成本对策略的影响\n成本控制：优化策略降低成本\n\n建议：\n\n务必做成本敏感性分析\n选择合理的成本参数\n优化策略降低换手率\n选择流动性好的股票\n"},"quant/qlib/week3/04-绩效评估指标":{"slug":"quant/qlib/week3/04-绩效评估指标","filePath":"quant/qlib/week3/04-绩效评估指标.md","title":"04-绩效评估指标","links":[],"tags":[],"content":"绩效评估指标\n1. 收益率指标\n1.1 总收益率\ndef calculate_total_return(cumulative_returns):\n    &quot;&quot;&quot;\n    计算总收益率\n    \n    参数:\n        cumulative_returns: 累计收益率\n    \n    返回:\n        total_return: 总收益率\n    &quot;&quot;&quot;\n    total_return = cumulative_returns.iloc[-1]\n    return total_return\n1.2 年化收益率\ndef calculate_annual_return(returns, periods_per_year=252):\n    &quot;&quot;&quot;\n    计算年化收益率\n    \n    参数:\n        returns: 日收益率序列\n        periods_per_year: 每年交易天数\n    \n    返回:\n        annual_return: 年化收益率\n    &quot;&quot;&quot;\n    cumulative_return = (1 + returns).prod() - 1\n    n_periods = len(returns)\n    \n    annual_return = (1 + cumulative_return) ** (periods_per_year / n_periods) - 1\n    \n    return annual_return\n1.3 超额收益率\ndef calculate_excess_return(strategy_return, benchmark_return):\n    &quot;&quot;&quot;\n    计算超额收益率\n    \n    参数:\n        strategy_return: 策略收益率\n        benchmark_return: 基准收益率\n    \n    返回:\n        excess_return: 超额收益率\n    &quot;&quot;&quot;\n    excess_return = strategy_return - benchmark_return\n    return excess_return\n2. 风险指标\n2.1 波动率\ndef calculate_volatility(returns, periods_per_year=252):\n    &quot;&quot;&quot;\n    计算波动率\n    \n    参数:\n        returns: 日收益率序列\n        periods_per_year: 每年交易天数\n    \n    返回:\n        volatility: 年化波动率\n    &quot;&quot;&quot;\n    daily_vol = returns.std()\n    annual_vol = daily_vol * np.sqrt(periods_per_year)\n    \n    return annual_vol\n2.2 最大回撤\ndef calculate_max_drawdown(cumulative_returns):\n    &quot;&quot;&quot;\n    计算最大回撤\n    \n    参数:\n        cumulative_returns: 累计收益率\n    \n    返回:\n        max_drawdown: 最大回撤\n        drawdown_series: 回撤序列\n    &quot;&quot;&quot;\n    # 计算累计净值\n    cumulative_value = (1 + cumulative_returns).cumprod()\n    \n    # 计算历史最高点\n    running_max = cumulative_value.expanding().max()\n    \n    # 计算回撤\n    drawdown = (cumulative_value - running_max) / running_max\n    \n    # 最大回撤\n    max_drawdown = drawdown.min()\n    \n    return max_drawdown, drawdown\n2.3 VaR (在险价值)\ndef calculate_var(returns, confidence_level=0.95):\n    &quot;&quot;&quot;\n    计算VaR (在险价值）\n    \n    参数:\n        returns: 收益率序列\n        confidence_level: 置信水平\n    \n    返回:\n        var: VaR值\n    &quot;&quot;&quot;\n    var = np.percentile(returns, (1 - confidence_level) * 100)\n    return var\n3. 风险调整收益指标\n3.1 夏普比率\ndef calculate_sharpe_ratio(returns, risk_free_rate=0.03, periods_per_year=252):\n    &quot;&quot;&quot;\n    计算夏普比率\n    \n    参数:\n        returns: 日收益率序列\n        risk_free_rate: 无风险利率\n        periods_per_year: 每年交易天数\n    \n    返回:\n        sharpe_ratio: 夏普比率\n    &quot;&quot;&quot;\n    annual_return = calculate_annual_return(returns, periods_per_year)\n    annual_vol = calculate_volatility(returns, periods_per_year)\n    \n    sharpe_ratio = (annual_return - risk_free_rate) / annual_vol\n    \n    return sharpe_ratio\n3.2 信息比率\ndef calculate_information_ratio(strategy_returns, benchmark_returns, periods_per_year=252):\n    &quot;&quot;&quot;\n    计算信息比率\n    \n    参数:\n        strategy_returns: 策略收益率\n        benchmark_returns: 基准收益率\n        periods_per_year: 每年交易天数\n    \n    返回:\n        information_ratio: 信息比率\n    &quot;&quot;&quot;\n    excess_returns = strategy_returns - benchmark_returns\n    \n    annual_excess_return = calculate_annual_return(excess_returns, periods_per_year)\n    tracking_error = calculate_volatility(excess_returns, periods_per_year)\n    \n    information_ratio = annual_excess_return / tracking_error\n    \n    return information_ratio\n3.3 卡尔马比率\ndef calculate_calmar_ratio(returns, periods_per_year=252):\n    &quot;&quot;&quot;\n    计算卡尔马比率\n    \n    参数:\n        returns: 日收益率序列\n        periods_per_year: 每年交易天数\n    \n    返回:\n        calmar_ratio: 卡尔马比率\n    &quot;&quot;&quot;\n    cumulative_returns = returns.cumsum()\n    annual_return = calculate_annual_return(returns, periods_per_year)\n    max_drawdown, _ = calculate_max_drawdown(cumulative_returns)\n    \n    calmar_ratio = annual_return / abs(max_drawdown)\n    \n    return calmar_ratio\n4. 交易相关指标\n4.1 换手率\ndef calculate_turnover(portfolio_changes, avg_value):\n    &quot;&quot;&quot;\n    计算换手率\n    \n    参数:\n        portfolio_changes: 持仓变化（绝对值）\n        avg_value: 平均资产价值\n    \n    返回:\n        turnover: 换手率\n    &quot;&quot;&quot;\n    turnover = portfolio_changes.abs().sum() / avg_value\n    return turnover\n4.2 胜率\ndef calculate_win_rate(trades):\n    &quot;&quot;&quot;\n    计算胜率\n    \n    参数:\n        trades: 交易记录 (DataFrame with &#039;pnl&#039; column)\n    \n    返回:\n        win_rate: 胜率\n    &quot;&quot;&quot;\n    winning_trades = len(trades[trades[&#039;pnl&#039;] &gt; 0])\n    total_trades = len(trades)\n    \n    win_rate = winning_trades / total_trades if total_trades &gt; 0 else 0\n    \n    return win_rate\n4.3 盈亏比\ndef calculate_profit_loss_ratio(trades):\n    &quot;&quot;&quot;\n    计算盈亏比\n    \n    参数:\n        trades: 交易记录 (DataFrame with &#039;pnl&#039; column)\n    \n    返回:\n        profit_loss_ratio: 盈亏比\n    &quot;&quot;&quot;\n    winning_trades = trades[trades[&#039;pnl&#039;] &gt; 0][&#039;pnl&#039;]\n    losing_trades = trades[trades[&#039;pnl&#039;] &lt; 0][&#039;pnl&#039;]\n    \n    if len(losing_trades) == 0:\n        return float(&#039;inf&#039;)\n    \n    avg_profit = winning_trades.mean()\n    avg_loss = abs(losing_trades.mean())\n    \n    profit_loss_ratio = avg_profit / avg_loss\n    \n    return profit_loss_ratio\n5. 综合评估\ndef evaluate_portfolio(returns, benchmark_returns=None, risk_free_rate=0.03):\n    &quot;&quot;&quot;\n    综合评估投资组合\n    \n    参数:\n        returns: 策略收益率\n        benchmark_returns: 基准收益率\n        risk_free_rate: 无风险利率\n    \n    返回:\n        metrics: 评估指标字典\n    &quot;&quot;&quot;\n    cumulative_returns = returns.cumsum()\n    \n    metrics = {}\n    \n    # 收益率指标\n    metrics[&#039;total_return&#039;] = calculate_total_return(cumulative_returns)\n    metrics[&#039;annual_return&#039;] = calculate_annual_return(returns)\n    \n    if benchmark_returns is not None:\n        metrics[&#039;excess_return&#039;] = calculate_excess_return(\n            metrics[&#039;annual_return&#039;],\n            calculate_annual_return(benchmark_returns)\n        )\n    \n    # 风险指标\n    metrics[&#039;volatility&#039;] = calculate_volatility(returns)\n    metrics[&#039;max_drawdown&#039;], _ = calculate_max_drawdown(cumulative_returns)\n    metrics[&#039;var_95&#039;] = calculate_var(returns, 0.95)\n    \n    # 风险调整收益指标\n    metrics[&#039;sharpe_ratio&#039;] = calculate_sharpe_ratio(returns, risk_free_rate)\n    \n    if benchmark_returns is not None:\n        metrics[&#039;information_ratio&#039;] = calculate_information_ratio(\n            returns, benchmark_returns\n        )\n    \n    metrics[&#039;calmar_ratio&#039;] = calculate_calmar_ratio(returns)\n    \n    return metrics\n \n# 示例\nimport numpy as np\nimport pandas as pd\n \nnp.random.seed(42)\nreturns = pd.Series(np.random.randn(252) * 0.02)\n \nmetrics = evaluate_portfolio(returns)\n \nprint(&quot;投资组合评估指标:&quot;)\nfor key, value in metrics.items():\n    print(f&quot;  {key}: {value:.4f}&quot;)\n总结\n绩效评估是量化投资的重要环节：\n\n收益率指标：总收益、年化收益、超额收益\n风险指标：波动率、最大回撤、VaR\n风险调整收益指标：夏普比率、信息比率、卡尔马比率\n交易相关指标：换手率、胜率、盈亏比\n\n建议：\n\n综合使用多个指标评估策略\n关注风险调整后收益\n考虑交易成本影响\n进行样本外验证\n"},"quant/qlib/week3/05-实验分析方法":{"slug":"quant/qlib/week3/05-实验分析方法","filePath":"quant/qlib/week3/05-实验分析方法.md","title":"05-实验分析方法","links":[],"tags":[],"content":"实验分析方法\n1. 参数敏感性分析\n1.1 单参数敏感性\ndef single_parameter_sensitivity(strategy, param_name, param_values, \n                                   data, backtest_func):\n    &quot;&quot;&quot;\n    单参数敏感性分析\n    \n    参数:\n        strategy: 策略函数\n        param_name: 参数名\n        param_values: 参数值列表\n        data: 回测数据\n        backtest_func: 回测函数\n    \n    返回:\n        results: 分析结果\n    &quot;&quot;&quot;\n    results = []\n    \n    for value in param_values:\n        # 设置参数\n        params = {param_name: value}\n        \n        # 回测\n        performance = backtest_func(strategy, data, params)\n        \n        results.append({\n            param_name: value,\n            &#039;return&#039;: performance[&#039;return&#039;],\n            &#039;sharpe&#039;: performance[&#039;sharpe&#039;],\n            &#039;max_drawdown&#039;: performance[&#039;max_drawdown&#039;]\n        })\n    \n    return pd.DataFrame(results)\n \n# 示例：分析K值对Top-K策略的影响\nk_values = [10, 20, 30, 40, 50]\nresults = single_parameter_sensitivity(\n    topk_strategy, &#039;k&#039;, k_values, \n    returns_data, backtest\n)\n1.2 多参数敏感性\ndef multi_parameter_sensitivity(strategy, param_grid, data, backtest_func):\n    &quot;&quot;&quot;\n    多参数敏感性分析\n    \n    参数:\n        strategy: 策略函数\n        param_grid: 参数网格\n        data: 回测数据\n        backtest_func: 回测函数\n    \n    返回:\n        results: 分析结果\n    &quot;&quot;&quot;\n    from itertools import product\n    \n    # 生成所有参数组合\n    param_combinations = list(product(*param_grid.values()))\n    \n    results = []\n    \n    for combination in param_combinations:\n        # 设置参数\n        params = dict(zip(param_grid.keys(), combination))\n        \n        # 回测\n        performance = backtest_func(strategy, data, params)\n        \n        results.append({\n            **params,\n            &#039;sharpe&#039;: performance[&#039;sharpe&#039;],\n            &#039;return&#039;: performance[&#039;return&#039;]\n        })\n    \n    return pd.DataFrame(results)\n \n# 示例\nparam_grid = {\n    &#039;k&#039;: [20, 30, 40],\n    &#039;risk_aversion&#039;: [0.5, 1.0, 2.0],\n    &#039;max_weight&#039;: [0.05, 0.1, 0.15]\n}\n \nresults = multi_parameter_sensitivity(mv_strategy, param_grid, returns_data, backtest)\n1.3 敏感性热力图\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n \ndef plot_sensitivity_heatmap(results, pivot_col, pivot_row, value_col):\n    &quot;&quot;&quot;\n    绘制敏感性热力图\n    \n    参数:\n        results: 分析结果\n        pivot_col: 列参数\n        pivot_row: 行参数\n        value_col: 值参数\n    \n    返回:\n        fig: 图形对象\n    &quot;&quot;&quot;\n    # 创建透视表\n    pivot_table = results.pivot(\n        index=pivot_row,\n        columns=pivot_col,\n        values=value_col\n    )\n    \n    # 绘制热力图\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(pivot_table, annot=True, fmt=&#039;.3f&#039;, \n                cmap=&#039;RdYlGn&#039;, center=0, ax=ax)\n    \n    ax.set_title(f&#039;{value_col} Sensitivity&#039;)\n    plt.tight_layout()\n    \n    return fig\n \n# 示例\nfig = plot_sensitivity_heatmap(results, &#039;risk_aversion&#039;, &#039;k&#039;, &#039;sharpe&#039;)\nplt.show()\n2. 持仓周期研究\n2.1 调仓频率类型\n日频调仓：每天调整持仓\n\n优点：及时反映市场变化\n缺点：交易成本高\n\n周频调仓：每周调整持仓\n\n平衡成本和及时性\n常用频率\n\n月频调仓：每月调整持仓\n\n优点：成本低\n缺点：反应滞后\n\n2.2 持仓周期分析\ndef holding_period_study(predictions, holding_periods=[1, 5, 20]):\n    &quot;&quot;&quot;\n    持仓周期研究\n    \n    参数:\n        predictions: 预测分数\n        holding_periods: 持仓周期列表（天数）\n    \n    返回:\n        results: 分析结果\n    &quot;&quot;&quot;\n    results = []\n    \n    for period in holding_periods:\n        # 生成持仓信号（每period天调仓一次）\n        signals = generate_signals(predictions, rebalance_freq=period)\n        \n        # 回测\n        performance = backtest(signals)\n        \n        results.append({\n            &#039;holding_period&#039;: period,\n            &#039;return&#039;: performance[&#039;return&#039;],\n            &#039;sharpe&#039;: performance[&#039;sharpe&#039;],\n            &#039;turnover&#039;: performance[&#039;turnover&#039;]\n        })\n    \n    return pd.DataFrame(results)\n \n# 示例\nholding_periods = [1, 5, 10, 20]  # 1天、1周、2周、1月\nresults = holding_period_study(predictions, holding_periods)\n3. 样本外验证\n3.1 滚动窗口验证\ndef rolling_window_backtest(data, strategy_func, \n                               window_size=252, test_size=63):\n    &quot;&quot;&quot;\n    滚动窗口验证\n    \n    参数:\n        data: 数据\n        strategy_func: 策略函数\n        window_size: 训练窗口大小\n        test_size: 测试窗口大小\n    \n    返回:\n        results: 验证结果\n    &quot;&quot;&quot;\n    results = []\n    \n    for i in range(window_size, len(data), test_size):\n        # 训练集\n        train_data = data[i-window_size:i]\n        \n        # 测试集\n        test_data = data[i:i+test_size]\n        \n        # 训练模型\n        model = train_model(train_data)\n        \n        # 生成预测\n        predictions = model.predict(test_data)\n        \n        # 回测\n        performance = backtest(predictions, test_data)\n        \n        results.append({\n            &#039;test_start&#039;: test_data.index[0],\n            &#039;test_end&#039;: test_data.index[-1],\n            &#039;return&#039;: performance[&#039;return&#039;],\n            &#039;sharpe&#039;: performance[&#039;sharpe&#039;]\n        })\n    \n    return pd.DataFrame(results)\n \n# 示例\nresults = rolling_window_backtest(data, strategy_func, \n                                 window_size=252, test_size=63)\n3.2 扩展窗口验证\ndef expanding_window_backtest(data, strategy_func, \n                                initial_size=252, test_size=63):\n    &quot;&quot;&quot;\n    扩展窗口验证\n    \n    参数:\n        data: 数据\n        strategy_func: 策略函数\n        initial_size: 初始训练集大小\n        test_size: 测试窗口大小\n    \n    返回:\n        results: 验证结果\n    &quot;&quot;&quot;\n    results = []\n    \n    for i in range(initial_size, len(data), test_size):\n        # 训练集（不断扩大）\n        train_data = data[:i]\n        \n        # 测试集\n        test_data = data[i:i+test_size]\n        \n        # 训练模型\n        model = train_model(train_data)\n        \n        # 生成预测\n        predictions = model.predict(test_data)\n        \n        # 回测\n        performance = backtest(predictions, test_data)\n        \n        results.append({\n            &#039;test_start&#039;: test_data.index[0],\n            &#039;test_end&#039;: test_data.index[-1],\n            &#039;train_size&#039;: len(train_data),\n            &#039;return&#039;: performance[&#039;return&#039;],\n            &#039;sharpe&#039;: performance[&#039;sharpe&#039;]\n        })\n    \n    return pd.DataFrame(results)\n \n# 示例\nresults = expanding_window_backtest(data, strategy_func, \n                                  initial_size=252, test_size=63)\n4. 避免过拟合\n4.1 样本外验证\ndef out_of_sample_validation(data, split_ratio=0.7):\n    &quot;&quot;&quot;\n    样本外验证\n    \n    参数:\n        data: 数据\n        split_ratio: 训练集比例\n    \n    返回:\n        train_data: 训练集\n        test_data: 测试集\n    &quot;&quot;&quot;\n    # 划分数据\n    split_idx = int(len(data) * split_ratio)\n    \n    train_data = data[:split_idx]\n    test_data = data[split_idx:]\n    \n    return train_data, test_data\n \n# 使用示例\ntrain_data, test_data = out_of_sample_validation(data, split_ratio=0.7)\n \n# 训练模型\nmodel = train_model(train_data)\n \n# 测试模型\npredictions = model.predict(test_data)\nperformance = evaluate(predictions, test_data)\n4.2 参数限制\n# ❌ 错误做法：过度优化\nparams = {\n    &#039;k&#039;: 23,\n    &#039;lookback&#039;: 17,\n    &#039;threshold&#039;: 0.032,\n    &#039;smoothing&#039;: 0.7,\n    # ... 太多参数\n}\n \n# ✅ 正确做法：少量关键参数\nparams = {\n    &#039;k&#039;: 20,        # 清晰的含义\n    &#039;lookback&#039;: 20   # 基于经济逻辑\n}\n4.3 交叉验证\nfrom sklearn.model_selection import TimeSeriesSplit\n \ndef time_series_cv(X, y, model_func, n_splits=5):\n    &quot;&quot;&quot;\n    时间序列交叉验证\n    \n    参数:\n        X: 特征\n        y: 目标\n        model_func: 模型函数\n        n_splits: 折数\n    \n    返回:\n        scores: 每折的分数\n    &quot;&quot;&quot;\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    scores = []\n    \n    for train_idx, test_idx in tscv.split(X):\n        X_train, X_test = X[train_idx], X[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n        \n        # 训练模型\n        model = model_func()\n        model.fit(X_train, y_train)\n        \n        # 预测\n        y_pred = model.predict(X_test)\n        \n        # 评估\n        score = evaluate(y_pred, y_test)\n        scores.append(score)\n    \n    return scores\n \n# 示例\nscores = time_series_cv(X, y, model_func, n_splits=5)\nprint(f&quot;平均分数: {np.mean(scores):.4f}&quot;)\nprint(f&quot;标准差: {np.std(scores):.4f}&quot;)\n总结\n实验分析是验证策略有效性的关键：\n\n参数敏感性：了解参数变化的影响\n持仓周期：研究调仓频率的影响\n样本外验证：防止过拟合\n交叉验证：提高结果的可靠性\n\n建议：\n\n务必进行样本外验证\n使用多种验证方法\n关注参数稳定性\n避免过度优化\n"},"quant/qlib/week3/06-回测流程与实践":{"slug":"quant/qlib/week3/06-回测流程与实践","filePath":"quant/qlib/week3/06-回测流程与实践.md","title":"06-回测流程与实践","links":[],"tags":[],"content":"回测流程与实践\n1. Qlib 回测框架\n1.1 回测架构\n# 1. 数据准备\ndata = qlib.data.get_data(start_date, end_date)\n \n# 2. 模型训练\nmodel = train_model(train_data)\n \n# 3. 策略定义\nstrategy = Strategy(model)\n \n# 4. 回测配置\nconfig = {\n    &#039;executor&#039;: SimulatorExecutor(...),\n    &#039;account&#039;: 1000000,\n    &#039;benchmark&#039;: &#039;SPY&#039;\n}\n \n# 5. 执行回测\nresult = qlib.backtest.run(\n    strategy=strategy,\n    data=data,\n    config=config\n)\n \n# 6. 分析结果\nmetrics = qlib.backtest.analyze(result)\n1.2 Qlib 回测流程\n1. 初始化Qlib\n   ↓\n2. 加载数据\n   ↓\n3. 训练模型\n   ↓\n4. 定义策略\n   ↓\n5. 配置Executor\n   ↓\n6. 执行回测\n   ↓\n7. 分析结果\n   ↓\n8. 生成报告\n\n2. 完整回测步骤\n2.1 Step 1: 数据准备\nimport qlib\nfrom qlib.constant import REG_CN\n \n# 初始化Qlib\nqlib.init(provider_uri=&#039;~/.qlib/qlib_data/cn_data&#039;, region=REG_CN)\n \n# 获取股票列表\ninstruments = qlib.get_instruments(&#039;csi300&#039;)\n \n# 获取数据\nfrom qlib.data import D\n \ndata = D.features(\n    instruments,\n    fields=[&#039;$close&#039;, &#039;$volume&#039;, &#039;$factor&#039;],\n    start_time=&#039;2020-01-01&#039;,\n    end_time=&#039;2022-12-31&#039;\n)\n2.2 Step 2: 特征工程\n# 定义特征\nfeatures = [\n    &#039;$close&#039;,\n    &#039;$volume&#039;,\n    &#039;Ref($close, 1)/Ref($close, 0) - 1&#039;,  # 收益率\n    &#039;Mean($close, 5)&#039;,  # 5日均线\n    &#039;Std($close, 20)&#039;  # 20日波动率\n]\n \n# 计算特征\nfeature_data = D.features(\n    instruments,\n    fields=features,\n    start_time=&#039;2020-01-01&#039;,\n    end_time=&#039;2022-12-31&#039;\n)\n2.3 Step 3: 模型训练\n# 定义标签\nlabel = &#039;Ref($close, 2)/Ref($close, 1) - 1&#039;\n \n# 划分数据集\ntrain_data = feature_data[&#039;2020-01-01&#039;:&#039;2021-12-31&#039;]\ntest_data = feature_data[&#039;2022-01-01&#039;:&#039;2022-12-31&#039;]\n \n# 训练模型\nimport lightgbm as lgb\n \nmodel = lgb.LGBMRegressor(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    random_state=42\n)\n \nmodel.fit(\n    train_data[features].values,\n    train_data[label].values\n)\n \n# 生成预测\npredictions = model.predict(test_data[features].values)\n2.4 Step 4: 策略定义\ndef topk_strategy(predictions, k=20):\n    &quot;&quot;&quot;\n    Top-K投资组合策略\n    \n    参数:\n        predictions: 预测分数\n        k: 选择股票数量\n    \n    返回:\n        weights: 股票权重\n    &quot;&quot;&quot;\n    # 按预测分数排序\n    sorted_predictions = predictions.sort_values(ascending=False)\n    \n    # 选择Top-K\n    topk = sorted_predictions[:k]\n    \n    # 等权重\n    weight = 1.0 / k\n    \n    # 分配权重\n    weights = pd.Series(0, index=predictions.index)\n    weights[topk.index] = weight\n    \n    return weights\n2.5 Step 5: 配置 Executor\nfrom qlib.backtest.executor import SimulatorExecutor\nfrom qlib.backtest.backtest import backtest_executor\n \n# Executor配置\nexecutor_config = {\n    &#039;time_per_step&#039;: &#039;day&#039;,\n    &#039;generate_portfolio_metrics&#039;: True\n}\n \n# 交易所配置\nexchange = {\n    &#039;freq&#039;: &#039;day&#039;,\n    &#039;limit_threshold&#039;: 0.095,\n    &#039;deal_price&#039;: &#039;close&#039;,\n    &#039;open_cost&#039;: 0.0005,  # 买入费率万分之五\n    &#039;close_cost&#039;: 0.0015,  # 卖出费率千分之1.5\n    &#039;min_cost&#039;: 5  # 最低手续费5元\n}\n \n# 创建Executor\nexecutor = SimulatorExecutor(\n    exchange=exchange,\n    **executor_config\n)\n2.6 Step 6: 执行回测\nfrom qlib.backtest import backtest\n \n# 执行回测\nportfolio_metrics, indicators = backtest(\n    executor=executor,\n    strategy=lambda x: topk_strategy(x, k=20),\n    test_data=test_data\n)\n \n# 获取结果\nprint(&quot;回测结果:&quot;)\nprint(f&quot;总收益率: {portfolio_metrics[&#039;return&#039;]:.4f}&quot;)\nprint(f&quot;年化收益率: {portfolio_metrics[&#039;annualized_return&#039;]:.4f}&quot;)\nprint(f&quot;夏普比率: {indicators[&#039;sharpe_ratio&#039;]:.4f}&quot;)\nprint(f&quot;最大回撤: {indicators[&#039;max_drawdown&#039;]:.4f}&quot;)\n2.7 Step 7: 分析结果\nimport matplotlib.pyplot as plt\n \n# 绘制累计收益曲线\nplt.figure(figsize=(12, 6))\nplt.plot(portfolio_metrics[&#039;cumulative_return&#039;].index,\n         portfolio_metrics[&#039;cumulative_return&#039;].values,\n         label=&#039;Strategy&#039;)\nplt.title(&#039;Cumulative Return&#039;)\nplt.xlabel(&#039;Date&#039;)\nplt.ylabel(&#039;Cumulative Return&#039;)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n \n# 绘制回撤曲线\nplt.figure(figsize=(12, 6))\nplt.plot(indicators[&#039;drawdown&#039;].index,\n         indicators[&#039;drawdown&#039;].values,\n         color=&#039;red&#039;)\nplt.fill_between(indicators[&#039;drawdown&#039;].index,\n                 indicators[&#039;drawdown&#039;].values, 0,\n                 alpha=0.3, color=&#039;red&#039;)\nplt.title(&#039;Drawdown&#039;)\nplt.xlabel(&#039;Date&#039;)\nplt.ylabel(&#039;Drawdown&#039;)\nplt.grid(True, alpha=0.3)\nplt.show()\n3. 完整回测示例\nimport qlib\nfrom qlib.constant import REG_CN\nfrom qlib.data import D\nfrom qlib.backtest.executor import SimulatorExecutor\nfrom qlib.backtest import backtest\nimport lightgbm as lgb\nimport pandas as pd\nimport numpy as np\n \n# 1. 初始化Qlib\nqlib.init(provider_uri=&#039;~/.qlib/qlib_data/cn_data&#039;, region=REG_CN)\n \n# 2. 获取数据\ninstruments = qlib.get_instruments(&#039;csi300&#039;)\n \n# 3. 定义特征\nfeatures = [\n    &#039;$close&#039;,\n    &#039;Ref($close, 1)/Ref($close, 0) - 1&#039;,\n    &#039;Mean($close, 5)&#039;,\n    &#039;Std($close, 20)&#039;\n]\n \n# 4. 获取数据\ndata = D.features(\n    instruments,\n    fields=features,\n    start_time=&#039;2020-01-01&#039;,\n    end_time=&#039;2022-12-31&#039;\n)\n \n# 5. 划分数据集\ntrain_data = data[&#039;2020-01-01&#039;:&#039;2021-12-31&#039;]\ntest_data = data[&#039;2022-01-01&#039;:&#039;2022-12-31&#039;]\n \n# 6. 训练模型\nmodel = lgb.LGBMRegressor(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1\n)\nmodel.fit(train_data[features], train_data[&#039;$close&#039;])\n \n# 7. 生成预测\npredictions = model.predict(test_data[features])\npredictions = pd.Series(predictions, index=test_data.index)\n \n# 8. 定义策略\ndef strategy(pred):\n    sorted_pred = pred.sort_values(ascending=False)\n    top20 = sorted_pred[:20]\n    weights = pd.Series(0, index=pred.index)\n    weights[top20.index] = 1.0/20\n    return weights\n \n# 9. 配置Executor\nexchange = {\n    &#039;freq&#039;: &#039;day&#039;,\n    &#039;limit_threshold&#039;: 0.095,\n    &#039;deal_price&#039;: &#039;close&#039;,\n    &#039;open_cost&#039;: 0.0005,\n    &#039;close_cost&#039;: 0.0015,\n    &#039;min_cost&#039;: 5\n}\n \nexecutor = SimulatorExecutor(exchange=exchange)\n \n# 10. 执行回测\nportfolio_metrics, indicators = backtest(\n    executor=executor,\n    strategy=strategy,\n    test_data=test_data\n)\n \n# 11. 输出结果\nprint(&quot;回测完成！&quot;)\nprint(f&quot;总收益率: {portfolio_metrics[&#039;return&#039;]:.4f}&quot;)\nprint(f&quot;年化收益率: {portfolio_metrics[&#039;annualized_return&#039;]:.4f}&quot;)\nprint(f&quot;夏普比率: {indicators[&#039;sharpe_ratio&#039;]:.4f}&quot;)\nprint(f&quot;最大回撤: {indicators[&#039;max_drawdown&#039;]:.4f}&quot;)\n4. 实践建议\n4.1 回测原则\n\n\n简单开始\n\n从简单策略开始\n逐步增加复杂度\n理解每个环节\n\n\n\n严格验证\n\n使用样本外验证\n多时间段验证\n成本敏感性分析\n\n\n\n风险控制\n\n设置止损机制\n分散投资\n限制单股权重\n\n\n\n4.2 常见错误\n\n\n未来函数\n\n使用未来数据\n数据泄露\n解决方案：检查数据对齐\n\n\n\n成本低估\n\n忽略交易成本\n滑点和市场冲击\n解决方案：使用合理的成本参数\n\n\n\n过拟合\n\n过度优化参数\n样本内表现好，样本外差\n解决方案：样本外验证\n\n\n\n4.3 最佳实践\n# ✅ 正确做法\n \n# 1. 使用样本外验证\ntrain_data = data[&#039;2020&#039;:&#039;2021&#039;]\ntest_data = data[&#039;2022&#039;]\n \n# 2. 使用合理的成本参数\nexchange = {\n    &#039;open_cost&#039;: 0.0005,\n    &#039;close_cost&#039;: 0.0015,\n    &#039;min_cost&#039;: 5\n}\n \n# 3. 分散投资\ndef strategy(pred):\n    top30 = pred.nlargest(30)\n    weights = pd.Series(0, index=pred.index)\n    weights[top30.index] = 1.0/30\n    return weights\n \n# 4. 风险控制\nmax_weight = 0.1\nweights = weights.clip(upper=max_weight)\nweights = weights / weights.sum()\n \n# ❌ 错误做法\n \n# 1. 使用全部数据训练和测试\nmodel = Model()\nmodel.fit(data, labels)  # 使用全部数据\npredictions = model.predict(data)  # 在相同数据上预测\n \n# 2. 忽略交易成本\nexchange = {\n    &#039;open_cost&#039;: 0,\n    &#039;close_cost&#039;: 0\n}\n \n# 3. 集中投资\ndef strategy(pred):\n    top5 = pred.nlargest(5)  # 只选5只\n    weights = pd.Series(0, index=pred.index)\n    weights[top5.index] = 1.0/5\n    return weights  # 单股权重20%\n总结\n完整的回测流程包括：\n\n数据准备：加载和清洗数据\n特征工程：计算特征和标签\n模型训练：训练预测模型\n策略定义：定义交易策略\n回测配置：配置Executor\n执行回测：运行回测\n分析结果：评估策略表现\n\n建议：\n\n从简单策略开始\n严格验证\n控制风险\n避免常见错误\n"},"quant/qlib/week3/07-学习检查清单":{"slug":"quant/qlib/week3/07-学习检查清单","filePath":"quant/qlib/week3/07-学习检查清单.md","title":"07-学习检查清单","links":[],"tags":[],"content":"学习检查清单\n📋 学习目标检查\n✅ 交易策略理论\n\n\n 理解交易策略的核心组成\n\n能解释信号生成、仓位管理、风险控制、执行规则\n能画出完整的策略工作流程图\n\n\n\n 掌握IC、换手率、回撤等基础概念\n\n能计算IC和信息比率\n能计算换手率和回撤\n理解各指标的含义\n\n\n\n 理解Top-K策略原理\n\n能实现Top-K均等权重策略\n了解K值选择的影响\n知道Top-K策略的优缺点\n\n\n\n 理解IC权重策略\n\n能计算滚动IC\n能实现IC权重分配\n了解IC权重策略的适用场景\n\n\n\n 了解均值-方差优化（MVO）\n\n理解马科维茨理论\n能使用CVXPY求解MV优化\n了解有效前沿的概念\n\n\n\n✅ 投资组合构建方法\n\n\n 能实现Top-K均等权重\n\n能编写完整的策略代码\n能分析K值敏感性\n能优化调仓频率\n\n\n\n 能实现IC权重分配\n\n能计算历史IC\n能实现权重平滑\n能限制单股权重\n\n\n\n 能实现MV优化\n\n能估计期望收益和协方差矩阵\n能添加约束条件\n能进行正则化处理\n\n\n\n 能对比三种方法\n\n能从性能、复杂度、风险等维度对比\n能选择合适的方法\n能结合多种方法\n\n\n\n✅ Executor与成本模型\n\n\n 理解Executor机制\n\n能解释Executor的工作原理\n能实现简单的Executor\n能管理持仓和交易\n\n\n\n 掌握交易成本模型\n\n能计算手续费\n能计算滑点成本\n能计算市场冲击\n\n\n\n 能进行成本敏感性分析\n\n能测试不同成本参数的影响\n能分析换手率与成本的关系\n能优化策略降低成本\n\n\n\n✅ 绩效评估指标\n\n\n 能计算收益率指标\n\n能计算总收益率\n能计算年化收益率\n能计算超额收益率\n\n\n\n 能计算风险指标\n\n能计算波动率\n能计算最大回撤\n能计算VaR\n\n\n\n 能计算风险调整收益指标\n\n能计算夏普比率\n能计算信息比率\n能计算卡尔马比率\n\n\n\n 能计算交易相关指标\n\n能计算换手率\n能计算胜率\n能计算盈亏比\n\n\n\n✅ 实验分析方法\n\n\n 能进行参数敏感性分析\n\n能实现单参数敏感性分析\n能实现多参数敏感性分析\n能绘制敏感性热力图\n\n\n\n 能进行持仓周期研究\n\n能测试不同调仓频率\n能分析持仓周期的影响\n能选择最优调仓频率\n\n\n\n 能进行样本外验证\n\n能实现滚动窗口验证\n能实现扩展窗口验证\n能进行时间序列交叉验证\n\n\n\n✅ 回测流程与实践\n\n\n 能使用Qlib框架\n\n能初始化Qlib\n能加载数据\n能进行特征工程\n\n\n\n 能完成完整回测\n\n能训练模型\n能定义策略\n能配置Executor\n能执行回测\n能分析结果\n\n\n\n 能避免常见错误\n\n能避免未来函数\n能正确处理成本\n能防止过拟合\n\n\n\n🎯 实践能力检查\n✅ 能独立完成的项目\n\n\n 项目1: Top-K策略回测\n\n实现Top-K策略\n完成完整回测\n分析策略表现\n\n\n\n 项目2: IC权重策略\n\n计算历史IC\n实现IC权重策略\n对比Top-K策略\n\n\n\n 项目3: MV优化策略\n\n估计收益和风险\n求解MV优化\n分析组合特征\n\n\n\n 项目4: 参数优化\n\n进行参数敏感性分析\n寻找最优参数组合\n验证参数稳定性\n\n\n\n 项目5: 成本分析\n\n进行成本敏感性测试\n优化策略降低成本\n分析成本对收益的影响\n\n\n\n✅ 能回答的问题\n\n\n为什么需要回测？\n\n验证策略有效性\n评估策略风险\n优化策略参数\n\n\n\nTop-K、IC权重、MV优化，哪个更好？\n\n没有绝对最好的方法\n根据实际情况选择\n都需要学习和理解\n\n\n\n交易成本有多大影响？\n\n影响可能非常大\n高换手率策略对成本更敏感\n务必做成本敏感性分析\n\n\n\n如何避免过拟合？\n\n使用样本外验证\n限制参数数量\n简化策略逻辑\n\n\n\n哪些指标最重要？\n\n夏普比率：风险调整后收益\n最大回撤：最大损失\nIC：预测能力\n换手率：交易活跃度\n\n\n\n📚 学习路径\n🟢 初学者路径\n1. 交易策略理论\n   ↓\n2. Top-K策略实现\n   ↓\n3. 绩效评估指标\n   ↓\n4. 简单回测\n\n🟡 进阶路径\n1. IC权重策略\n   ↓\n2. MV优化方法\n   ↓\n3. 参数敏感性分析\n   ↓\n4. 样本外验证\n\n🔴 实战路径\n1. 完整Qlib回测\n   ↓\n2. 策略优化\n   ↓\n3. 成本控制\n   ↓\n4. 实盘验证\n\n🔧 工具箱\n必备 Python 库\nimport qlib              # 量化投资框架\nimport pandas as pd       # 数据处理\nimport numpy as np        # 数值计算\nimport lightgbm as lgb    # 梯度提升树\nimport cvxpy as cp        # 凸优化\nimport matplotlib.pyplot as plt  # 可视化\nimport seaborn as sns     # 统计绘图\n常用代码片段\n1. Top-K策略\ndef topk_strategy(predictions, k=20):\n    sorted_pred = predictions.sort_values(ascending=False)\n    topk = sorted_pred[:k]\n    weight = 1.0 / k\n    weights = pd.Series(0, index=predictions.index)\n    weights[topk.index] = weight\n    return weights\n2. IC计算\nfrom scipy.stats import spearmanr\n \ndef calculate_ic(predictions, returns):\n    ic, _ = spearmanr(predictions, returns)\n    return ic\n3. MV优化\ndef mv_optimization(returns, risk_aversion=1.0, max_weight=0.1):\n    n = len(returns.columns)\n    w = cp.Variable(n)\n    \n    mu = returns.mean().values\n    sigma = returns.cov().values\n    sigma_reg = sigma + 1e-6 * np.eye(n)\n    \n    objective = cp.Maximize(mu @ w - risk_aversion * cp.quad_form(w, sigma_reg))\n    constraints = [cp.sum(w) == 1, w &gt;= 0, w &lt;= max_weight]\n    \n    problem = cp.Problem(objective, constraints)\n    problem.solve()\n    \n    return pd.Series(w.value, index=returns.columns)\n4. 绩效评估\ndef evaluate_portfolio(returns, benchmark_returns=None):\n    cumulative_returns = returns.cumsum()\n    \n    metrics = {}\n    metrics[&#039;total_return&#039;] = cumulative_returns.iloc[-1]\n    metrics[&#039;volatility&#039;] = returns.std() * np.sqrt(252)\n    \n    _, max_drawdown = calculate_max_drawdown(cumulative_returns)\n    metrics[&#039;max_drawdown&#039;] = max_drawdown\n    \n    sharpe = returns.mean() / returns.std() * np.sqrt(252)\n    metrics[&#039;sharpe_ratio&#039;] = sharpe\n    \n    return metrics\n🚀 下一步学习\n完成 Week 4 后，你已经掌握了：\n✅ 交易策略理论\n✅ 投资组合构建方法\n✅ Executor与成本模型\n✅ 绩效评估指标\n✅ 实验分析方法\n✅ 完整回测流程\n下一步：实战应用\n你将学习：\n\n完整策略开发\n实盘风险管理\n策略监控系统\n持续优化改进\n\n\n祝学习顺利！ 🎉"},"quant/qlib/week3/index":{"slug":"quant/qlib/week3/index","filePath":"quant/qlib/week3/index.md","title":"index","links":["quant/qlib/week3/01-交易策略理论","quant/qlib/week3/02-投资组合构建方法","quant/qlib/week3/03-Executor与成本模型","quant/qlib/week3/04-绩效评估指标","quant/qlib/week3/05-实验分析方法","quant/qlib/week3/06-回测流程与实践","/"],"tags":[],"content":"回测引擎与策略实战\n回测（Backtesting）是量化投资的核心环节，通过历史数据验证交易策略的有效性。本模块系统讲解了回测引擎、投资组合构建、绩效评估等关键内容。\n\n📖 文档目录\n1️⃣ 交易策略理论\n→ 阅读完整文档\n核心内容：\n\nTop-K策略原理与实现\nIC权重分配方法\n均值-方差优化（MVO）\n三种方法对比分析\n\n适合人群：想要理解交易策略基础的学习者\n\n2️⃣ 投资组合构建方法\n→ 阅读完整文档\n核心内容：\n\nTop-K均等权重算法\nIC权重分配算法\n均值-方差优化求解\n参数选择与调优\n\n适合人群：需要实现投资组合构建的开发者\n\n3️⃣ Executor与成本模型\n→ 阅读完整文档\n核心内容：\n\nExecutor机制详解\n交易成本模型（手续费、滑点、市场冲击）\n成本敏感性分析\n成本参数配置\n\n适合人群：需要理解交易成本的研究者\n\n4️⃣ 绩效评估指标\n→ 阅读完整文档\n核心内容：\n\n收益率指标（总收益、年化收益、超额收益）\n风险指标（波动率、最大回撤、VaR）\n风险调整收益指标（夏普比率、信息比率、卡尔马比率）\n交易相关指标（换手率、胜率、盈亏比）\n\n适合人群：需要评估策略表现的分析师\n\n5️⃣ 实验分析方法\n→ 阅读完整文档\n核心内容：\n\n参数敏感性分析\n持仓周期研究\n样本外验证方法\n滚动窗口验证\n\n适合人群：需要优化和验证策略的研究者\n\n6️⃣ 回测流程与实践\n→ 阅读完整文档\n核心内容：\n\nQlib回测框架详解\n完整回测步骤\n数据准备与特征工程\n策略定义与执行\n结果分析与报告\n\n适合人群：需要完成完整回测的实践者\n\n🎯 学习路径\n🟢 初学者路径\n交易策略理论 → 投资组合构建 → 绩效评估 → 回测流程\n\n目标：理解回测基本概念，能够完成简单策略回测\n🟡 进阶路径\n成本模型 → 参数敏感性 → 样本外验证 → 策略优化\n\n目标：深入理解回测细节，能够优化和验证策略\n🔴 实战路径\n从实际项目出发 → 遇到问题查文档 → 理论学习 → 实践应用\n\n目标：解决实际问题，积累实战经验\n\n📚 学习前准备\n必备知识\n\n\n Python 编程基础\n\n数据处理（pandas, numpy）\n可视化（matplotlib, seaborn）\n\n\n\n 机器学习基础\n\nLightGBM 模型训练\nIC/ICIR 评估指标\n\n\n\n 量化投资基础\n\n特征工程（Qlib）\n时序数据划分\n\n\n\n推荐顺序\n\n先学习 特征工程模块 和 LightGBM 模块\n再学习本回测模块\n最后进行实战练习\n\n\n💡 实用提示\n\n文档示例：所有代码示例均可直接运行\n实战导向：内容针对量化投资实战设计\n最佳实践：包含大量实战经验总结\n持续更新：跟随最新技术发展\n\n\n🔗 相关资源\n官方文档：\n\nQlib 官方文档\nLightGBM 官方文档\n\n开源项目：\n\nQlib GitHub\nQuantopian Zipline\n\n学习资源：\n\nQuantStart 量化教程\nQuantopian 讲座\n\n\n← 返回首页"},"quant/qlib/week5/01_基础理论系列/index":{"slug":"quant/qlib/week5/01_基础理论系列/index","filePath":"quant/qlib/week5/01_基础理论系列/index.md","title":"index","links":["02_PyTorch框架系列/"],"tags":[],"content":"基础理论系列 - RNN与LSTM原理\n📚 系列概述\n本系列文档涵盖深度学习基础、序列数据处理、RNN与LSTM的核心原理和数学基础。\n\n📖 文档列表\n\n深度学习基础\n序列数据与RNN\nLSTM原理详解\n\n\n深度学习基础\n核心概念\n1. 深度学习定义\n\n本质: 基于多层神经网络的机器学习方法\n特点: 自动学习特征表达\n优势: 在非结构化数据（图像、语音、文本）上表现优异\n\n2. 神经元模型\n输入层 → 加权求和 → 激活函数 → 输出\n\n数学表达式:\ny = f(Σ(w_i * x_i) + b)\n\n3. 常用激活函数\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n函数公式输出范围特点Sigmoid1/(1+e^(-x))(0, 1)输出概率，梯度消失Tanh(e^x-e^(-x))/(e^x+e^(-x))(-1, 1)零中心，梯度消失ReLUmax(0, x)[0, +∞)缓解梯度消失，计算快Leaky ReLUmax(αx, x), α&lt;&lt;1(-∞, +∞)解决ReLU死亡问题\n深度学习 vs 传统机器学习\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n维度传统机器学习深度学习特征工程手工设计自动学习数据需求中等大量训练时间短长可解释性较好较差\n量化投资中的应用场景\n\n时序预测: 股价、收益率预测\n特征学习: 自动提取有效因子\n模式识别: 市场状态识别\n风险预测: 违约概率、波动率预测\n\n\n序列数据与RNN\n序列数据特征\n1. 定义\n具有时间或顺序依赖的数据\n2. 特点\n\n顺序重要: 数据点的顺序有意义\n长度可变: 不同序列长度不同\n上下文依赖: 当前值依赖历史\n模式重复: 存在周期性模式\n\n3. 常见类型\n\n时间序列: 股价、天气、销量\n文本: 句子、文章\n语音: 音频波形\n视频: 帧序列\n\nRNN（循环神经网络）\n1. 核心思想\n\n共享参数: 同一神经网络处理每个时间步\n隐藏状态: 将历史信息传递到下一个时间步\n\n2. RNN架构\n时间步 t:\n  输入 x_t ──┐\n              ├── 加权求和 → 激活 → 隐藏状态 h_t\n  上一步 h_{t-1} ──┘\n              ↓\n          输出 y_t\n\n3. 数学公式\n隐藏状态更新:\nh_t = f(W_h * h_{t-1} + W_x * x_t + b)\n\n输出计算:\ny_t = g(W_y * h_t + c)\n\n变量说明:\n\nh_t: 时间t的隐藏状态（包含历史信息）\nx_t: 时间t的输入\nW_h: 隐藏状态权重矩阵\nW_x: 输入权重矩阵\nW_y: 输出权重矩阵\nb, c: 偏置项\nf, g: 激活函数（通常为tanh）\n\n4. RNN计算示例\n# 参数\nW_h = 0.5\nW_x = 0.3\nb = 0.1\n \n# 输入序列\ninputs = [0.1, 0.2, 0.3, 0.4]\nh_prev = 0.0  # 初始隐藏状态\n \n# 时间步计算\nfor t, x_t in enumerate(inputs, 1):\n    h_t = tanh(W_h * h_prev + W_x * x_t + b)\n    # h_t 包含了历史信息 (x_1, x_2, ..., x_t)\n    h_prev = h_t\nRNN的局限性\n梯度消失问题\n原因分析:\n链式法则:\n∂L/∂h_1 = ∂L/∂h_T * ∂h_T/∂h_{T-1} * ... * ∂h_2/∂h_1\n\n如果 |∂h_t/∂h_{t-1}| &lt; 1，乘积会快速趋近于0\n梯度消失示例:\n假设 |∂h_t/∂h_{t-1}| = 0.9:\n\n时间步 10: 0.9^10 = 0.35\n时间步 50: 0.9^50 = 0.005\n时间步 100: 0.9^100 = 0.00003\n\n影响:\n\n早期信息的梯度接近0\n无法学习长期依赖\n只能记住短期信息（约10-20步）\n\n解决方案\n\n使用LSTM（推荐）\n使用GRU\n梯度裁剪（解决梯度爆炸）\n\n\nLSTM原理详解\nLSTM的创新\n三大核心创新\n1. 细胞状态（Cell State, C_t）\n\n信息高速公路\n可以长期保持信息\n通过门控制信息流动\n\n2. 遗忘门（Forget Gate, f_t）\n\n决定丢弃什么信息\n输出0-1的值（sigmoid）\n0 = 完全遗忘，1 = 完全保留\n\n3. 输入门（Input Gate, i_t）\n\n决定添加什么信息\n控制新信息的流入\n选择性地更新细胞状态\n\n4. 输出门（Output Gate, o_t）\n\n决定输出什么信息\n基于细胞状态和隐藏状态\n生成最终的输出\n\nLSTM单元结构\n完整计算流程\nStep 1: 遗忘门\nf_t = σ(W_f * [h_{t-1}, x_t] + b_f)\n\n\n决定从细胞状态中丢弃什么信息\nσ: sigmoid函数（输出0-1）\n\nStep 2: 输入门\ni_t = σ(W_i * [h_{t-1}, x_t] + b_i)\nC̃_t = tanh(W_C * [h_{t-1}, x_t] + b_C)\n\n\ni_t：决定更新哪些值\nC̃_t：候选细胞状态\n\nStep 3: 细胞状态更新\nC_t = f_t * C_{t-1} + i_t * C̃_t\n\n\nf_t * C_{t-1}：遗忘部分信息\ni_t * C̃_t：添加部分新信息\n\nStep 4: 输出门\no_t = σ(W_o * [h_{t-1}, x_t] + b_o)\n\n\n决定输出什么信息\n\nStep 5: 隐藏状态更新\nh_t = o_t * tanh(C_t)\n\nLSTM门的作用示例\n股票价格预测场景\n场景1: 市场突然暴跌\n\n遗忘门：保留（记住暴跌事件）\n输入门：添加（记录恐慌情绪）\n输出门：输出（影响短期预测）\n\n场景2: 长期牛市趋势\n\n遗忘门：保留（长期趋势很重要）\n输入门：添加（持续更新）\n输出门：输出（影响预测）\n\n场景3: 短期噪音\n\n遗忘门：遗忘（忽略短期波动）\n输入门：不添加（不记录噪音）\n输出门：输出（基于长期信息）\n\nLSTM vs RNN vs GRU\n架构对比\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n特性RNNLSTMGRU长期依赖差优秀良好参数数量少多中训练速度快中快计算复杂度低高中门数量032细胞状态无有无记忆能力短期长中长量化使用很少常用常用\nGRU简介\nGRU（Gated Recurrent Unit）是LSTM的简化版本:\nGRU计算:\nz_t = σ(W_z * [h_{t-1}, x_t])  # 更新门\nr_t = σ(W_r * [h_{t-1}, x_t])  # 重置门\nh̃_t = tanh(W * [r_t * h_{t-1}, x_t])\nh_t = (1 - z_t) * h_{t-1} + z_t * h̃_t\n\nGRU特点:\n\n只有2个门（更新门、重置门）\n参数更少，训练更快\n性能与LSTM相近\n\n选择建议\nRNN:\n\n简单短期序列\n快速原型\n很少在量化中使用\n\nLSTM:\n\n长期依赖很重要\n计算资源充足\n量化投资常用\n\nGRU:\n\n平衡性能和速度\n参数较少\n也能用于量化\n\nLSTM的优势\n1. 解决梯度消失\n\n细胞状态提供梯度通路\n门机制控制梯度流动\n能够学习长期依赖\n\n2. 灵活的信息控制\n\n遗忘门：控制信息遗忘\n输入门：控制信息添加\n输出门：控制信息输出\n\n3. 应用广泛\n\n时间序列预测\n自然语言处理\n语音识别\n视频分析\n\n\n核心知识点总结\n深度学习基础\n\n✅ 神经元模型和激活函数\n✅ 多层感知机（MLP）\n✅ 深度学习 vs 传统机器学习\n✅ 在量化投资中的应用\n\nRNN原理\n\n✅ 序列数据特征\n✅ RNN架构和数学公式\n✅ 梯度消失问题\n✅ RNN的局限性\n\nLSTM原理\n\n✅ LSTM的三大创新\n✅ 细胞状态和门机制\n✅ LSTM计算流程\n✅ LSTM vs RNN vs GRU对比\n✅ 选择建议\n\n\n下一步\n继续学习: PyTorch框架系列"},"quant/qlib/week5/02_PyTorch框架系列/index":{"slug":"quant/qlib/week5/02_PyTorch框架系列/index","filePath":"quant/qlib/week5/02_PyTorch框架系列/index.md","title":"index","links":["03_LSTM模型构建系列/"],"tags":[],"content":"PyTorch框架系列 - Tensor与模型构建\n📚 系列概述\n本系列文档涵盖PyTorch的核心概念、Tensor操作、自动微分、模型定义和常用层。\n\n📖 文档列表\n\nPyTorch简介\nTensor基础\nAutograd自动微分\nnn.Module模型定义\n常用层\n\n\nPyTorch简介\n什么是PyTorch\n\n基于Python的深度学习框架\nFacebook（Meta）开发\n学术研究首选框架\n\n核心特点\n1. 动态计算图\n\n运行时构建计算图\n灵活性高\n适合研究\n\n2. GPU加速\n\n自动利用GPU\n显著提升训练速度\n支持CUDA\n\n3. 自动微分\n\n自动计算梯度\n简化反向传播\n支持复杂计算图\n\n4. 丰富的API\n\n预定义层和模型\n优化器和损失函数\n数据处理工具\n\n为什么量化投资用PyTorch\n1. 灵活性强\n\n可以自定义复杂的模型结构\n容易实现研究想法\n\n2. 社区活跃\n\n大量教程和示例\n问题容易解决\n\n3. 易于部署\n\n支持导出为多种格式\n生产环境友好\n\n\nTensor基础\nTensor定义\n\nPyTorch的核心数据结构\n类似于NumPy数组\n可以运行在GPU上\n\n创建Tensor\nimport torch\n \n# 从Python列表创建\nt1 = torch.tensor([1, 2, 3, 4])\n \n# 从NumPy创建\nimport numpy as np\nnp_array = np.array([[1, 2], [3, 4]])\nt2 = torch.from_numpy(np_array)\n \n# 创建特殊Tensor\nzeros = torch.zeros(2, 3)        # 全零\nones = torch.ones(2, 3)          # 全一\nrandom = torch.randn(2, 3)       # 标准正态分布\n \n# 创建序列\narange = torch.arange(0, 10)      # 0-9\nlinspace = torch.linspace(0, 10, 5)  # 0到10，5个点\nTensor操作\n基本运算\na = torch.tensor([1, 2, 3])\nb = torch.tensor([4, 5, 6])\n \n# 加法、减法、乘法、除法\nc = a + b\nd = a - b\ne = a * b\nf = a / b\n \n# 点积\ndot = torch.dot(a, b)\n \n# 矩阵乘法\nmat_a = torch.randn(2, 3)\nmat_b = torch.randn(3, 4)\nmat_c = torch.mm(mat_a, mat_b)  # 或 mat_a @ mat_b\n统计运算\nx = torch.randn(10)\n \n# 均值、标准差、方差\nmean = x.mean()\nstd = x.std()\nvar = x.var()\n \n# 最大值、最小值\nmax_val = x.max()\nmin_val = x.min()\n \n# 求和\nsum_val = x.sum()\n形状操作\nx = torch.arange(12)\n \n# reshape\nx_reshaped = x.view(3, 4)  # 或 x.reshape(3, 4)\n \n# 转置\nx_transposed = x_reshaped.t()\n \n# squeeze: 去除维度为1的\nx_squeezed = torch.randn(1, 10, 1).squeeze()\n \n# unsqueeze: 增加维度\nx_unsqueezed = x.unsqueeze(0)\nTensor vs NumPy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n特性NumPyTensorGPU支持❌✅自动微分❌✅性能CPUCPU/GPUAPI类似类似互操作易易\n互操作\n# NumPy → Tensor\nnp_array = np.array([1, 2, 3])\ntensor = torch.from_numpy(np_array)\n \n# Tensor → NumPy\narray = tensor.numpy()\n\nAutograd自动微分\nAutograd概述\n\nPyTorch的自动微分引擎\n自动计算梯度\n支持复杂的计算图\n\n关键概念\n1. requires_grad\n\n标记需要计算梯度的Tensor\n默认为False\n通常是模型参数\n\n2. backward()\n\n反向传播，计算梯度\n从loss开始\n沿计算图传播梯度\n\n3. grad\n\n存储梯度值\n在backward()后填充\n用于参数更新\n\n简单示例\n单变量梯度\nimport torch\n \n# 创建需要梯度的Tensor\nx = torch.tensor(2.0, requires_grad=True)\n \n# 定义函数: y = x^2 + 3x + 1\ny = x**2 + 3 * x + 1\n \n# 反向传播\ny.backward()\n \n# 梯度: dy/dx = 2x + 3 = 2*2 + 3 = 7\nprint(x.grad)  # tensor(7.)\n多变量梯度\nx1 = torch.tensor(2.0, requires_grad=True)\nx2 = torch.tensor(3.0, requires_grad=True)\n \n# y = x1^2 + x2^2\ny = x1**2 + x2**2\n \ny.backward()\n \nprint(f&quot;∂y/∂x1 = {x1.grad}&quot;)  # 4.0\nprint(f&quot;∂y/∂x2 = {x2.grad}&quot;)  # 6.0\n训练循环中的梯度\n# 模型参数\nw = torch.tensor([1.0], requires_grad=True)\nb = torch.tensor([0.0], requires_grad=True)\n \n# 输入和目标\nx = torch.tensor([2.0])\ny_true = torch.tensor([5.0])\n \n# 前向传播\ny_pred = w * x + b\n \n# 计算损失\nloss = (y_pred - y_true) ** 2\n \n# 反向传播\nloss.backward()\n \nprint(f&quot;∂loss/∂w = {w.grad}&quot;)  # -4.0\nprint(f&quot;∂loss/∂b = {b.grad}&quot;)  # -2.0\n \n# 参数更新\nlearning_rate = 0.1\nwith torch.no_grad():\n    w -= learning_rate * w.grad\n    b -= learning_rate * b.grad\n \nprint(f&quot;w = {w.data}&quot;)  # 1.4\nprint(f&quot;b = {b.data}&quot;)  # 0.2\n \n# 重要: 更新后清零梯度\nw.grad.zero_()\nb.grad.zero_()\n梯度计算注意事项\n1. 清零梯度\n# 每次backward()前需要清零梯度\noptimizer.zero_grad()\n# 或手动清零\nmodel.zero_grad()\n2. 禁用梯度计算\n# 评估时禁用梯度\nwith torch.no_grad():\n    predictions = model(X)\n \n# 推理模式\nmodel.eval()\n3. 梯度裁剪\n# 防止梯度爆炸\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\nnn.Module模型定义\nnn.Module概述\n\nPyTorch中所有神经网络模型的基类\n提供模型管理和自动微分功能\n必须实现__init__和forward方法\n\n模型定义模板\nimport torch.nn as nn\n \nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 定义层\n \n    def forward(self, x):\n        # 前向传播\n        return output\n线性模型示例\nclass SimpleLinearModel(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        # 定义层\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n \n    def forward(self, x):\n        # 前向传播\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n \n# 创建模型\nmodel = SimpleLinearModel(input_size=10, hidden_size=20, output_size=1)\n \n# 查看模型\nprint(model)\n \n# 参数数量\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f&quot;总参数: {total_params}&quot;)\n查看模型参数\n# 遍历参数\nfor name, param in model.named_parameters():\n    print(f&quot;{name}: {param.shape}&quot;)\n \n# 访问特定层\nfc1_weights = model.fc1.weight\nfc1_bias = model.fc1.bias\n \nprint(fc1_weights.shape)\nprint(fc1_bias.shape)\n模型方法\n# 训练模式\nmodel.train()\n \n# 评估模式\nmodel.eval()\n \n# 移动到GPU\nmodel.to(&#039;cuda&#039;)\n \n# 保存模型参数\ntorch.save(model.state_dict(), &#039;model.pth&#039;)\n \n# 加载模型参数\nmodel.load_state_dict(torch.load(&#039;model.pth&#039;))\n\n常用层\n全连接层（nn.Linear）\n公式\ny = xW^T + b\n\n示例\n# 输入维度10，输出维度5\nlinear = nn.Linear(10, 5)\n \n# 输入 (batch_size=3, input_size=10)\nx = torch.randn(3, 10)\n \n# 输出 (batch_size=3, output_size=5)\ny = linear(x)\n \nprint(y.shape)  # torch.Size([3, 5])\nLSTM层（nn.LSTM）\n参数说明\n\ninput_size: 输入特征维度\nhidden_size: 隐藏状态维度\nnum_layers: LSTM层数\nbatch_first: batch是否在第一维\nbidirectional: 是否双向\ndropout: Dropout比例\n\n输入格式\n\nbatch_first=False: (seq_len, batch, input_size)\nbatch_first=True: (batch, seq_len, input_size)\n\n输出格式\n\noutput: (batch, seq_len, hidden_size)\nh_n: (num_layers, batch, hidden_size)\nc_n: (num_layers, batch, hidden_size)\n\n示例\n# 创建LSTM\nlstm = nn.LSTM(\n    input_size=10,\n    hidden_size=20,\n    num_layers=2,\n    batch_first=True,\n    dropout=0.2\n)\n \n# 输入 (batch=5, seq_len=8, input_size=10)\nx = torch.randn(5, 8, 10)\n \n# 前向传播\noutput, (h_n, c_n) = lstm(x)\n \nprint(f&quot;output: {output.shape}&quot;)  # (5, 8, 20)\nprint(f&quot;h_n: {h_n.shape}&quot;)        # (2, 5, 20)\nprint(f&quot;c_n: {c_n.shape}&quot;)        # (2, 5, 20)\n激活函数\nimport torch.nn as nn\n \nx = torch.randn(5)\n \n# ReLU\nrelu = nn.ReLU()\ny_relu = relu(x)  # max(0, x)\n \n# Sigmoid\nsigmoid = nn.Sigmoid()\ny_sigmoid = sigmoid(x)  # 1 / (1 + exp(-x))\n \n# Tanh\ntanh = nn.Tanh()\ny_tanh = tanh(x)  # (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n \n# Leaky ReLU\nleaky_relu = nn.LeakyReLU(0.01)\ny_leaky = leaky_relu(x)  # max(0.01x, x)\nDropout\n作用\n\n防止过拟合\n随机丢弃部分神经元\n\n示例\ndropout = nn.Dropout(p=0.5)  # 50%的神经元被置零\n \nx = torch.ones(10)\ny = dropout(x)\n \nprint(x)  # tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\nprint(y)  # tensor([2., 0., 2., 0., 2., 0., 0., 2., 0., 2.])  # 约50%为0\nBatchNorm\n作用\n\n加速训练\n提高稳定性\n\n1D BatchNorm\nbatchnorm1d = nn.BatchNorm1d(num_features=10)\n \n# 输入 (batch=5, features=10)\nx = torch.randn(5, 10)\ny = batchnorm1d(x)\n2D BatchNorm（用于CNN）\nbatchnorm2d = nn.BatchNorm2d(num_features=10)\n \n# 输入 (batch=5, channels=10, height=20, width=20)\nx = torch.randn(5, 10, 20, 20)\ny = batchnorm2d(x)\nEmbedding层\n作用\n\n将离散值映射为稠密向量\n用于自然语言处理\n\n示例\n# 词汇表大小10000，嵌入维度100\nembedding = nn.Embedding(num_embeddings=10000, embedding_dim=100)\n \n# 输入 (batch=5, seq_len=10)\nx = torch.randint(0, 10000, (5, 10))\n \n# 输出 (batch=5, seq_len=10, embedding_dim=100)\ny = embedding(x)\n \nprint(y.shape)  # torch.Size([5, 10, 100])\n\n核心知识点总结\nPyTorch简介\n\n✅ 动态计算图\n✅ GPU加速\n✅ 自动微分\n✅ 丰富的API\n\nTensor基础\n\n✅ Tensor创建\n✅ Tensor操作\n✅ Tensor vs NumPy\n✅ 互操作\n\nAutograd\n\n✅ requires_grad\n✅ backward()\n✅ 梯度计算\n✅ 梯度清零和裁剪\n\nnn.Module\n\n✅ 模型定义模板\n✅ forward方法\n✅ 模型参数管理\n✅ 训练/评估模式\n\n常用层\n\n✅ nn.Linear\n✅ nn.LSTM\n✅ 激活函数\n✅ Dropout\n✅ BatchNorm\n\n\n下一步\n继续学习: LSTM模型构建系列"},"quant/qlib/week5/03_LSTM模型构建系列/index":{"slug":"quant/qlib/week5/03_LSTM模型构建系列/index","filePath":"quant/qlib/week5/03_LSTM模型构建系列/index.md","title":"index","links":["04_时序数据处理系列/"],"tags":[],"content":"LSTM模型构建系列 - 架构与实现\n📚 系列概述\n本系列文档涵盖LSTM模型的各种架构、模型定义、超参数配置和变体。\n\n📖 文档列表\n\nLSTM模型架构\n单层LSTM\n多层LSTM\n双向LSTM\nLSTM变体\n超参数选择\n\n\nLSTM模型架构\n完整LSTM模型结构\n输入 (batch, seq_len, input_size)\n    ↓\nLSTM层 (多层)\n    ↓\nDropout层 (防止过拟合)\n    ↓\n全连接层\n    ↓\n输出 (batch, output_size)\n\n参数说明\n输入参数\n\ninput_size: 特征维度\nhidden_size: LSTM隐藏单元数\nnum_layers: LSTM层数\ndropout: Dropout比例\noutput_size: 输出维度\n\n模型参数\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数说明推荐值影响input_size输入特征维度由数据决定不可改变hidden_size隐藏单元数32-128模型容量num_layersLSTM层数1-3模型深度dropoutDropout比例0.1-0.3防止过拟合batch_firstbatch是否在前True数据格式\n\n单层LSTM\n模型定义\nimport torch.nn as nn\n \nclass SingleLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, dropout=0.2):\n        super().__init__()\n        self.hidden_size = hidden_size\n \n        # LSTM层\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            batch_first=True\n        )\n \n        # Dropout层\n        self.dropout = nn.Dropout(dropout)\n \n        # 全连接层\n        self.fc = nn.Linear(hidden_size, output_size)\n \n    def forward(self, x):\n        &quot;&quot;&quot;\n        输入: (batch, seq_len, input_size)\n        输出: (batch, output_size)\n        &quot;&quot;&quot;\n        # 前向传播\n        lstm_out, _ = self.lstm(x)\n \n        # 取最后一个时间步的输出\n        last_output = lstm_out[:, -1, :]\n \n        # Dropout\n        last_output = self.dropout(last_output)\n \n        # 全连接层\n        output = self.fc(last_output)\n \n        return output\n使用示例\n# 创建模型\nmodel = SingleLSTM(\n    input_size=10,\n    hidden_size=64,\n    output_size=1,\n    dropout=0.2\n)\n \n# 输入数据\nx = torch.randn(32, 20, 10)  # (batch=32, seq_len=20, input_size=10)\n \n# 前向传播\noutput = model(x)\n \nprint(output.shape)  # torch.Size([32, 1])\n适用场景\n\n简单时序预测任务\n数据量有限\n快速原型开发\n\n\n多层LSTM\n模型定义\nclass MultiLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout, output_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n \n        # 多层LSTM\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers &gt; 1 else 0\n        )\n \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n \n        # 全连接层\n        self.fc = nn.Linear(hidden_size, output_size)\n \n    def forward(self, x):\n        &quot;&quot;&quot;\n        输入: (batch, seq_len, input_size)\n        输出: (batch, output_size)\n        &quot;&quot;&quot;\n        lstm_out, _ = self.lstm(x)\n        last_output = lstm_out[:, -1, :]\n        last_output = self.dropout(last_output)\n        output = self.fc(last_output)\n        return output\n使用示例\n# 创建模型\nmodel = MultiLSTM(\n    input_size=10,\n    hidden_size=64,\n    num_layers=3,\n    dropout=0.2,\n    output_size=1\n)\n \n# 输入数据\nx = torch.randn(32, 20, 10)\n \n# 前向传播\noutput = model(x)\n \nprint(output.shape)  # torch.Size([32, 1])\n多层LSTM的特点\n优势:\n\n增强模型表达能力\n学习更复杂的特征\n提升模型性能\n\n劣势:\n\n参数量增加\n训练时间增加\n过拟合风险增加\n\n适用场景\n\n复杂时序模式\n数据量充足\n需要更强的表达能力\n\n\n双向LSTM（Bi-LSTM）\n模型定义\nclass BiLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout, output_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n \n        # 双向LSTM\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=True,  # 双向\n            dropout=dropout if num_layers &gt; 1 else 0\n        )\n \n        # 双向输出维度是hidden_size * 2\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_size * 2, output_size)\n \n    def forward(self, x):\n        &quot;&quot;&quot;\n        输入: (batch, seq_len, input_size)\n        输出: (batch, output_size)\n        &quot;&quot;&quot;\n        lstm_out, _ = self.lstm(x)\n        last_output = lstm_out[:, -1, :]\n        last_output = self.dropout(last_output)\n        output = self.fc(last_output)\n        return output\n使用示例\n# 创建模型\nmodel = BiLSTM(\n    input_size=10,\n    hidden_size=64,\n    num_layers=2,\n    dropout=0.2,\n    output_size=1\n)\n \n# 输入数据\nx = torch.randn(32, 20, 10)\n \n# 前向传播\noutput = model(x)\n \nprint(output.shape)  # torch.Size([32, 1])\n双向LSTM的特点\n优势:\n\n同时利用过去和未来信息\n适合需要上下文的任务\n性能通常更好\n\n劣势:\n\n参数量增加一倍\n不能用于实时预测（需要未来数据）\n训练时间增加\n\n适用场景\n\n文本分类、情感分析\n机器翻译\n需要上下文信息的任务\n❌ 不推荐: 实时股票预测\n\n\nLSTM变体\n1. 堆叠LSTM（Stacked LSTM）\n定义\n\n多层LSTM堆叠\n每层学习不同层次的抽象\n\n架构\n输入 → LSTM层1 → LSTM层2 → ... → LSTM层N → 输出\n\n实现\nclass StackedLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super().__init__()\n \n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=0.2\n        )\n \n        self.fc = nn.Linear(hidden_size, output_size)\n \n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        output = self.fc(lstm_out[:, -1, :])\n        return output\n2. 编码器-解码器LSTM\n定义\n\n编码器：将序列编码为固定长度向量\n解码器：从向量生成输出序列\n\n架构\n输入序列 → 编码器LSTM → 上下文向量 → 解码器LSTM → 输出序列\n\n实现\nclass EncoderDecoderLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n \n        # 编码器\n        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n \n        # 解码器\n        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n \n        # 输出层\n        self.fc = nn.Linear(hidden_size, output_size)\n \n    def forward(self, x):\n        # 编码\n        _, (h_n, c_n) = self.encoder(x)\n \n        # 解码\n        decoder_input = h_n[-1].unsqueeze(1).repeat(1, x.size(1), 1)\n        decoder_output, _ = self.decoder(decoder_input, (h_n, c_n))\n \n        # 输出\n        output = self.fc(decoder_output)\n \n        return output\n3. 注意力LSTM（Attention LSTM）\n定义\n\n添加注意力机制\n动态关注重要时间步\n\n实现\nclass AttentionLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n \n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n \n        # 注意力层\n        self.attention = nn.Linear(hidden_size, 1)\n \n        self.fc = nn.Linear(hidden_size, output_size)\n \n    def forward(self, x):\n        # LSTM输出\n        lstm_out, _ = self.lstm(x)\n \n        # 计算注意力权重\n        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n \n        # 加权求和\n        context = torch.sum(attention_weights * lstm_out, dim=1)\n \n        # 输出\n        output = self.fc(context)\n \n        return output\n\n超参数选择\n关键超参数\n1. hidden_size（隐藏单元数）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n任务规模hidden_size说明小规模32-64简单任务，数据量小中等规模64-128一般任务大规模128-256复杂任务，数据量大\n选择原则:\n\n从小开始，逐步增加\n监控过拟合\n考虑计算资源\n\n2. num_layers（LSTM层数）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n模式num_layers说明简单1简单任务中等2-3一般任务复杂3-5复杂任务\n选择原则:\n\n不要过度堆叠\n2-3层通常足够\n超过5层收益递减\n\n3. dropout（Dropout比例）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n场景dropout说明无过拟合0.0-0.1训练集表现好轻微过拟合0.1-0.3轻微过拟合严重过拟合0.3-0.5严重过拟合\n选择原则:\n\n从0.1开始\n根据验证集调整\n不要超过0.5\n\n4. learning_rate（学习率）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n优化器learning_range说明Adam0.0001-0.001推荐默认值SGD0.01-0.1需要momentumRMSprop0.001-0.01RNN专用\n学习率调度:\n# 初始学习率\nlearning_rate = 0.001\n \n# 学习率衰减\nscheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=10,  # 每10个epoch\n    gamma=0.1      # 学习率乘0.1\n)\n \n# 余弦退火\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=50  # 总epoch数\n)\n5. batch_size（批大小）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n硬件batch_size说明CPU16-32内存有限单GPU32-128GPU内存多GPU64-256并行计算\n选择原则:\n\n2的幂次方（32, 64, 128）\n根据GPU内存调整\n越大越稳定，但越慢\n\n6. seq_len（序列长度）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n预测目标seq_len说明短期5-10日内交易中期20-60几天到几周长期60-120几个月\n选择原则:\n\n基于业务逻辑\n通过实验确定\n考虑计算成本\n\n超参数搜索\n网格搜索\nfrom itertools import product\n \n# 参数网格\nparam_grid = {\n    &#039;hidden_size&#039;: [32, 64, 128],\n    &#039;num_layers&#039;: [1, 2, 3],\n    &#039;dropout&#039;: [0.1, 0.2, 0.3],\n    &#039;learning_rate&#039;: [0.0001, 0.001, 0.01],\n    &#039;batch_size&#039;: [16, 32, 64]\n}\n \n# 生成所有组合\nparam_combinations = list(product(\n    param_grid[&#039;hidden_size&#039;],\n    param_grid[&#039;num_layers&#039;],\n    param_grid[&#039;dropout&#039;],\n    param_grid[&#039;learning_rate&#039;],\n    param_grid[&#039;batch_size&#039;]\n))\n随机搜索\nimport random\n \n# 随机搜索n次\nn_trials = 20\n \nfor _ in range(n_trials):\n    # 随机选择参数\n    hidden_size = random.choice([32, 64, 128])\n    num_layers = random.choice([1, 2, 3])\n    dropout = random.uniform(0.1, 0.3)\n    learning_rate = random.choice([0.0001, 0.001, 0.01])\n    batch_size = random.choice([16, 32, 64])\n \n    # 训练和评估\n    # ...\n贝叶斯优化\nfrom skopt import gp_minimize\n \n# 定义搜索空间\nspace = [\n    (32, 256),           # hidden_size\n    (1, 4),              # num_layers\n    (0.1, 0.5),          # dropout\n    (0.0001, 0.01, &#039;log&#039;),  # learning_rate\n    (16, 128)            # batch_size\n]\n \n# 定义目标函数\ndef objective(params):\n    hidden_size, num_layers, dropout, learning_rate, batch_size = params\n \n    # 训练模型\n    model = MultiLSTM(\n        input_size=10,\n        hidden_size=hidden_size,\n        num_layers=int(num_layers),\n        dropout=dropout,\n        output_size=1\n    )\n \n    # 返回验证损失\n    return val_loss\n \n# 优化\nresult = gp_minimize(objective, space, n_calls=50)\n\n核心知识点总结\nLSTM模型架构\n\n✅ 完整LSTM结构\n✅ 参数说明\n✅ 数据流\n\n单层LSTM\n\n✅ 模型定义\n✅ 使用示例\n✅ 适用场景\n\n多层LSTM\n\n✅ 模型定义\n✅ 优劣势分析\n✅ 适用场景\n\n双向LSTM\n\n✅ 模型定义\n✅ 优劣势分析\n✅ 适用场景\n\nLSTM变体\n\n✅ 堆叠LSTM\n✅ 编码器-解码器LSTM\n✅ 注意力LSTM\n\n超参数选择\n\n✅ 关键超参数\n✅ 推荐值\n✅ 超参数搜索方法\n\n\n下一步\n继续学习: 时序数据处理系列"},"quant/qlib/week5/04_时序数据处理系列/index":{"slug":"quant/qlib/week5/04_时序数据处理系列/index","filePath":"quant/qlib/week5/04_时序数据处理系列/index.md","title":"index","links":["05_模型训练优化系列/"],"tags":[],"content":"时序数据处理系列 - 预处理与Dataset\n📚 系列概述\n本系列文档涵盖时序数据的构造、标准化、数据划分、PyTorch Dataset和DataLoader。\n\n📖 文档列表\n\n滑动窗口方法\n数据划分\n特征标准化\nPyTorch Dataset\nDataLoader\n\n\n滑动窗口方法\n原理\n\n使用过去N天的数据预测下一天\n窗口滑动，生成多个样本\n是构造时序训练数据的标准方法\n\n示例\n原始数据\nDay 1: [0.1, 0.2, 0.3]\nDay 2: [0.2, 0.3, 0.4]\nDay 3: [0.3, 0.4, 0.5]\nDay 4: [0.4, 0.5, 0.6]\nDay 5: [0.5, 0.6, 0.7]\n\n序列长度 = 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n样本输入 (X)目标 (y)1Day 1-3Day 42Day 2-4Day 5\nPython实现\nimport numpy as np\n \ndef create_sequences(data, seq_len, target_idx=0):\n    &quot;&quot;&quot;\n    滑动窗口构造时序序列\n \n    参数:\n        data: 原始数据 (n_samples, n_features)\n        seq_len: 序列长度\n        target_idx: 目标特征索引\n \n    返回:\n        X: 输入序列 (n_samples-seq_len, seq_len, n_features)\n        y: 目标值 (n_samples-seq_len,)\n    &quot;&quot;&quot;\n    X, y = [], []\n \n    for i in range(len(data) - seq_len):\n        X.append(data[i:i+seq_len])\n        y.append(data[i+seq_len, target_idx])\n \n    return np.array(X), np.array(y)\n \n# 示例\ndata = np.random.randn(100, 10)  # 100天，10个特征\nX, y = create_sequences(data, seq_len=20, target_idx=0)\n \nprint(X.shape)  # (80, 20, 10)\nprint(y.shape)  # (80,)\n高级滑动窗口\ndef create_sequences_multi_step(data, seq_len, target_len, target_idx=0):\n    &quot;&quot;&quot;\n    多步预测滑动窗口\n \n    参数:\n        data: 原始数据\n        seq_len: 输入序列长度\n        target_len: 预测步数\n        target_idx: 目标特征索引\n \n    返回:\n        X: (n_samples-seq_len-target_len, seq_len, n_features)\n        y: (n_samples-seq_len-target_len, target_len)\n    &quot;&quot;&quot;\n    X, y = [], []\n \n    for i in range(len(data) - seq_len - target_len):\n        X.append(data[i:i+seq_len])\n        y.append(data[i+seq_len:i+seq_len+target_len, target_idx])\n \n    return np.array(X), np.array(y)\n \n# 示例：预测未来5天\nX, y = create_sequences_multi_step(data, seq_len=20, target_len=5)\nprint(X.shape)  # (75, 20, 10)\nprint(y.shape)  # (75, 5)\n滑动窗口选择建议\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n预测目标seq_len说明短期预测5-10日内交易中期预测20-60几天到几周长期预测60-120几个月\n\n数据划分\n时间序列划分原则\n\n按时间顺序划分\n不能随机划分\n训练集 &lt; 验证集 &lt; 测试集\n\n划分方法\ndef time_series_split(data, train_ratio=0.7, val_ratio=0.15):\n    &quot;&quot;&quot;\n    时间序列数据划分\n \n    参数:\n        data: 完整数据\n        train_ratio: 训练集比例\n        val_ratio: 验证集比例\n \n    返回:\n        train, val, test\n    &quot;&quot;&quot;\n    n = len(data)\n    train_end = int(n * train_ratio)\n    val_end = int(n * (train_ratio + val_ratio))\n \n    train = data[:train_end]\n    val = data[train_end:val_end]\n    test = data[val_end:]\n \n    return train, val, test\n \n# 示例\ndata = np.random.randn(1000, 10)\ntrain, val, test = time_series_split(data)\n \nprint(f&quot;Train: {train.shape}&quot;)  # (700, 10)\nprint(f&quot;Val: {val.shape}&quot;)      # (150, 10)\nprint(f&quot;Test: {test.shape}&quot;)     # (150, 10)\n滚动窗口验证\nfrom sklearn.model_selection import TimeSeriesSplit\n \ntscv = TimeSeriesSplit(n_splits=5)\n \nfor fold, (train_index, test_index) in enumerate(tscv.split(X)):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n \n    print(f&quot;Fold {fold+1}:&quot;)\n    print(f&quot;  Train: {X_train.shape}&quot;)\n    print(f&quot;  Test: {X_test.shape}&quot;)\nWalk-Forward验证\ndef walk_forward_validation(data, initial_train_size, step_size):\n    &quot;&quot;&quot;\n    Walk-Forward验证\n \n    参数:\n        data: 完整数据\n        initial_train_size: 初始训练集大小\n        step_size: 每次前进步数\n    &quot;&quot;&quot;\n    splits = []\n    n = len(data)\n \n    train_end = initial_train_size\n \n    while train_end + step_size &lt; n:\n        test_start = train_end\n        test_end = min(test_start + step_size, n)\n \n        splits.append((\n            data[:train_end],\n            data[test_start:test_end]\n        ))\n \n        train_end += step_size\n \n    return splits\n \n# 示例\nsplits = walk_forward_validation(data, initial_train_size=500, step_size=100)\n \nfor i, (train, test) in enumerate(splits):\n    print(f&quot;Fold {i+1}: Train {train.shape}, Test {test.shape}&quot;)\n\n特征标准化\n标准化方法\n1. Z-score标准化（StandardScaler）\nfrom sklearn.preprocessing import StandardScaler\n \n# 拟合训练集\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))\n \n# 转换验证集和测试集\nX_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1]))\nX_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1]))\n \n# 恢复形状\nX_train_scaled = X_train_scaled.reshape(X_train.shape)\nX_val_scaled = X_val_scaled.reshape(X_val.shape)\nX_test_scaled = X_test_scaled.reshape(X_test.shape)\n2. Min-Max标准化\nfrom sklearn.preprocessing import MinMaxScaler\n \nscaler = MinMaxScaler(feature_range=(0, 1))\nX_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))\n \nX_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1]))\nX_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1]))\n \n# 恢复形状\nX_train_scaled = X_train_scaled.reshape(X_train.shape)\nX_val_scaled = X_val_scaled.reshape(X_val.shape)\nX_test_scaled = X_test_scaled.reshape(X_test.shape)\n3. RobustScaler\nfrom sklearn.preprocessing import RobustScaler\n \n# 对异常值更鲁棒\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))\n \nX_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1]))\nX_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1]))\n \n# 恢复形状\nX_train_scaled = X_train_scaled.reshape(X_train.shape)\nX_val_scaled = X_val_scaled.reshape(X_val.shape)\nX_test_scaled = X_test_scaled.reshape(X_test.shape)\n滚动标准化\ndef rolling_standardize(data, window=20):\n    &quot;&quot;&quot;\n    滚动窗口标准化\n \n    参数:\n        data: 原始数据 (n_samples, n_features)\n        window: 滚动窗口大小\n \n    返回:\n        scaled: 标准化后的数据\n    &quot;&quot;&quot;\n    scaled = np.zeros_like(data)\n \n    for i in range(len(data)):\n        if i &lt; window:\n            # 初期使用累积数据\n            mean = data[:i+1].mean(axis=0)\n            std = data[:i+1].std(axis=0)\n        else:\n            # 使用滚动窗口\n            mean = data[i-window:i].mean(axis=0)\n            std = data[i-window:i].std(axis=0)\n \n        scaled[i] = (data[i] - mean) / (std + 1e-8)\n \n    return scaled\n \n# 示例\nX_train_scaled = rolling_standardize(X_train, window=20)\n标准化注意事项\n1. 只用训练集拟合\n# ❌ 错误：用所有数据拟合\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(all_data)\n \n# ✅ 正确：只用训练集拟合\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)\n2. 反标准化预测结果\n# 预测\npredictions = model(X_test_scaled)\n \n# 反标准化\npredictions_original = scaler.inverse_transform(predictions)\n\nPyTorch Dataset\n自定义Dataset\nfrom torch.utils.data import Dataset\nimport torch\n \nclass TimeSeriesDataset(Dataset):\n    def __init__(self, X, y):\n        &quot;&quot;&quot;\n        参数:\n            X: 输入数据 (n_samples, seq_len, n_features)\n            y: 目标值 (n_samples,)\n        &quot;&quot;&quot;\n        self.X = torch.FloatTensor(X)\n        self.y = torch.FloatTensor(y)\n \n    def __len__(self):\n        &quot;&quot;&quot;返回样本数量&quot;&quot;&quot;\n        return len(self.X)\n \n    def __getitem__(self, idx):\n        &quot;&quot;&quot;\n        获取单个样本\n \n        返回:\n            X: (seq_len, n_features)\n            y: scalar\n        &quot;&quot;&quot;\n        return self.X[idx], self.y[idx]\n创建Dataset\n# 创建Dataset\ntrain_dataset = TimeSeriesDataset(X_train, y_train)\nval_dataset = TimeSeriesDataset(X_val, y_val)\ntest_dataset = TimeSeriesDataset(X_test, y_test)\n \n# 查看大小\nprint(f&quot;Train dataset size: {len(train_dataset)}&quot;)\nprint(f&quot;Val dataset size: {len(val_dataset)}&quot;)\nprint(f&quot;Test dataset size: {len(test_dataset)}&quot;)\n \n# 获取单个样本\nX_sample, y_sample = train_dataset[0]\nprint(f&quot;Sample X shape: {X_sample.shape}&quot;)\nprint(f&quot;Sample y: {y_sample}&quot;)\n高级Dataset\nclass AdvancedTimeSeriesDataset(Dataset):\n    def __init__(self, X, y, transform=None):\n        self.X = torch.FloatTensor(X)\n        self.y = torch.FloatTensor(y)\n        self.transform = transform\n \n    def __len__(self):\n        return len(self.X)\n \n    def __getitem__(self, idx):\n        X, y = self.X[idx], self.y[idx]\n \n        # 应用变换\n        if self.transform:\n            X = self.transform(X)\n \n        return X, y\n \n# 定义变换\ndef add_noise(x, noise_level=0.01):\n    &quot;&quot;&quot;添加噪声&quot;&quot;&quot;\n    noise = torch.randn_like(x) * noise_level\n    return x + noise\n \n# 创建Dataset\ntrain_dataset = AdvancedTimeSeriesDataset(\n    X_train,\n    y_train,\n    transform=lambda x: add_noise(x, 0.01)\n)\n\nDataLoader\n创建DataLoader\nfrom torch.utils.data import DataLoader\n \n# 创建DataLoader\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    shuffle=True,  # 训练集打乱\n    num_workers=4,\n    pin_memory=True  # 加速GPU传输\n)\n \nval_loader = DataLoader(\n    val_dataset,\n    batch_size=32,\n    shuffle=False,  # 验证集不打乱\n    num_workers=4,\n    pin_memory=True\n)\n \ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=32,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True\n)\n使用DataLoader\n# 训练循环\nfor epoch in range(num_epochs):\n    model.train()\n \n    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n        # 前向传播\n        predictions = model(X_batch)\n        loss = criterion(predictions.squeeze(), y_batch)\n \n        # 反向传播\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n \n        # 打印进度\n        if batch_idx % 100 == 0:\n            print(f&quot;Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}&quot;)\n \n    # 验证\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            predictions = model(X_batch)\n            loss = criterion(predictions.squeeze(), y_batch)\n            val_loss += loss.item()\n \n    val_loss /= len(val_loader)\n    print(f&quot;Epoch {epoch}, Val Loss: {val_loss:.4f}&quot;)\nDataLoader参数\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n参数说明推荐值batch_size批大小32, 64, 128shuffle是否打乱训练集=True，验证/测试=Falsenum_workers加载进程数4-8pin_memory锁页内存True（GPU训练）drop_last丢弃不完整batchFalse\n动态批大小\n# 根据序列长度动态调整\nclass DynamicBatchSampler:\n    def __init__(self, dataset, max_batch_size=32):\n        self.dataset = dataset\n        self.max_batch_size = max_batch_size\n \n    def __iter__(self):\n        batch = []\n        for idx in range(len(self.dataset)):\n            batch.append(idx)\n            if len(batch) &gt;= self.max_batch_size:\n                yield batch\n                batch = []\n \n        if batch:\n            yield batch\n \n# 使用\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_sampler=DynamicBatchSampler(train_dataset, max_batch_size=32),\n    collate_fn=lambda batch: default_collate([train_dataset[i] for i in batch])\n)\n\n数据处理流程\n完整流程\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\n \n# 1. 加载数据\ndata = np.load(&#039;stock_data.npy&#039;)  # (n_days, n_features)\n \n# 2. 构造序列\ndef create_sequences(data, seq_len, target_idx=0):\n    X, y = [], []\n    for i in range(len(data) - seq_len):\n        X.append(data[i:i+seq_len])\n        y.append(data[i+seq_len, target_idx])\n    return np.array(X), np.array(y)\n \nX, y = create_sequences(data, seq_len=20)\n \n# 3. 划分数据\ntrain_size = int(0.7 * len(X))\nval_size = int(0.15 * len(X))\n \nX_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\ny_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n \n# 4. 标准化\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\nX_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\nX_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n \n# 5. 创建Dataset\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.FloatTensor(X)\n        self.y = torch.FloatTensor(y)\n \n    def __len__(self):\n        return len(self.X)\n \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n \ntrain_dataset = TimeSeriesDataset(X_train_scaled, y_train)\nval_dataset = TimeSeriesDataset(X_val_scaled, y_val)\ntest_dataset = TimeSeriesDataset(X_test_scaled, y_test)\n \n# 6. 创建DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n \nprint(f&quot;Train: {len(train_dataset)} samples&quot;)\nprint(f&quot;Val: {len(val_dataset)} samples&quot;)\nprint(f&quot;Test: {len(test_dataset)} samples&quot;)\n\n核心知识点总结\n滑动窗口\n\n✅ 基本原理\n✅ 单步预测\n✅ 多步预测\n✅ 序列长度选择\n\n数据划分\n\n✅ 时间序列划分原则\n✅ 滚动窗口验证\n✅ Walk-Forward验证\n\n特征标准化\n\n✅ Z-score标准化\n✅ Min-Max标准化\n✅ RobustScaler\n✅ 滚动标准化\n\nDataset\n\n✅ 自定义Dataset\n✅ 高级Dataset\n✅ 数据变换\n\nDataLoader\n\n✅ 创建DataLoader\n✅ 参数配置\n✅ 使用示例\n\n\n下一步\n继续学习: 模型训练优化系列"},"quant/qlib/week5/05_模型训练优化系列/index":{"slug":"quant/qlib/week5/05_模型训练优化系列/index","filePath":"quant/qlib/week5/05_模型训练优化系列/index.md","title":"index","links":["06_实战应用系列/"],"tags":[],"content":"模型训练优化系列 - 训练与优化策略\n📚 系列概述\n本系列文档涵盖损失函数、优化器、训练循环、早停策略、正则化方法和超参数优化。\n\n📖 文档列表\n\n损失函数\n优化器\n训练循环\n早停策略\n正则化\n学习率调度\n\n\n损失函数\n回归任务损失函数\n1. MSE（Mean Squared Error）\nimport torch.nn as nn\n \ncriterion = nn.MSELoss()\n \npredictions = torch.randn(10, 1)\ntargets = torch.randn(10, 1)\n \nloss = criterion(predictions, targets)\n特点:\n\n对大误差敏感\n计算简单\n常用\n\n适用场景:\n\n回归任务\n异常值较少的数据\n\n2. MAE（Mean Absolute Error）\ncriterion = nn.L1Loss()\n \nloss = criterion(predictions, targets)\n特点:\n\n对异常值鲁棒\n梯度恒定\n不常用\n\n适用场景:\n\n异常值较多\n需要鲁棒性\n\n3. Smooth L1 Loss\ncriterion = nn.SmoothL1Loss()\n \nloss = criterion(predictions, targets)\n特点:\n\nMSE和MAE的折中\n对异常值鲁棒\n推荐使用\n\n适用场景:\n\n通用回归任务\n平衡鲁棒性和稳定性\n\n自定义损失函数\ndef custom_loss(predictions, targets, model, lambda_l1=0.01, lambda_l2=0.01):\n    &quot;&quot;&quot;\n    自定义损失函数\n \n    参数:\n        predictions: 模型预测\n        targets: 真实值\n        model: 模型\n        lambda_l1: L1正则化系数\n        lambda_l2: L2正则化系数\n    &quot;&quot;&quot;\n    # MSE\n    mse = torch.mean((predictions - targets) ** 2)\n \n    # L1正则化\n    l1_reg = sum(p.abs().sum() for p in model.parameters())\n \n    # L2正则化\n    l2_reg = sum(p.pow(2).sum() for p in model.parameters())\n \n    # 总损失\n    total_loss = mse + lambda_l1 * l1_reg + lambda_l2 * l2_reg\n \n    return total_loss\n损失函数选择建议\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n场景推荐损失函数通用回归MSE异常值多MAE平衡选择Smooth L1需要正则化自定义损失\n\n优化器\n常用优化器\n1. SGD（随机梯度下降）\noptimizer = torch.optim.SGD(\n    model.parameters(),\n    lr=0.01,\n    momentum=0.9,\n    weight_decay=1e-4  # L2正则化\n)\n特点:\n\n简单稳定\n收敛慢\n需要调参\n\n适用场景:\n\n数据量大\n需要稳定训练\n\n2. Adam（推荐）\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=0.001,\n    betas=(0.9, 0.999),\n    weight_decay=1e-4\n)\n特点:\n\n自适应学习率\n收敛快\n默认选择\n\n适用场景:\n\n通用场景\n快速迭代\n\n3. RMSprop\noptimizer = torch.optim.RMSprop(\n    model.parameters(),\n    lr=0.001,\n    alpha=0.99\n)\n特点:\n\n适合RNN\n自适应学习率\n\n适用场景:\n\nRNN/LSTM\n时序预测\n\n优化器对比\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n优化器适用场景优点缺点推荐学习率SGD数据量大简单稳定收敛慢0.01-0.1Adam通用收敛快可能过拟合0.0001-0.001RMSpropRNN适合序列参数调整0.001-0.01\n优化器选择建议\n选择Adam:\n\n默认选择\n快速迭代\n通用场景\n\n选择SGD:\n\n数据量大\n需要稳定\n研究论文\n\n选择RMSprop:\n\nRNN/LSTM\n时序预测\n\n\n训练循环\n完整训练循环\nimport copy\nfrom tqdm import tqdm\n \ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50):\n    &quot;&quot;&quot;\n    训练模型\n \n    参数:\n        model: 模型\n        train_loader: 训练数据加载器\n        val_loader: 验证数据加载器\n        criterion: 损失函数\n        optimizer: 优化器\n        num_epochs: 训练轮数\n \n    返回:\n        model: 最佳模型\n        train_losses: 训练损失\n        val_losses: 验证损失\n    &quot;&quot;&quot;\n \n    # 记录损失\n    train_losses = []\n    val_losses = []\n \n    # 最佳模型\n    best_model = None\n    best_val_loss = float(&#039;inf&#039;)\n \n    for epoch in range(num_epochs):\n        # 训练阶段\n        model.train()\n        train_loss = 0.0\n \n        # 使用进度条\n        pbar = tqdm(train_loader, desc=f&#039;Epoch {epoch+1}/{num_epochs}&#039;)\n \n        for X_batch, y_batch in pbar:\n            # 前向传播\n            predictions = model(X_batch)\n            loss = criterion(predictions.squeeze(), y_batch)\n \n            # 反向传播\n            optimizer.zero_grad()\n            loss.backward()\n \n            # 梯度裁剪\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n \n            # 参数更新\n            optimizer.step()\n \n            train_loss += loss.item()\n \n            # 更新进度条\n            pbar.set_postfix({&#039;loss&#039;: loss.item()})\n \n        train_loss /= len(train_loader)\n        train_losses.append(train_loss)\n \n        # 验证阶段\n        model.eval()\n        val_loss = 0.0\n \n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                predictions = model(X_batch)\n                loss = criterion(predictions.squeeze(), y_batch)\n                val_loss += loss.item()\n \n        val_loss /= len(val_loader)\n        val_losses.append(val_loss)\n \n        # 保存最佳模型\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            best_model = copy.deepcopy(model.state_dict())\n \n        # 打印进度\n        print(f&quot;Epoch {epoch+1}/{num_epochs}&quot;)\n        print(f&quot;  Train Loss: {train_loss:.4f}&quot;)\n        print(f&quot;  Val Loss: {val_loss:.4f}&quot;)\n \n    # 加载最佳模型\n    model.load_state_dict(best_model)\n \n    return model, train_losses, val_losses\n训练循环示例\n# 定义模型\nmodel = LSTMModel(\n    input_size=10,\n    hidden_size=64,\n    num_layers=2,\n    output_size=1\n)\n \n# 定义损失函数和优化器\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n \n# 训练\nmodel, train_losses, val_losses = train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    criterion=criterion,\n    optimizer=optimizer,\n    num_epochs=50\n)\n\n早停策略（Early Stopping）\n原理\n\n监控验证集损失\n连续N个epoch不改善则停止\n防止过拟合\n\n实现\nclass EarlyStopping:\n    def __init__(self, patience=7, min_delta=0, verbose=True):\n        &quot;&quot;&quot;\n        早停策略\n \n        参数:\n            patience: 容忍不改善的epoch数\n            min_delta: 最小改善幅度\n            verbose: 是否打印信息\n        &quot;&quot;&quot;\n        self.patience = patience\n        self.min_delta = min_delta\n        self.verbose = verbose\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.best_model = None\n \n    def __call__(self, val_loss, model):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.save_checkpoint(val_loss, model)\n        elif val_loss &gt; self.best_loss - self.min_delta:\n            self.counter += 1\n            if self.verbose:\n                print(f&#039;EarlyStopping counter: {self.counter} out of {self.patience}&#039;)\n            if self.counter &gt;= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n \n    def save_checkpoint(self, val_loss, model):\n        &quot;&quot;&quot;保存最佳模型&quot;&quot;&quot;\n        if self.verbose:\n            print(f&#039;Validation loss decreased ({self.best_loss:.6f} --&gt; {val_loss:.6f}). Saving model...&#039;)\n        self.best_model = copy.deepcopy(model.state_dict())\n \n    def load_best_model(self, model):\n        &quot;&quot;&quot;加载最佳模型&quot;&quot;&quot;\n        if self.best_model is not None:\n            model.load_state_dict(self.best_model)\n使用早停\n# 创建早停\nearly_stopping = EarlyStopping(patience=10, min_delta=0.0001, verbose=True)\n \n# 训练循环\nfor epoch in range(num_epochs):\n    # 训练\n    train_loss = train(model, train_loader, criterion, optimizer)\n \n    # 验证\n    val_loss = validate(model, val_loader, criterion)\n \n    # 检查早停\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(&quot;Early stopping!&quot;)\n        break\n \n# 加载最佳模型\nearly_stopping.load_best_model(model)\n\n正则化\n1. L1/L2正则化\ndef l1_regularization(model, lambda_l1=0.01):\n    &quot;&quot;&quot;\n    L1正则化\n \n    参数:\n        model: 模型\n        lambda_l1: L1系数\n \n    返回:\n        L1损失\n    &quot;&quot;&quot;\n    l1_loss = 0\n    for param in model.parameters():\n        l1_loss += torch.sum(torch.abs(param))\n    return lambda_l1 * l1_loss\n \ndef l2_regularization(model, lambda_l2=0.01):\n    &quot;&quot;&quot;\n    L2正则化\n \n    参数:\n        model: 模型\n        lambda_l2: L2系数\n \n    返回:\n        L2损失\n    &quot;&quot;&quot;\n    l2_loss = 0\n    for param in model.parameters():\n        l2_loss += torch.sum(param ** 2)\n    return lambda_l2 * l2_loss\n \n# 使用\nloss = criterion(predictions, targets)\nloss += l1_regularization(model, lambda_l1=0.01)\nloss += l2_regularization(model, lambda_l2=0.01)\n2. Dropout\nclass LSTMModelWithDropout(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout=0.2):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.dropout = nn.Dropout(dropout)  # Dropout层\n        self.fc = nn.Linear(hidden_size, 1)\n \n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        lstm_out = self.dropout(lstm_out)\n        output = self.fc(lstm_out[:, -1, :])\n        return output\n3. 批量归一化（BatchNorm）\nclass LSTMModelWithBN(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.bn = nn.BatchNorm1d(hidden_size)  # BatchNorm\n        self.fc = nn.Linear(hidden_size, 1)\n \n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        lstm_out = self.bn(lstm_out[:, -1, :])\n        output = self.fc(lstm_out)\n        return output\n4. 梯度裁剪\n# 反向传播后\nloss.backward()\n \n# 梯度裁剪\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n \n# 或梯度裁剪\ntorch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)\n\n学习率调度\n学习率衰减\n1. StepLR\nscheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=10,  # 每10个epoch\n    gamma=0.1      # 学习率乘0.1\n)\n \n# 训练循环\nfor epoch in range(num_epochs):\n    train(model, train_loader, optimizer, criterion)\n    scheduler.step()  # 更新学习率\n2. ReduceLROnPlateau\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode=&#039;min&#039;,      # 监控指标越小越好\n    factor=0.1,      # 学习率乘0.1\n    patience=5,      # 容忍5个epoch不改善\n    verbose=True\n)\n \n# 训练循环\nfor epoch in range(num_epochs):\n    train_loss = train(model, train_loader, optimizer, criterion)\n    val_loss = validate(model, val_loader, criterion)\n    scheduler.step(val_loss)  # 基于验证损失调整\n3. CosineAnnealingLR\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=50,        # 总epoch数\n    eta_min=1e-6     # 最小学习率\n)\n \n# 训练循环\nfor epoch in range(num_epochs):\n    train(model, train_loader, optimizer, criterion)\n    scheduler.step()\n4. Warmup\nfrom torch.optim.lr_scheduler import LambdaLR\n \ndef warmup_lambda(epoch, warmup_epochs=10):\n    &quot;&quot;&quot;Warmup学习率调度&quot;&quot;&quot;\n    if epoch &lt; warmup_epochs:\n        return epoch / warmup_epochs\n    return 1.0\n \nscheduler = LambdaLR(\n    optimizer,\n    lr_lambda=lambda epoch: warmup_lambda(epoch, warmup_epochs=10)\n)\n学习率可视化\nimport matplotlib.pyplot as plt\n \n# 记录学习率\nlearning_rates = []\n \nfor epoch in range(num_epochs):\n    # 训练\n    train(model, train_loader, optimizer, criterion)\n \n    # 记录学习率\n    learning_rates.append(optimizer.param_groups[0][&#039;lr&#039;])\n \n    # 更新学习率\n    scheduler.step()\n \n# 绘制学习率曲线\nplt.figure(figsize=(10, 6))\nplt.plot(range(num_epochs), learning_rates)\nplt.xlabel(&#039;Epoch&#039;)\nplt.ylabel(&#039;Learning Rate&#039;)\nplt.title(&#039;Learning Rate Schedule&#039;)\nplt.show()\n\n训练技巧\n1. 混合精度训练\nfrom torch.cuda.amp import autocast, GradScaler\n \nscaler = GradScaler()\n \nfor X_batch, y_batch in train_loader:\n    optimizer.zero_grad()\n \n    with autocast():\n        predictions = model(X_batch)\n        loss = criterion(predictions.squeeze(), y_batch)\n \n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n2. 梯度累积\naccumulation_steps = 4\n \nfor i, (X_batch, y_batch) in enumerate(train_loader):\n    predictions = model(X_batch)\n    loss = criterion(predictions.squeeze(), y_batch)\n \n    loss = loss / accumulation_steps\n    loss.backward()\n \n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n3. 模型并行\n# 多GPU训练\nif torch.cuda.device_count() &gt; 1:\n    model = nn.DataParallel(model)\n \nmodel.to(&#039;cuda&#039;)\n\n核心知识点总结\n损失函数\n\n✅ MSE、MAE、Smooth L1\n✅ 自定义损失函数\n✅ 选择建议\n\n优化器\n\n✅ SGD、Adam、RMSprop\n✅ 优化器对比\n✅ 选择建议\n\n训练循环\n\n✅ 完整训练循环\n✅ 进度条\n✅ 梯度裁剪\n\n早停策略\n\n✅ EarlyStopping实现\n✅ 使用方法\n✅ 模型保存\n\n正则化\n\n✅ L1/L2正则化\n✅ Dropout\n✅ BatchNorm\n✅ 梯度裁剪\n\n学习率调度\n\n✅ StepLR\n✅ ReduceLROnPlateau\n✅ CosineAnnealingLR\n✅ Warmup\n\n\n下一步\n继续学习: 实战应用系列"},"quant/qlib/week5/06_实战应用系列/index":{"slug":"quant/qlib/week5/06_实战应用系列/index","filePath":"quant/qlib/week5/06_实战应用系列/index.md","title":"index","links":[],"tags":[],"content":"实战应用系列 - 案例与最佳实践\n📚 系列概述\n本系列文档涵盖完整的LSTM预测流程、超参数调优、模型保存与加载、评估指标和最佳实践。\n\n📖 文档列表\n\n完整预测流程\n超参数调优\n模型保存与加载\n评估指标\nLSTM vs LightGBM\n最佳实践\n\n\n完整预测流程\n步骤1: 数据准备\nimport pandas as pd\nimport numpy as np\n \n# 加载数据\ndata = pd.read_csv(&#039;stock_prices.csv&#039;)\n \n# 特征工程\nfeatures = [&#039;close&#039;, &#039;volume&#039;, &#039;ma5&#039;, &#039;ma20&#039;, &#039;rsi&#039;]\ndata = data[features].values\n \n# 标准化\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n \n# 构造序列\ndef create_sequences(data, seq_len, target_idx=0):\n    X, y = [], []\n    for i in range(len(data) - seq_len):\n        X.append(data[i:i+seq_len])\n        y.append(data[i+seq_len, target_idx])\n    return np.array(X), np.array(y)\n \nseq_len = 20\nX, y = create_sequences(data_scaled, seq_len)\n \n# 划分数据\ntrain_size = int(0.7 * len(X))\nval_size = int(0.15 * len(X))\n \nX_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\ny_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n \nprint(f&quot;Train: {X_train.shape}&quot;)\nprint(f&quot;Val: {X_val.shape}&quot;)\nprint(f&quot;Test: {X_test.shape}&quot;)\n步骤2: 模型定义\nimport torch\nimport torch.nn as nn\n \nclass StockPredictionLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers &gt; 1 else 0\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_size, 1)\n \n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        last_output = lstm_out[:, -1, :]\n        last_output = self.dropout(last_output)\n        output = self.fc(last_output)\n        return output\n \n# 创建模型\nmodel = StockPredictionLSTM(\n    input_size=X_train.shape[2],\n    hidden_size=64,\n    num_layers=2,\n    dropout=0.2\n)\n步骤3: Dataset和DataLoader\nfrom torch.utils.data import Dataset, DataLoader\n \nclass StockDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.FloatTensor(X)\n        self.y = torch.FloatTensor(y)\n \n    def __len__(self):\n        return len(self.X)\n \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n \n# 创建Dataset和DataLoader\ntrain_dataset = StockDataset(X_train, y_train)\nval_dataset = StockDataset(X_val, y_val)\ntest_dataset = StockDataset(X_test, y_test)\n \ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n步骤4: 训练\nimport copy\n \n# 定义损失函数和优化器\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n \n# 训练\nnum_epochs = 50\nbest_val_loss = float(&#039;inf&#039;)\nbest_model = None\n \nfor epoch in range(num_epochs):\n    # 训练\n    model.train()\n    train_loss = 0.0\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        predictions = model(X_batch)\n        loss = criterion(predictions.squeeze(), y_batch)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        train_loss += loss.item()\n \n    train_loss /= len(train_loader)\n \n    # 验证\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            predictions = model(X_batch)\n            loss = criterion(predictions.squeeze(), y_batch)\n            val_loss += loss.item()\n \n    val_loss /= len(val_loader)\n \n    print(f&quot;Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}&quot;)\n \n    # 保存最佳模型\n    if val_loss &lt; best_val_loss:\n        best_val_loss = val_loss\n        best_model = copy.deepcopy(model.state_dict())\n \n# 加载最佳模型\nmodel.load_state_dict(best_model)\n步骤5: 评估\n# 测试\nmodel.eval()\npredictions = []\nactuals = []\n \nwith torch.no_grad():\n    for X_batch, y_batch in test_loader:\n        pred = model(X_batch)\n        predictions.extend(pred.squeeze().numpy())\n        actuals.extend(y_batch.numpy())\n \n# 反标准化\npredictions = scaler.inverse_transform(\n    np.column_stack([predictions, np.zeros((len(predictions), X_train.shape[2]-1))])\n)[:, 0]\n \nactuals = scaler.inverse_transform(\n    np.column_stack([actuals, np.zeros((len(actuals), X_train.shape[2]-1))])\n)[:, 0]\n \n# 计算指标\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n \nmse = mean_squared_error(actuals, predictions)\nmae = mean_absolute_error(actuals, predictions)\nr2 = r2_score(actuals, predictions)\n \nprint(f&quot;MSE: {mse:.4f}&quot;)\nprint(f&quot;MAE: {mae:.4f}&quot;)\nprint(f&quot;R²: {r2:.4f}&quot;)\n\n超参数调优\n网格搜索\nfrom itertools import product\n \n# 定义参数网格\nparam_grid = {\n    &#039;hidden_size&#039;: [32, 64, 128],\n    &#039;num_layers&#039;: [1, 2, 3],\n    &#039;dropout&#039;: [0.1, 0.2, 0.3],\n    &#039;learning_rate&#039;: [0.0001, 0.001, 0.01],\n    &#039;batch_size&#039;: [16, 32, 64]\n}\n \n# 生成所有参数组合\nparam_combinations = list(product(\n    param_grid[&#039;hidden_size&#039;],\n    param_grid[&#039;num_layers&#039;],\n    param_grid[&#039;dropout&#039;],\n    param_grid[&#039;learning_rate&#039;],\n    param_grid[&#039;batch_size&#039;]\n))\n \nbest_params = None\nbest_val_loss = float(&#039;inf&#039;)\n \nfor hidden_size, num_layers, dropout, lr, batch_size in param_combinations:\n    print(f&quot;Testing: hidden_size={hidden_size}, num_layers={num_layers}, dropout={dropout}, lr={lr}, batch_size={batch_size}&quot;)\n \n    # 创建模型\n    model = StockPredictionLSTM(\n        input_size=X_train.shape[2],\n        hidden_size=hidden_size,\n        num_layers=num_layers,\n        dropout=dropout\n    )\n \n    # 创建DataLoader\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n \n    # 定义优化器\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n \n    # 训练（快速验证）\n    for epoch in range(10):\n        model.train()\n        for X_batch, y_batch in train_loader:\n            optimizer.zero_grad()\n            predictions = model(X_batch)\n            loss = criterion(predictions.squeeze(), y_batch)\n            loss.backward()\n            optimizer.step()\n \n    # 验证\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for X_batch, y_batch in val_loader:\n            predictions = model(X_batch)\n            loss = criterion(predictions.squeeze(), y_batch)\n            val_loss += loss.item()\n \n    val_loss /= len(val_loader)\n \n    print(f&quot;  Val Loss: {val_loss:.4f}&quot;)\n \n    # 更新最佳参数\n    if val_loss &lt; best_val_loss:\n        best_val_loss = val_loss\n        best_params = {\n            &#039;hidden_size&#039;: hidden_size,\n            &#039;num_layers&#039;: num_layers,\n            &#039;dropout&#039;: dropout,\n            &#039;learning_rate&#039;: lr,\n            &#039;batch_size&#039;: batch_size\n        }\n \nprint(f&quot;\\nBest parameters: {best_params}&quot;)\nprint(f&quot;Best validation loss: {best_val_loss:.4f}&quot;)\n随机搜索\nimport random\n \nn_trials = 50\n \nfor trial in range(n_trials):\n    # 随机选择参数\n    hidden_size = random.choice([32, 64, 128, 256])\n    num_layers = random.choice([1, 2, 3, 4])\n    dropout = random.uniform(0.1, 0.5)\n    learning_rate = random.choice([0.0001, 0.001, 0.01])\n    batch_size = random.choice([16, 32, 64, 128])\n \n    print(f&quot;Trial {trial+1}/{n_trials}&quot;)\n \n    # 训练和验证\n    # ...\n\n模型保存与加载\n保存模型\n# 1. 保存整个模型（包含结构和参数）\ntorch.save(model, &#039;lstm_model.pth&#039;)\n \n# 2. 只保存模型参数（推荐）\ntorch.save(model.state_dict(), &#039;lstm_model_state.pth&#039;)\n \n# 3. 保存检查点（包含优化器状态）\ncheckpoint = {\n    &#039;epoch&#039;: epoch,\n    &#039;model_state_dict&#039;: model.state_dict(),\n    &#039;optimizer_state_dict&#039;: optimizer.state_dict(),\n    &#039;loss&#039;: loss,\n    &#039;best_val_loss&#039;: best_val_loss\n}\ntorch.save(checkpoint, &#039;checkpoint.pth&#039;)\n加载模型\n# 1. 加载整个模型\nmodel = torch.load(&#039;lstm_model.pth&#039;)\nmodel.eval()  # 设置为评估模式\n \n# 2. 加载模型参数（需要先定义模型）\nmodel = StockPredictionLSTM(\n    input_size=X_train.shape[2],\n    hidden_size=64,\n    num_layers=2,\n    dropout=0.2\n)\nmodel.load_state_dict(torch.load(&#039;lstm_model_state.pth&#039;))\nmodel.eval()\n \n# 3. 加载检查点\ncheckpoint = torch.load(&#039;checkpoint.pth&#039;)\nmodel.load_state_dict(checkpoint[&#039;model_state_dict&#039;])\noptimizer.load_state_dict(checkpoint[&#039;optimizer_state_dict&#039;])\nepoch = checkpoint[&#039;epoch&#039;]\nloss = checkpoint[&#039;loss&#039;]\nbest_val_loss = checkpoint[&#039;best_val_loss&#039;]\n最佳实践\n# 训练时保存最佳模型\nbest_val_loss = float(&#039;inf&#039;)\n \nfor epoch in range(num_epochs):\n    # 训练和验证\n    val_loss = validate(model, val_loader)\n \n    # 保存最佳模型\n    if val_loss &lt; best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), &#039;best_model.pth&#039;)\n        print(f&quot;Saved best model with val_loss: {val_loss:.4f}&quot;)\n \n    # 定期保存检查点\n    if (epoch + 1) % 10 == 0:\n        checkpoint = {\n            &#039;epoch&#039;: epoch,\n            &#039;model_state_dict&#039;: model.state_dict(),\n            &#039;optimizer_state_dict&#039;: optimizer.state_dict(),\n            &#039;best_val_loss&#039;: best_val_loss\n        }\n        torch.save(checkpoint, f&#039;checkpoint_epoch_{epoch+1}.pth&#039;)\n\n评估指标\n回归指标\n1. MSE（均方误差）\nmse = torch.mean((predictions - targets) ** 2)\nprint(f&quot;MSE: {mse.item():.4f}&quot;)\n2. MAE（平均绝对误差）\nmae = torch.mean(torch.abs(predictions - targets))\nprint(f&quot;MAE: {mae.item():.4f}&quot;)\n3. RMSE（均方根误差）\nrmse = torch.sqrt(torch.mean((predictions - targets) ** 2))\nprint(f&quot;RMSE: {rmse.item():.4f}&quot;)\n4. R²（决定系数）\nss_res = torch.sum((targets - predictions) ** 2)\nss_tot = torch.sum((targets - torch.mean(targets)) ** 2)\nr2 = 1 - ss_res / ss_tot\nprint(f&quot;R²: {r2.item():.4f}&quot;)\n量化投资专用指标\n1. IC（信息系数）\nfrom scipy.stats import spearmanr\n \nic, _ = spearmanr(predictions.numpy(), targets.numpy())\nprint(f&quot;IC: {ic:.4f}&quot;)\n2. ICIR（信息系数信息比率）\n# 计算多期IC\nic_values = []\nfor i in range(n_periods):\n    ic, _ = spearmanr(preds[i], targets[i])\n    ic_values.append(ic)\n \nicir = np.mean(ic_values) / np.std(ic_values)\nprint(f&quot;ICIR: {icir:.4f}&quot;)\n3. MAPE（平均绝对百分比误差）\nmape = torch.mean(torch.abs((targets - predictions) / targets)) * 100\nprint(f&quot;MAPE: {mape.item():.2f}%&quot;)\n完整评估\ndef evaluate_model(model, test_loader, scaler):\n    &quot;&quot;&quot;\n    评估模型\n \n    参数:\n        model: 模型\n        test_loader: 测试数据加载器\n        scaler: 标准化器\n \n    返回:\n        评估结果字典\n    &quot;&quot;&quot;\n    model.eval()\n    predictions = []\n    actuals = []\n \n    with torch.no_grad():\n        for X_batch, y_batch in test_loader:\n            pred = model(X_batch)\n            predictions.extend(pred.squeeze().numpy())\n            actuals.extend(y_batch.numpy())\n \n    predictions = np.array(predictions)\n    actuals = np.array(actuals)\n \n    # 反标准化\n    predictions_original = scaler.inverse_transform(\n        np.column_stack([predictions, np.zeros((len(predictions), X_train.shape[2]-1))])\n    )[:, 0]\n \n    actuals_original = scaler.inverse_transform(\n        np.column_stack([actuals, np.zeros((len(actuals), X_train.shape[2]-1))])\n    )[:, 0]\n \n    # 计算指标\n    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n    from scipy.stats import spearmanr\n \n    results = {\n        &#039;MSE&#039;: mean_squared_error(actuals_original, predictions_original),\n        &#039;MAE&#039;: mean_absolute_error(actuals_original, predictions_original),\n        &#039;RMSE&#039;: np.sqrt(mean_squared_error(actuals_original, predictions_original)),\n        &#039;R²&#039;: r2_score(actuals_original, predictions_original),\n        &#039;IC&#039;: spearmanr(predictions_original, actuals_original)[0],\n        &#039;MAPE&#039;: np.mean(np.abs((actuals_original - predictions_original) / actuals_original)) * 100\n    }\n \n    return results, predictions_original, actuals_original\n \n# 评估\nresults, preds, actuals = evaluate_model(model, test_loader, scaler)\n \nfor metric, value in results.items():\n    print(f&quot;{metric}: {value:.4f}&quot;)\n\nLSTM vs LightGBM\n对比维度\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n维度LSTMLightGBM数据需求大量中等训练时间长短特征工程少多可解释性低高长期依赖优秀差过拟合风险高中推理速度中快GPU支持✅❌学习曲线慢快\n性能对比实验\n# LSTM模型\nlstm_model = StockPredictionLSTM(\n    input_size=X_train.shape[2],\n    hidden_size=64,\n    num_layers=2,\n    dropout=0.2\n)\n \n# 训练LSTM\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n \nfor epoch in range(50):\n    lstm_model.train()\n    for X_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        predictions = lstm_model(X_batch)\n        loss = criterion(predictions.squeeze(), y_batch)\n        loss.backward()\n        optimizer.step()\n \n# LightGBM模型\nimport lightgbm as lgb\n \n# 展平数据\nX_train_flat = X_train.reshape(X_train.shape[0], -1)\nX_test_flat = X_test.reshape(X_test.shape[0], -1)\n \n# 训练\nlgb_model = lgb.LGBMRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=6\n)\nlgb_model.fit(X_train_flat, y_train)\n \n# 预测\nlstm_preds = lstm_model(torch.FloatTensor(X_test)).squeeze().numpy()\nlgb_preds = lgb_model.predict(X_test_flat)\n \n# 对比\nlstm_mse = np.mean((lstm_preds - y_test) ** 2)\nlgb_mse = np.mean((lgb_preds - y_test) ** 2)\n \nprint(f&quot;LSTM MSE: {lstm_mse:.4f}&quot;)\nprint(f&quot;LightGBM MSE: {lgb_mse:.4f}&quot;)\n选择建议\n选择LSTM:\n\n数据量大\n长期依赖重要\n特征工程少\n需要捕捉复杂模式\n\n选择LightGBM:\n\n数据量中等\n需要快速迭代\n可解释性重要\n计算资源有限\n\n\n最佳实践\n数据准备\n1. 数据质量检查\n# 检查缺失值\nprint(data.isnull().sum())\n \n# 检查异常值\nprint(data.describe())\n \n# 处理缺失值\ndata = data.fillna(method=&#039;ffill&#039;)\n2. 特征选择\n# 选择与目标相关的特征\nfrom sklearn.feature_selection import SelectKBest, f_regression\n \nselector = SelectKBest(f_regression, k=10)\nX_selected = selector.fit_transform(X, y)\n3. 时间序列划分\n# 严格按时间顺序划分\n# 避免未来函数\n# 保留样本外数据\n模型设计\n1. 模型复杂度\n# 从简单模型开始\n# 逐步增加复杂度\n# 避免过度复杂\n2. 超参数选择\n# hidden_size: 32-128\n# num_layers: 1-3\n# dropout: 0.1-0.3\n# learning_rate: 0.0001-0.01\n3. 正则化\n# 使用Dropout防止过拟合\n# 使用BatchNorm加速训练\n# 使用L1/L2正则化\n训练策略\n1. 早停\n# 监控验证集损失\n# 防止过拟合\n# 节省训练时间\n2. 学习率调度\n# 初始学习率较大\n# 逐渐降低学习率\n# 使用学习率衰减\n3. 梯度裁剪\n# 防止梯度爆炸\n# 稳定训练过程\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n评估与验证\n1. 多指标评估\n# 不要只看一个指标\n# 综合考虑多个指标\n# 关注风险调整收益\n2. 样本外验证\n# 保留一部分数据不参与训练\n# 严格验证泛化能力\n# 避免过拟合\n3. 稳定性测试\n# 在不同时间段测试\n# 检查参数稳定性\n# 验证鲁棒性\n常见问题\nQ1: LSTM过拟合怎么办？\nA: 多种方法结合:\n\n增加Dropout比例\n减少模型复杂度\n增加训练数据\n使用更强的正则化\n\nQ2: LSTM训练很慢怎么办？\nA: 优化训练速度:\n\n使用GPU\n减小batch_size\n减少模型复杂度\n使用更快的优化器\n\nQ3: LSTM vs LightGBM如何选择？\nA: 根据实际情况:\n\n数据量大、长期依赖重要：LSTM\n需要快速迭代、可解释性重要：LightGBM\n两者都试，选择更好的\n\nQ4: 如何确定序列长度？\nA: 通过实验确定:\n\n尝试不同的序列长度（10, 20, 30, 60）\n使用验证集选择最优长度\n考虑业务逻辑（如一周、一月）\n\n\n核心知识点总结\n完整预测流程\n\n✅ 数据准备\n✅ 模型定义\n✅ Dataset和DataLoader\n✅ 训练\n✅ 评估\n\n超参数调优\n\n✅ 网格搜索\n✅ 随机搜索\n✅ 贝叶斯优化\n\n模型保存与加载\n\n✅ 保存模型\n✅ 加载模型\n✅ 最佳实践\n\n评估指标\n\n✅ 回归指标\n✅ 量化投资指标\n✅ 完整评估\n\nLSTM vs LightGBM\n\n✅ 对比维度\n✅ 性能对比\n✅ 选择建议\n\n最佳实践\n\n✅ 数据准备\n✅ 模型设计\n✅ 训练策略\n✅ 评估验证\n\n\n附录\n专业术语表\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n术语英文解释RNNRecurrent Neural Network循环神经网络LSTMLong Short-Term Memory长短期记忆网络GRUGated Recurrent Unit门控循环单元ICInformation Coefficient信息系数ICIRIC Information Ratio信息系数信息比率Early Stopping早停防止过拟合的技术Dropout丢弃随机丢弃神经元\n推荐学习资源\n书籍:\n\n《深度学习》（Goodfellow等）\n《动手学深度学习》\n《Python深度学习》\n\n在线课程:\n\nCoursera: Deep Learning Specialization\nFast.ai: Practical Deep Learning for Coders\n\n文档和教程:\n\nPyTorch官方文档：pytorch.org/docs/\nPyTorch教程：pytorch.org/tutorials/\n\n\n系列文档结束\n祝学习顺利！🎓"},"quant/qlib/week5/index":{"slug":"quant/qlib/week5/index","filePath":"quant/qlib/week5/index.md","title":"index","links":["01_基础理论系列/","02_PyTorch框架系列/","03_LSTM模型构建系列/","04_时序数据处理系列/","05_模型训练优化系列/","06_实战应用系列/"],"tags":[],"content":"Week 5 LSTM深度学习模型 - 知识点体系\n📚 文档说明\n\n文档版本: v1.0\n创建日期: 2025-01-09\n学习主题: LSTM深度学习模型\n适用对象: Qlib量化投资学习者\n建议学习时间: 5-6小时（4-5天）\n\n\n📖 文档体系\n本文档采用分系列组织方式，将Week 5的知识点分为6个系列，每个系列聚焦一个核心主题。\n系列列表\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n系列说明文档路径1. 基础理论系列深度学习基础、RNN与LSTM原理01_基础理论系列2. PyTorch框架系列Tensor、Autograd、nn.Module、常用层02_PyTorch框架系列3. LSTM模型构建系列单层、多层、双向LSTM架构03_LSTM模型构建系列4. 时序数据处理系列滑动窗口、标准化、Dataset04_时序数据处理系列5. 模型训练优化系列损失函数、优化器、训练循环05_模型训练优化系列6. 实战应用系列完整流程、调优、评估06_实战应用系列\n\n🎯 学习路径\n推荐学习顺序\nStep 1: 基础理论系列\n   ↓\nStep 2: PyTorch框架系列\n   ↓\nStep 3: LSTM模型构建系列\n   ↓\nStep 4: 时序数据处理系列\n   ↓\nStep 5: 模型训练优化系列\n   ↓\nStep 6: 实战应用系列\n\n每日学习计划\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n天数学习内容预计时间Day 1基础理论系列 + PyTorch框架系列1.5-2小时Day 2LSTM模型构建系列1小时Day 3时序数据处理系列1小时Day 4模型训练优化系列1-1.5小时Day 5实战应用系列1.5小时\n\n📊 知识点概览\n1. 基础理论系列\n核心知识点:\n\n✅ 深度学习基础（神经元、激活函数、神经网络）\n✅ 序列数据特点\n✅ RNN原理与局限性（梯度消失）\n✅ LSTM原理（细胞状态、门机制）\n✅ LSTM vs RNN vs GRU对比\n\n关键概念:\n\n梯度消失问题\n细胞状态\n遗忘门、输入门、输出门\n长期依赖\n\n2. PyTorch框架系列\n核心知识点:\n\n✅ Tensor创建与操作\n✅ Autograd自动微分\n✅ nn.Module模型定义\n✅ 常用层（LSTM、Linear、Dropout）\n\n关键概念:\n\nrequires_grad\nbackward()\n计算图\nGPU加速\n\n3. LSTM模型构建系列\n核心知识点:\n\n✅ 单层LSTM\n✅ 多层LSTM\n✅ 双向LSTM\n✅ LSTM变体（堆叠、编码器-解码器、注意力）\n✅ 超参数选择\n\n关键概念:\n\nhidden_size\nnum_layers\ndropout\nbatch_first\nbidirectional\n\n4. 时序数据处理系列\n核心知识点:\n\n✅ 滑动窗口方法\n✅ 时间序列划分\n✅ 特征标准化（Z-score、Min-Max、RobustScaler）\n✅ PyTorch Dataset\n✅ DataLoader\n\n关键概念:\n\n序列长度（seq_len）\n训练/验证/测试集\n数据泄漏\n批处理\n\n5. 模型训练优化系列\n核心知识点:\n\n✅ 损失函数（MSE、MAE、Smooth L1）\n✅ 优化器（SGD、Adam、RMSprop）\n✅ 训练循环\n✅ 早停策略\n✅ 正则化（L1/L2、Dropout、BatchNorm）\n✅ 学习率调度\n\n关键概念:\n\n前向传播\n反向传播\n梯度裁剪\n过拟合\n学习率衰减\n\n6. 实战应用系列\n核心知识点:\n\n✅ 完整预测流程\n✅ 超参数调优（网格搜索、随机搜索）\n✅ 模型保存与加载\n✅ 评估指标（MSE、MAE、IC、ICIR）\n✅ LSTM vs LightGBM对比\n✅ 最佳实践\n\n关键概念:\n\n端到端流程\n模型持久化\n泛化能力\n样本外验证\n鲁棒性\n\n\n💡 学习建议\n1. 理论学习\n\n先理解RNN的局限性\n再学习LSTM的创新点\n掌握门机制的作用\n理解细胞状态的意义\n\n2. 实践练习\n\n从简单的单层LSTM开始\n逐步增加复杂度\n使用真实数据训练\n对比不同架构的性能\n\n3. 代码实现\n\n跟着文档实现代码\n理解每一步的作用\n尝试修改参数\n观察效果变化\n\n4. 问题解决\n\n遇到问题时先查看文档\n利用PyTorch官方文档\n搜索Stack Overflow\n在社区提问\n\n\n🔧 技能检查清单\n基础理论\n\n 理解深度学习基本概念\n 掌握序列数据特点\n 理解RNN的工作原理\n 知道RNN的局限性\n 掌握LSTM的架构\n 理解门机制的作用\n 知道LSTM vs GRU的区别\n\nPyTorch框架\n\n 能够创建和操作Tensor\n 理解自动微分原理\n 能够定义自定义模型\n 掌握常用层的使用\n 能够使用GPU加速\n\n模型构建\n\n 能够定义单层LSTM\n 能够定义多层LSTM\n 理解双向LSTM\n 知道如何选择超参数\n 了解LSTM变体\n\n数据处理\n\n 能够实现滑动窗口\n 能够划分时序数据\n 掌握特征标准化\n 能够创建Dataset\n 能够使用DataLoader\n\n训练优化\n\n 能够选择合适的损失函数\n 能够选择合适的优化器\n 能够实现完整的训练循环\n 能够使用早停策略\n 理解正则化方法\n 能够调整学习率\n\n实战应用\n\n 能够完成完整的预测流程\n 能够进行超参数调优\n 能够保存和加载模型\n 能够计算评估指标\n 理解LSTM vs LightGBM\n 掌握最佳实践\n\n\n📚 扩展阅读\n推荐书籍\n\n《深度学习》（Ian Goodfellow等）\n《动手学深度学习》（Dive into Deep Learning）\n《Python深度学习》（François Chollet）\n\n在线课程\n\nCoursera: Deep Learning Specialization\nFast.ai: Practical Deep Learning for Coders\nUdacity: Deep Learning Nanodegree\n\n论文\n\nHochreiter &amp; Schmidhuber (1997). Long Short-Term Memory\nCho et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder\n\n官方文档\n\nPyTorch: pytorch.org/docs/\nPyTorch Tutorials: pytorch.org/tutorials/\n\n\n🎓 学习成果\n完成本系列学习后，您将能够：\n理论层面\n\n✅ 深入理解LSTM的工作原理\n✅ 掌握时序数据的处理方法\n✅ 理解深度学习的核心概念\n\n实践层面\n\n✅ 使用PyTorch构建LSTM模型\n✅ 训练和优化LSTM模型\n✅ 评估和对比模型性能\n\n应用层面\n\n✅ 将LSTM应用于量化投资\n✅ 进行股票价格预测\n✅ 提取时序特征\n\n\n📞 支持与反馈\n如有问题或建议，请通过以下方式联系：\n\nGitHub Issues: github.com/anomalyco/opencode/issues\n学习交流群: [待添加]\n\n\n祝学习顺利！🎓"},"zlyq/AI-辅助策略研究":{"slug":"zlyq/AI-辅助策略研究","filePath":"zlyq/AI 辅助策略研究.md","title":"AI 辅助策略研究","links":[],"tags":[],"content":"AI 辅助策略研究\n目标\n\n入职可以高效快速的理解公司已经有的策略细节，避免各种繁琐无效沟通 ✅\n可以快速搭建自己的策略想法，0-1 快速展示策略到画布（交互-回显到画布都未去做，验已经验证逻辑可行）✅\nAI 基于现有所有策略知识库，相关数据库，给定目标，进行自主推导，模拟研究员角色，真正的 Agent 角色，进行一个策略迭代 loop：\n\n想法→策略→验证→结果分析→优化策略-验证\n先基于已知的一些简单策略进行尝试\n\n\n回测结果解读助手，可以根据回测结果结合目前能够提供的各类数据进行关联性分析\n\n主要是数据维度\n\n\nAI 基于海量数据进行统计分析，做到自然语言获取到统计数据，例如：\n\n统计当日某竞争者的利润，和 gasfee 的关系，以及图表化的展示，这样的话扩展性很好，想问什么就能得到什么结果\n\n\n\n\nPRD 需求\n一：策略解释器（已经验证，可行性高）\n\n可以辅助新人理解，也可以加深老人理解\n\n不同研究员产出的策略理解\n\n基于现有的研究员的策略节点，然后理解策略内容\n\n节点功能理解\n节点参数的理解\n\n\n\n从前端选择目前平台有的策略完整节点，送给 AI 进行理解\n\n二：AI 策略组合（已经验证，可行性高，主要要考虑前端交互）\n\n研究员提出一些想法，AI 可以根据现有的节点模块进行自由组合，比较快速的进行一个初步验证，这个部分比较复杂，得考虑交互，但是实用性比较好\n\n形式\n比如用户说想验证 BTC 波动率是如何影响其他小币种，比如 USUAL/USDT 等，在一段时间的一个价格比率变化，那么 AI 会根据这个进行一个初步的节点组合，返回给前端，快速进行策略迭代\n\n三：回测结果解读助手\n\n这里需要重点研究研究\n\n给 AI 的一些数据\n\n回测指标\n盈亏分布\n交易订单分析\n\nAI 能够给出的一些分析\n\n当时的订单的亏损订单的数据\n当时订单亏损，同时的竞争对手的数据\n\n\n四：AI 头脑风暴\n\n这部分不再是 req-resp 的一个形式，而是 AI 作为一个完整的智能体，它可以基于现有的所有节点进行自我学习（比如基于 cex-dex 跨所套利的一些论文，策略方案来源于其他人或者其他源，来进行一个自我的回测→结果→重新执行）\n\n\n核心交互流程设计\n场景 A：AI 策略解释（策略节点 → 解释）\n\n前端：发送策略 ID 给 AI 系统。\nAI 系统：通过 API 从核心引擎拉取该策略的 JSON 节点图。\nAI 系统：检索”节点知识库”（向量数据库），获取每个节点的功能文档。\nAI 系统：调用 LLM 生成解读，返回前端。\n\n\n场景 B：回测结果解读\n\n核心引擎：完成回测，将结果摘要（非海量原始数据，而是统计摘要）存入中间层。\nAI 系统：读取摘要 + 抽取典型亏损订单。\nAI 系统：利用 Python Data Agent（如 Pandas Query Tool）进行横向对比（同期其他币种表现）。\nAI 系统：产出结论，推送给研究员。\n"},"zlyq/ReseachAgent":{"slug":"zlyq/ReseachAgent","filePath":"zlyq/ReseachAgent.md","title":"ReseachAgent","links":[],"tags":[],"content":"RAG 的目的\n\n提供领域锚点，防止 LLM 产生不切实际的假设\n\n什么是好\n\n要有具体的回测目标，验证能跑通并不是好，需要满足一些指标之类\n"},"zlyq/提示词":{"slug":"zlyq/提示词","filePath":"zlyq/提示词.md","title":"提示词","links":[],"tags":[],"content":"我现在AI 辅助系统只有这两个功能，一个是理解回测平台生成的策略 json ,一个是自然语言生成策略 json 并验证 ，那么现在我希望的是可以有类似 Agent ,自己生成想法然后通过我这个系统验证，实际上有点像是 自主决策的 Agent 了，你觉得这个功能如何，是否可以参考 RDAgent\n"},"关于":{"slug":"关于","filePath":"关于.md","title":"关于","links":["/","quant/qlib/","blockchain/"],"tags":[],"content":"title: 关于\n关于我\n\n热爱技术，分享知识，持续成长\n\n\n👤 个人简介\n我是一名热爱技术的开发者，专注于量化投资和区块链技术两个领域。我喜欢探索新技术，分享学习心得，记录成长历程。\n\n🎯 技术兴趣\n量化投资\n使用机器学习和深度学习技术进行量化分析和交易策略研究。\n\n特征工程: Qlib平台的应用与实践\n机器学习: LightGBM、XGBoost等模型\n深度学习: PyTorch、LSTM、Transformer\n回测系统: 策略评估与绩效分析\n\n区块链技术\n探索去中心化世界，从智能合约到DeFi应用。\n\n智能合约: Solidity开发与安全\nDeFi应用: 去中心化金融协议\nWeb3开发: DApp前端与区块链交互\nNFT市场: 数字资产与收藏品\n\n\n🛠️ 技术栈\n编程语言\n\n  Python\n  JavaScript\n  TypeScript\n  Go\n  Solidity\n\n框架与工具\n\n  PyTorch\n  Qlib\n  LightGBM\n  Ethers.js\n  Hardhat\n  React\n  Next.js\n\n\n📚 学习理念\n理论与实践并重\n\n深入理解理论知识\n大量动手实践\n解决实际问题\n\n持续学习\n\n紧跟技术发展\n保持好奇心\n分享学习心得\n\n开放交流\n\n参与技术社区\n分享知识经验\n与他人共同成长\n\n\n🎓 教育背景\n[待添加]\n\n💼 工作经历\n[待添加]\n\n📝 写作动机\n为什么写技术博客？\n\n巩固知识: 通过写作加深对知识的理解\n分享经验: 帮助他人少走弯路\n记录成长: 留下学习轨迹和思考过程\n建立影响力: 与更多人交流和学习\n\n写作原则\n\n原创性: 分享自己的理解和实践\n实用性: 提供可操作的代码和示例\n系统性: 构建完整的知识体系\n持续更新: 跟进技术发展和新的理解\n\n\n🚀 未来规划\n2025年计划\n量化投资\n\n Week 4 - 强化学习\n Week 6 - Transformer模型\n 高级回测技巧\n 多因子模型\n 端到端策略系统\n\n区块链\n\n 区块链基础系列\n 智能合约开发实战\n DeFi项目开发\n NFT市场构建\n Web3全栈开发\n\n技术博客\n\n 增加更多实战案例\n 优化文档结构\n 添加视频教程\n 建立读者社区\n\n\n🤝 交流与合作\n我欢迎各种形式的交流与合作！\n合作机会\n\n技术文章写作\n开源项目贡献\n技术分享演讲\n量化策略研究\n区块链项目开发\n\n联系方式\n\n📧 Email: [待添加]\n🐦 Twitter: [待添加]\n💻 GitHub: [待添加]\n💬 微信: [待添加]\n📱 LinkedIn: [待添加]\n\n\n🔗 链接\n量化投资\n\nQlib官方文档\nLightGBM官方文档\nPyTorch官方文档\n\n区块链\n\nEthereum.org\nSolidity文档\nOpenZeppelin\n\n技术社区\n\nQuantStart\nGitHub\nStack Overflow\n\n\n📄 许可证\n本博客内容采用 Creative Commons Attribution 4.0 International (CC BY 4.0) 许可证。\n您可以自由地：\n\n✅ 分享：复制和重新分发材料\n✅ 修改：改编、转换和基于材料构建\n\n但需要遵守以下条件：\n\n⚠️ 署名：必须提供适当的署名，提供指向许可证的链接，并说明是否进行了更改\n\n\n🙏 致谢\n感谢以下项目和社区：\n量化投资\n\nMicrosoft Research Asia（Qlib开发团队）\nLightGBM开发团队\nPyTorch团队\n\n区块链\n\nEthereum基金会\nSolidity团队\nOpenZeppelin团队\n\n开源社区\n\n所有开源项目的贡献者\n技术社区的每一位成员\n\n\n\n  🚀 让我们一起成长\n  在技术的道路上，我们并不孤单\n  \n    返回首页\n    量化学习\n    区块链\n  \n\n\n\n  Made with ❤️ by [你的名字]\n"},"快速开始":{"slug":"快速开始","filePath":"快速开始.md","title":"快速开始","links":["quant/qlib/week1/01-qlib特征工程全景概览","quant/qlib/week2/01-Gradient-Boosting原理","quant/qlib/week2/03-模型训练","quant/qlib/week2/02-时序数据划分","quant/qlib/week2/04-IC-Rank-IC评估指标","quant/qlib/week2/05-特征重要性分析","quant/qlib/week1/04-相对强弱预测的量化思维","quant/qlib/week1/05-qlib特征工程实践指南","quant/qlib/week3/01-交易策略理论","quant/qlib/week3/02-投资组合构建方法","/","quant/qlib/week1/","quant/qlib/week2/"],"tags":[],"content":"title: 快速开始\n快速开始指南\n本指南帮助你快速上手量化投资技术文档中的内容。\n\n🎯 我是新手，从哪里开始？\n第一步：了解基础概念\n如果你是量化投资的新手，建议按照以下顺序学习：\n\n\n特征工程基础\n\n阅读 Qlib特征工程全景概览\n了解量化数据的特点和挑战\n\n\n\n机器学习基础\n\n阅读 Gradient Boosting原理\n理解LightGBM的核心概念\n\n\n\n实践应用\n\n阅读 模型训练\n开始动手构建你的第一个模型\n\n\n\n\n📚 根据目标选择学习路径\n路径A：因子研究\n特征工程全景概览 → Horizon对齐详解 → 横截面标准化 → 相对强弱预测 → 实践指南\n\n适合人群：专注于因子研发和优化的研究员\n路径B：模型构建\nGradient Boosting原理 → 时序数据划分 → 模型训练 → IC评估指标 → 特征重要性分析\n\n适合人群：负责构建和优化预测模型的开发者\n路径C：量化交易\n相对强弱预测 → IC/Rank IC评估 → 模型训练实战 → 特征工程实践\n\n适合人群：希望将模型应用于实际交易的量化交易员\n\n🛠️ 准备工作\n环境配置\n# 安装Python依赖\npip install qlib lightgbm numpy pandas scikit-learn scipy\n \n# 安装可视化工具（可选）\npip install matplotlib seaborn shap\n数据准备\n\n确保有量化数据源（股票行情、财务数据等）\n推荐使用标准化的数据格式\n注意数据的时序性和因果性\n\n\n💻 快速示例\n示例1：训练一个简单的LightGBM模型\nimport lightgbm as lgb\nimport numpy as np\n \n# 准备数据\nX_train = np.random.randn(1000, 50)  # 1000个样本，50个特征\ny_train = np.random.randn(1000)       # 目标变量\n \nX_val = np.random.randn(200, 50)\ny_val = np.random.randn(200)\n \n# 创建数据集\ntrain_data = lgb.Dataset(X_train, label=y_train)\nval_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n \n# 设置参数\nparams = {\n    &#039;objective&#039;: &#039;regression&#039;,\n    &#039;metric&#039;: &#039;rmse&#039;,\n    &#039;num_leaves&#039;: 31,\n    &#039;learning_rate&#039;: 0.05,\n}\n \n# 训练模型\nmodel = lgb.train(\n    params,\n    train_data,\n    num_boost_round=1000,\n    valid_sets=[train_data, val_data],\n    callbacks=[lgb.early_stopping(stopping_rounds=50)]\n)\n \n# 预测\ny_pred = model.predict(X_val)\n示例2：计算IC指标\nfrom scipy.stats import pearsonr\n \n# 计算IC\nic = pearsonr(y_pred, y_val)[0]\nprint(f&quot;IC = {ic:.4f}&quot;)\n\n❓ 常见问题\nQ1: 如何处理时序数据的划分？\n使用时间序列交叉验证，确保训练集严格在验证集之前。详细方法请参考 时序数据划分。\nQ2: IC值多少算是好？\n一般来说：\n\nIC &gt; 0.05：优秀\nIC ∈ [0.02, 0.05]：良好\nIC &lt; 0.02：较差\n\n但具体要看市场和数据特性。\nQ3: 如何避免未来信息泄露？\n严格遵守因果性原则，确保在时刻t的预测只能使用t时刻及之前的数据。详细内容请参考 时序数据划分。\nQ4: 如何选择LightGBM的参数？\n建议从简单参数开始，逐步调优：\n\n先设置 num_leaves 和 learning_rate\n调整 bagging_fraction 和 feature_fraction\n最后微调正则化参数\n\n详见 模型训练。\n\n📖 推荐阅读顺序\n理论学习\n\nQlib特征工程全景概览\nGradient Boosting原理\nRank IC评估指标\n\n实践应用\n\n时序数据划分\n模型训练\n特征重要性分析\n\n进阶提升\n\n相对强弱预测的量化思维\nQlib特征工程实践指南\n交易策略理论\n投资组合构建方法\n\n高级实战\n\n相对强弱预测的量化思维\nQlib特征工程实践指南\n交易策略理论\n投资组合构建方法\n\n\n🤝 获取帮助\n\n仔细阅读相关文档，大部分问题都有详细解答\n检查代码示例，确保理解每个步骤\n在实践中不断尝试和调整参数\n\n\n← 返回首页 | 特征工程 | LightGBM"},"站点导航":{"slug":"站点导航","filePath":"站点导航.md","title":"站点导航","links":["/","快速开始","关于","quant/qlib/","quant/qlib/week1/","quant/qlib/week2/","quant/qlib/week3/","quant/qlib/week5/","quant/qlib/week1/01-qlib特征工程全景概览","quant/qlib/week1/02-horizon对齐详解","quant/qlib/week1/03-横截面标准化与中性化","quant/qlib/week1/04-相对强弱预测的量化思维","quant/qlib/week1/05-qlib特征工程实践指南","quant/qlib/week2/01-Gradient-Boosting原理","quant/qlib/week2/02-时序数据划分","quant/qlib/week2/03-模型训练","quant/qlib/week2/04-IC-Rank-IC评估指标","quant/qlib/week2/05-特征重要性分析","quant/qlib/week2/06-学习检查清单","quant/qlib/week3/01-交易策略理论","quant/qlib/week3/02-投资组合构建方法","quant/qlib/week3/03-Executor与成本模型","quant/qlib/week3/04-绩效评估指标","quant/qlib/week3/05-实验分析方法","quant/qlib/week3/06-回测流程与实践","quant/qlib/week3/07-学习检查清单","quant/qlib/week5/01_基础理论系列/","quant/qlib/week5/02_PyTorch框架系列/","quant/qlib/week5/03_LSTM模型构建系列/","quant/qlib/week5/04_时序数据处理系列/","quant/qlib/week5/05_模型训练优化系列/","quant/qlib/week5/06_实战应用系列/","blockchain/","zlyq/ResearchAgent","zlyq/提示词"],"tags":[],"content":"title: 站点导航\n站点导航\n\n快速找到你想要的内容\n\n\n📋 主页和导航\n\n首页\n快速开始\n关于\n\n\n📊 量化投资\nQlib学习路径\n\nQlib学习总览\n\nWeek 1 - 特征工程\nWeek 2 - LightGBM\nWeek 3 - 回测系统\nWeek 5 - LSTM深度学习\n\n\n\nWeek 1 - 特征工程\n系统讲解Qlib特征工程的核心概念与实践方法。\n\n特征工程模块\n\n01-qlib特征工程全景概览\n02-horizon对齐详解\n03-横截面标准化与中性化\n04-相对强弱预测的量化思维\n05-qlib特征工程实践指南\n\n\n\nWeek 2 - LightGBM\n深入学习LightGBM在量化投资中的应用。\n\nLightGBM模块\n\n01-Gradient Boosting原理\n02-时序数据划分\n03-模型训练\n04-IC-Rank-IC评估指标\n05-特征重要性分析\n06-学习检查清单\n\n\n\nWeek 3 - 回测系统\n完整讲解策略回测、投资组合构建、绩效评估。\n\n回测引擎模块\n\n01-交易策略理论\n02-投资组合构建方法\n03-Executor与成本模型\n04-绩效评估指标\n05-实验分析方法\n06-回测流程与实践\n07-学习检查清单\n\n\n\nWeek 5 - LSTM深度学习\n深入学习LSTM神经网络，掌握时序数据的深度学习方法。\n\nLSTM模块\n\n基础理论系列 - 深度学习基础、RNN与LSTM原理\nPyTorch框架系列 - Tensor、Autograd、nn.Module\nLSTM模型构建系列 - 单层、多层、双向LSTM\n时序数据处理系列 - 滑动窗口、标准化、Dataset\n模型训练优化系列 - 损失函数、优化器、训练循环\n实战应用系列 - 完整流程、调优、评估\n\n\n\n\n🔗 区块链技术\n探索去中心化世界，从智能合约到DeFi应用。\n\n区块链技术总览\n\n\n🚧 建设中 - 区块链相关内容正在整理中，敬请期待！\n\n\n🤖 AI 辅助策略研究\n\n[AI辅助策略研究](zlyq/AI 辅助策略研究.md)\nResearchAgent\n提示词\n\n\n📚 学习路径\n新手入门\n特征工程基础 → LightGBM原理 → 模型训练 → 投资组合构建 → 策略回测\n\n适合初学者，从基础概念开始，逐步学习完整的量化投资流程。\n进阶提升\n时序数据划分 → IC优化训练 → 特征重要性分析 → 成本模型 → 绩效评估\n\n适合有一定基础的学习者，深入理解和优化各个环节。\n实战应用\n从实际项目出发 → 遇到问题查文档 → 理论学习 → 实践应用\n\n适合有经验的开发者，通过解决实际问题提升技能。\n\n🏷️ 标签索引\n按主题分类\n特征工程\n\nQlib特征工程\nHorizon对齐\n横截面标准化\n\n机器学习\n\nLightGBM\nGradient Boosting\n模型训练\n\n深度学习\n\nLSTM\nPyTorch\n时序预测\n\n回测系统\n\n回测引擎\n交易策略\n绩效评估\n\n按难度分类\n入门级\n\nQlib特征工程全景概览\n快速开始\n\n进阶级\n\nLightGBM原理\nIC-Rank-IC评估指标\n\n高级\n\nLSTM深度学习\n投资组合构建方法\n\n\n🔗 相关资源\n官方文档\n量化投资\n\nQlib官方文档\nLightGBM官方文档\nPyTorch官方文档\n\n区块链\n\nEthereum.org\nSolidity文档\nOpenZeppelin\n\n开源项目\n\nQlib GitHub\nLightGBM GitHub\nPyTorch GitHub\n\n学习资源\n\nQuantStart量化教程\nFast.ai深度学习\nEthereum开发者文档\n\n\n💡 使用技巧\n如何使用本站\n\n按学习路径学习：从Week 1开始，系统学习量化投资知识\n按主题查找：使用标签索引快速找到感兴趣的内容\n理论与实践结合：每个模块都包含代码示例，建议动手实践\n遇到问题查阅：将本站作为参考文档，随时查阅相关知识点\n\n浏览建议\n\n首次访问建议阅读快速开始\n按照推荐的学习路径逐步学习\n动手运行代码示例加深理解\n遇到问题使用搜索功能或查阅站点导航\n\n\n🔍 搜索建议\n如果你在寻找特定的内容，可以尝试以下关键词：\n\n特征工程: horizon、标准化、中性化\n机器学习: LightGBM、IC、特征重要性\n深度学习: LSTM、PyTorch、时序预测\n回测: 策略、绩效评估、投资组合\n\n\n📧 反馈与建议\n如果你发现文档中的错误或有改进建议，欢迎反馈：\n\n📝 提交Issue\n📧 Email: [待添加]\n💬 微信: [待添加]\n\n\n\n  💡 还在等什么？\n  开始你的学习之旅，探索量化投资的世界\n  \n    开始学习\n    快速开始\n  \n\n\n\n  如有疑问，欢迎查看 [关于](关于.md) 页面了解更多信息\n"}}