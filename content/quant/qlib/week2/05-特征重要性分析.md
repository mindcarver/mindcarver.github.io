# ç‰¹å¾é‡è¦æ€§åˆ†æ

## 1. ç‰¹å¾é‡è¦æ€§åŸºç¡€

### 1.1 ç‰¹å¾é‡è¦æ€§çš„æ„ä¹‰

**ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ**

åœ¨é‡åŒ–æŠ•èµ„ä¸­ï¼Œç‰¹å¾é‡è¦æ€§åˆ†ææœ‰ä¸‰å¤§ä»·å€¼ï¼š

1. **æ¨¡å‹è§£é‡Šæ€§**ï¼šç†è§£æ¨¡å‹å¦‚ä½•åšå†³ç­–
2. **ç‰¹å¾ç­›é€‰**ï¼šè¯†åˆ«æœ‰æ•ˆå› å­ï¼Œå‰”é™¤å†—ä½™å› å­
3. **é£é™©æ§åˆ¶**ï¼šé¿å…æ¨¡å‹ä¾èµ–æ— æ•ˆç‰¹å¾

**é‡åŒ–åœºæ™¯çš„ç‰¹æ®Šæ€§**

é‡åŒ–æ•°æ®çš„ç‰¹ç‚¹ï¼š
- **é«˜ç»´ç‰¹å¾**ï¼šæ•°ç™¾åˆ°æ•°åƒä¸ªå› å­
- **ä½ä¿¡å™ªæ¯”**ï¼šå¤§é‡å™ªå£°ç‰¹å¾
- **å¼ºç›¸å…³æ€§**ï¼šå› å­é—´å­˜åœ¨å¤šé‡å…±çº¿æ€§
- **éå¹³ç¨³æ€§**ï¼šå› å­é‡è¦æ€§éšæ—¶é—´å˜åŒ–

### 1.2 LightGBMçš„ç‰¹å¾é‡è¦æ€§ç±»å‹

LightGBMæä¾›ä¸¤ç§ç‰¹å¾é‡è¦æ€§ï¼š

1. **Splité‡è¦æ€§ï¼ˆåˆ†è£‚é‡è¦æ€§ï¼‰**
   - åŸºäºç‰¹å¾ä½œä¸ºåˆ†è£‚èŠ‚ç‚¹çš„æ¬¡æ•°
   - è¡¡é‡ç‰¹å¾åœ¨åˆ†è£‚å†³ç­–ä¸­çš„ä½¿ç”¨é¢‘ç‡

2. **Gainé‡è¦æ€§ï¼ˆå¢ç›Šé‡è¦æ€§ï¼‰**
   - åŸºäºåˆ†è£‚å¸¦æ¥çš„ä¿¡æ¯å¢ç›Š
   - è¡¡é‡ç‰¹å¾å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®åº¦

**ä»£ç ç¤ºä¾‹**

```python
import lightgbm as lgb
import matplotlib.pyplot as plt

# è®­ç»ƒæ¨¡å‹
model = lgb.train(params, train_data, num_boost_round=1000,
                  valid_sets=[train_data, val_data],
                  callbacks=[lgb.early_stopping(stopping_rounds=50)])

# è·å–ç‰¹å¾é‡è¦æ€§
split_importance = model.feature_importance(importance_type='split')
gain_importance = model.feature_importance(importance_type='gain')

# æ‰“å°
print("Splité‡è¦æ€§ï¼ˆå‰10ï¼‰ï¼š")
for i, (idx, imp) in enumerate(sorted(enumerate(split_importance),
                                      key=lambda x: x[1], reverse=True)[:10]):
    print(f"  {idx+1}. ç‰¹å¾{idx}: {imp}")

print("\nGainé‡è¦æ€§ï¼ˆå‰10ï¼‰ï¼š")
for i, (idx, imp) in enumerate(sorted(enumerate(gain_importance),
                                      key=lambda x: x[1], reverse=True)[:10]):
    print(f"  {idx+1}. ç‰¹å¾{idx}: {imp:.2f}")
```

## 2. ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–

### 2.1 æ¡å½¢å›¾å¯è§†åŒ–

```python
def plot_feature_importance(model, feature_names, importance_type='gain', top_n=20):
    """
    ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾

    å‚æ•°:
        model: LightGBMæ¨¡å‹
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        importance_type: 'split' æˆ– 'gain'
        top_n: æ˜¾ç¤ºå‰nä¸ªç‰¹å¾
    """
    # è·å–é‡è¦æ€§
    importance = model.feature_importance(importance_type=importance_type)

    # æ’åº
    indices = np.argsort(importance)[::-1][:top_n]
    importance = importance[indices]
    feature_names = np.array(feature_names)[indices]

    # ç»˜åˆ¶
    plt.figure(figsize=(12, 8))
    plt.barh(range(len(importance)), importance[::-1])
    plt.yticks(range(len(importance)), feature_names[::-1])
    plt.xlabel(f'Feature Importance ({importance_type})')
    plt.title(f'Top {top_n} Feature Importance')
    plt.tight_layout()
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
feature_names = [f'factor_{i}' for i in range(X.shape[1])]
plot_feature_importance(model, feature_names, importance_type='gain', top_n=20)
```

### 2.2 å¯¹æ•°é‡è¦æ€§å›¾

```python
def plot_feature_importance_log(model, feature_names, importance_type='gain'):
    """
    ç»˜åˆ¶å¯¹æ•°ç‰¹å¾é‡è¦æ€§å›¾

    é€‚ç”¨äºç‰¹å¾é‡è¦æ€§å·®å¼‚å·¨å¤§çš„æƒ…å†µ
    """
    importance = model.feature_importance(importance_type=importance_type)
    importance = np.maximum(importance, 1e-6)  # é¿å…log(0)

    indices = np.argsort(importance)[::-1]
    importance_log = np.log(importance[indices])

    plt.figure(figsize=(12, 8))
    plt.plot(range(len(importance_log)), importance_log, marker='o')
    plt.xlabel('Feature Rank')
    plt.ylabel('Log Feature Importance')
    plt.title(f'Feature Importance Distribution (Log Scale) - {importance_type}')
    plt.grid(True)
    plt.show()

plot_feature_importance_log(model, feature_names, importance_type='gain')
```

## 3. åŸºäºPermutationçš„ç‰¹å¾é‡è¦æ€§

### 3.1 Permutation ImportanceåŸç†

**æ ¸å¿ƒæ€æƒ³**

Permutation Importanceé€šè¿‡æ‰“ä¹±ç‰¹å¾çš„å€¼æ¥è¯„ä¼°å…¶é‡è¦æ€§ï¼š

1. è®¡ç®—åŸºå‡†æ¨¡å‹æ€§èƒ½
2. æ‰“ä¹±æŸä¸ªç‰¹å¾çš„å€¼
3. é‡æ–°è®¡ç®—æ¨¡å‹æ€§èƒ½
4. æ€§èƒ½ä¸‹é™è¶Šå¤§ï¼Œç‰¹å¾è¶Šé‡è¦

**ä¼˜åŠ¿**

- ä¸ä¾èµ–äºæ¨¡å‹ç±»å‹
- è€ƒè™‘ç‰¹å¾é—´çš„äº¤äº’ä½œç”¨
- æ›´çœŸå®åæ˜ ç‰¹å¾é‡è¦æ€§

**ä»£ç å®ç°**

```python
from sklearn.metrics import mean_squared_error
from scipy.stats import pearsonr

def permutation_importance(model, X, y, metric='ic', n_repeats=5, random_state=42):
    """
    è®¡ç®—Permutation Importance

    å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        X: ç‰¹å¾çŸ©é˜µ
        y: ç›®æ ‡å˜é‡
        metric: è¯„ä¼°æŒ‡æ ‡ ('rmse', 'r2', 'ic', 'rank_ic')
        n_repeats: é‡å¤æ¬¡æ•°
        random_state: éšæœºç§å­

    è¿”å›:
        importances: ç‰¹å¾é‡è¦æ€§æ•°ç»„ï¼Œshape=[n_features, n_repeats]
    """
    np.random.seed(random_state)
    n_features = X.shape[1]
    importances = np.zeros((n_features, n_repeats))

    # è®¡ç®—åŸºå‡†æ€§èƒ½
    y_pred = model.predict(X)
    if metric == 'rmse':
        baseline_score = np.sqrt(mean_squared_error(y, y_pred))
    elif metric == 'r2':
        from sklearn.metrics import r2_score
        baseline_score = r2_score(y, y_pred)
    elif metric == 'ic':
        baseline_score = pearsonr(y_pred, y)[0]
    elif metric == 'rank_ic':
        from scipy.stats import spearmanr
        baseline_score = spearmanr(y_pred, y)[0]
    else:
        raise ValueError(f"Unknown metric: {metric}")

    print(f"Baseline {metric}: {baseline_score:.4f}")

    # å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡ŒPermutation
    for feature_idx in range(n_features):
        print(f"Processing feature {feature_idx + 1}/{n_features}")

        for repeat in range(n_repeats):
            # æ‰“ä¹±ç‰¹å¾
            X_permuted = X.copy()
            np.random.shuffle(X_permuted[:, feature_idx])

            # é‡æ–°é¢„æµ‹
            y_pred_permuted = model.predict(X_permuted)

            # è®¡ç®—æ€§èƒ½
            if metric == 'rmse':
                score = np.sqrt(mean_squared_error(y, y_pred_permuted))
                importance = score - baseline_score  # RMSEå¢åŠ ï¼Œé‡è¦
            elif metric == 'r2':
                score = r2_score(y, y_pred_permuted)
                importance = baseline_score - score  # R2å‡å°‘ï¼Œé‡è¦
            elif metric == 'ic':
                score = pearsonr(y_pred_permuted, y)[0]
                importance = baseline_score - score  # ICå‡å°‘ï¼Œé‡è¦
            elif metric == 'rank_ic':
                score = spearmanr(y_pred_permuted, y)[0]
                importance = baseline_score - score  # Rank ICå‡å°‘ï¼Œé‡è¦

            importances[feature_idx, repeat] = importance

    return importances

# ä½¿ç”¨ç¤ºä¾‹
importances = permutation_importance(model, X_val, y_val, metric='ic', n_repeats=5)

# è®¡ç®—å¹³å‡é‡è¦æ€§
mean_importance = importances.mean(axis=1)
std_importance = importances.std(axis=1)

# æ’åº
sorted_indices = np.argsort(mean_importance)[::-1]

print("\nPermutation Importance (IC):")
for i, idx in enumerate(sorted_indices[:10]):
    print(f"  {i+1}. ç‰¹å¾{idx}: {mean_importance[idx]:.4f} Â± {std_importance[idx]:.4f}")
```

### 3.2 å¯è§†åŒ–Permutation Importance

```python
def plot_permutation_importance(importances, feature_names, top_n=20):
    """
    ç»˜åˆ¶Permutation Importance

    å‚æ•°:
        importances: ç‰¹å¾é‡è¦æ€§æ•°ç»„ï¼Œshape=[n_features, n_repeats]
        feature_names: ç‰¹å¾åç§°
        top_n: æ˜¾ç¤ºå‰nä¸ªç‰¹å¾
    """
    # è®¡ç®—å¹³å‡é‡è¦æ€§
    mean_imp = importances.mean(axis=1)
    std_imp = importances.std(axis=1)

    # æ’åº
    indices = np.argsort(mean_imp)[::-1][:top_n]
    mean_imp = mean_imp[indices]
    std_imp = std_imp[indices]
    feature_names = np.array(feature_names)[indices]

    # ç»˜åˆ¶
    plt.figure(figsize=(12, 8))
    plt.barh(range(len(mean_imp)), mean_imp[::-1], xerr=std_imp[::-1],
             color='steelblue', alpha=0.7, capsize=3)
    plt.yticks(range(len(mean_imp)), feature_names[::-1])
    plt.xlabel('Permutation Importance')
    plt.title(f'Top {top_n} Permutation Feature Importance')
    plt.tight_layout()
    plt.show()

plot_permutation_importance(importances, feature_names, top_n=20)
```

## 4. SHAPå€¼åˆ†æ

### 4.1 SHAPåŸç†

**SHAPï¼ˆSHapley Additive exPlanationsï¼‰**

SHAPå€¼åŸºäºåšå¼ˆè®ºä¸­çš„Shapleyå€¼ï¼Œæä¾›ä¸€è‡´çš„å±€éƒ¨è§£é‡Šã€‚

**æ ¸å¿ƒæ€æƒ³**

æ¯ä¸ªç‰¹å¾å¯¹é¢„æµ‹çš„è´¡çŒ®ï¼š

$$ \hat{y}_i = \text{Base Value} + \sum_{j=1}^M \text{SHAP}_{i,j} $$

å…¶ä¸­ï¼š
- $\hat{y}_i$ æ˜¯ç¬¬iä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼
- Base Valueæ˜¯æ‰€æœ‰æ ·æœ¬çš„å‡å€¼é¢„æµ‹
- $\text{SHAP}_{i,j}$ æ˜¯ç¬¬jä¸ªç‰¹å¾å¯¹ç¬¬iä¸ªæ ·æœ¬çš„è´¡çŒ®

**ä»£ç å®ç°**

```python
import shap

# è®¡ç®—SHAPå€¼
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)

# SHAP Summary Plot
shap.summary_plot(shap_values, X, feature_names=feature_names, plot_type='bar')

# SHAP Summary Plot (è¯¦ç»†)
shap.summary_plot(shap_values, X, feature_names=feature_names)
```

### 4.2 ç‰¹å¾é‡è¦æ€§æ’åº

```python
def shap_feature_importance(shap_values, feature_names, top_n=20):
    """
    åŸºäºSHAPå€¼çš„ç‰¹å¾é‡è¦æ€§

    å‚æ•°:
        shap_values: SHAPå€¼æ•°ç»„
        feature_names: ç‰¹å¾åç§°
        top_n: æ˜¾ç¤ºå‰nä¸ªç‰¹å¾

    è¿”å›:
        é‡è¦æ€§æ’åºç»“æœ
    """
    # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡ç»å¯¹SHAPå€¼
    mean_abs_shap = np.abs(shap_values).mean(axis=0)

    # æ’åº
    indices = np.argsort(mean_abs_shap)[::-1][:top_n]
    importance = mean_abs_shap[indices]
    names = np.array(feature_names)[indices]

    # æ‰“å°
    print("SHAP Feature Importance:")
    for i, (name, imp) in enumerate(zip(names, importance)):
        print(f"  {i+1}. {name}: {imp:.4f}")

    return indices, importance, names

# ä½¿ç”¨ç¤ºä¾‹
indices, importance, names = shap_feature_importance(shap_values, feature_names, top_n=20)
```

### 4.3 ä¸ªä½“è§£é‡Š

```python
def plot_shap_force(explainer, X, sample_idx, feature_names):
    """
    ç»˜åˆ¶å•ä¸ªæ ·æœ¬çš„SHAP Force Plot

    å‚æ•°:
        explainer: SHAPè§£é‡Šå™¨
        X: ç‰¹å¾çŸ©é˜µ
        sample_idx: æ ·æœ¬ç´¢å¼•
        feature_names: ç‰¹å¾åç§°
    """
    # è®¡ç®—SHAPå€¼
    shap_values = explainer.shap_values(X[[sample_idx]])

    # ç»˜åˆ¶Force Plot
    shap.force_plot(explainer.expected_value[0],
                   shap_values[0],
                   X[sample_idx],
                   feature_names=feature_names)

# ä½¿ç”¨ç¤ºä¾‹
plot_shap_force(explainer, X_val, sample_idx=0, feature_names=feature_names)
```

## 5. ç‰¹å¾ç›¸å…³æ€§åˆ†æ

### 5.1 ä¸ºä»€ä¹ˆç‰¹å¾ç›¸å…³æ€§é‡è¦ï¼Ÿ

é«˜ç›¸å…³çš„ç‰¹å¾ï¼š
- æä¾›å†—ä½™ä¿¡æ¯
- å¯èƒ½å¯¼è‡´é‡è¦æ€§"åˆ†æ•£"
- å¢åŠ æ¨¡å‹å¤æ‚åº¦ä½†ä¸å¢åŠ ä»·å€¼

**å»ºè®®ï¼š**
- åŒç±»ç‰¹å¾ä¿ç•™æœ€é‡è¦çš„ 1-2 ä¸ª
- ç›¸å…³æ€§ > 0.9 çš„ç‰¹å¾è€ƒè™‘åˆå¹¶æˆ–å‰”é™¤

### 5.2 ç›¸å…³æ€§è®¡ç®—

```python
def analyze_feature_correlation(X, features, threshold=0.7):
    """
    åˆ†æç‰¹å¾ç›¸å…³æ€§

    å‚æ•°:
        X: ç‰¹å¾ DataFrame
        features: ç‰¹å¾åˆ—è¡¨
        threshold: é«˜ç›¸å…³é˜ˆå€¼

    è¿”å›:
        high_corr_pairs: é«˜ç›¸å…³ç‰¹å¾å¯¹
    """
    # è®¡ç®—ç›¸å…³çŸ©é˜µ
    corr_matrix = X[features].corr()

    # æ‰¾é«˜ç›¸å…³å¯¹
    high_corr_pairs = []
    for i, feat1 in enumerate(features):
        for feat2 in features[i+1:]:
            corr = corr_matrix.loc[feat1, feat2]
            if abs(corr) > threshold:
                high_corr_pairs.append((feat1, feat2, corr))

    # æ’åº
    high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)

    # æ‰“å°
    print(f"\nğŸ“Š é«˜ç›¸å…³ç‰¹å¾å¯¹ (|corr| > {threshold}):")
    print("-" * 50)
    for feat1, feat2, corr in high_corr_pairs:
        print(f"  {feat1:12s} â†” {feat2:12s} : {corr:.3f}")

    return high_corr_pairs

# ä½¿ç”¨ç¤ºä¾‹
high_corr_pairs = analyze_feature_correlation(X_train, list(FEATURES.keys()), threshold=0.7)
```

**ç¤ºä¾‹è¾“å‡ºï¼š**

```
é«˜ç›¸å…³ç‰¹å¾å¯¹ (|corr| > 0.7):
--------------------------------------------------
RET_5D       â†” BIAS_10     :  0.931
RET_10D      â†” BIAS_20     :  0.924
BIAS_10      â†” BIAS_20     :  0.873
BIAS_5       â†” BIAS_10     :  0.860
RET_10D      â†” BIAS_10     :  0.834
RET_20D      â†” BIAS_20     :  0.829
RET_5D       â†” BIAS_5      :  0.818
RET_1D       â†” BODY        :  0.814
RET_5D       â†” BIAS_20     :  0.769
VOL_10       â†” VOL_20      :  0.745
```

### 5.3 ç›¸å…³æ€§çƒ­åŠ›å›¾å¯è§†åŒ–

```python
def plot_correlation_heatmap(X, features, top_n=30):
    """
    ç»˜åˆ¶ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾

    å‚æ•°:
        X: ç‰¹å¾ DataFrame
        features: ç‰¹å¾åˆ—è¡¨
        top_n: æ˜¾ç¤ºå‰nä¸ªç‰¹å¾
    """
    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    corr_matrix = X[features[:top_n]].corr()

    # ç»˜åˆ¶çƒ­åŠ›å›¾
    import seaborn as sns
    plt.figure(figsize=(12, 10))

    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    sns.heatmap(corr_matrix,
                mask=mask,
                cmap='coolwarm',
                center=0,
                square=True,
                linewidths=1,
                cbar_kws={"shrink": 0.8},
                annot=False,
                fmt='.2f',
                xticklabels=1,
                yticklabels=1)

    plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')
    plt.xlabel('Features', fontsize=12)
    plt.ylabel('Features', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
plot_correlation_heatmap(X_train, list(FEATURES.keys()), top_n=30)
```

### 5.4 åŸºäºç›¸å…³æ€§çš„ç‰¹å¾ç­›é€‰

```python
def filter_features_by_correlation(X, features, threshold=0.9, importance=None):
    """
    åŸºäºç›¸å…³æ€§ç­›é€‰ç‰¹å¾

    ç­–ç•¥ï¼š
    1. æ‰¾åˆ°é«˜ç›¸å…³ç‰¹å¾å¯¹
    2. å¦‚æœä¸¤ä¸ªç‰¹å¾ç›¸å…³æ€§ > threshold
    3. ä¿ç•™é‡è¦æ€§æ›´é«˜çš„ç‰¹å¾

    å‚æ•°:
        X: ç‰¹å¾ DataFrame
        features: ç‰¹å¾åˆ—è¡¨
        threshold: ç›¸å…³æ€§é˜ˆå€¼
        importance: ç‰¹å¾é‡è¦æ€§å­—å…¸ {feature: importance}

    è¿”å›:
        selected_features: ç­›é€‰åçš„ç‰¹å¾åˆ—è¡¨
    """
    # è®¡ç®—ç›¸å…³çŸ©é˜µ
    corr_matrix = X[features].corr()

    # å¦‚æœæ²¡æœ‰æä¾›é‡è¦æ€§ï¼Œä½¿ç”¨é»˜è®¤å€¼
    if importance is None:
        importance = {feat: 1.0 for feat in features}

    # æ ‡è®°è¦åˆ é™¤çš„ç‰¹å¾
    to_remove = set()

    for i, feat1 in enumerate(features):
        for feat2 in features[i+1:]:
            corr = abs(corr_matrix.loc[feat1, feat2])

            if corr > threshold:
                # ä¿ç•™é‡è¦æ€§æ›´é«˜çš„ç‰¹å¾
                if importance[feat1] >= importance[feat2]:
                    to_remove.add(feat2)
                    print(f"ç§»é™¤ {feat2} (ä¸ {feat1} ç›¸å…³æ€§={corr:.3f})")
                else:
                    to_remove.add(feat1)
                    print(f"ç§»é™¤ {feat1} (ä¸ {feat2} ç›¸å…³æ€§={corr:.3f})")

    # ç­›é€‰ç‰¹å¾
    selected_features = [f for f in features if f not in to_remove]

    print(f"\nåŸå§‹ç‰¹å¾æ•°: {len(features)}")
    print(f"ç­›é€‰åç‰¹å¾æ•°: {len(selected_features)}")
    print(f"åˆ é™¤ç‰¹å¾æ•°: {len(to_remove)}")

    return selected_features

# ä½¿ç”¨ç¤ºä¾‹
importance_dict = dict(zip(feature_names, gain_importance))
selected_features = filter_features_by_correlation(
    X_train, 
    list(FEATURES.keys()),
    threshold=0.9,
    importance=importance_dict
)
```

## 6. é‡è®­ç»ƒéªŒè¯

### 6.1 éªŒè¯ç‰¹å¾é€‰æ‹©çš„æ•ˆæœ

```python
def validate_feature_selection(X_train, y_train, X_valid, y_valid, X_test, y_test,
                         full_features, selected_features, params):
    """
    éªŒè¯ç‰¹å¾é€‰æ‹©çš„æ•ˆæœ

    å‚æ•°:
        X_train, y_train: è®­ç»ƒæ•°æ®
        X_valid, y_valid: éªŒè¯æ•°æ®
        X_test, y_test: æµ‹è¯•æ•°æ®
        full_features: æ‰€æœ‰ç‰¹å¾
        selected_features: é€‰ä¸­çš„ç‰¹å¾
        params: æ¨¡å‹å‚æ•°

    è¿”å›:
        comparison: å¯¹æ¯”ç»“æœ
    """
    import lightgbm as lgb
    from scipy.stats import pearsonr

    # è®­ç»ƒå…¨ç‰¹å¾æ¨¡å‹
    print("\nè®­ç»ƒå…¨ç‰¹å¾æ¨¡å‹...")
    train_data_full = lgb.Dataset(X_train[full_features], label=y_train)
    val_data_full = lgb.Dataset(X_valid[full_features], label=y_valid, reference=train_data_full)

    model_full = lgb.train(
        params,
        train_data_full,
        num_boost_round=1000,
        valid_sets=[train_data_full, val_data_full],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50, verbose=False),
            lgb.log_evaluation(period=100)
        ]
    )

    # è®­ç»ƒç²¾é€‰ç‰¹å¾æ¨¡å‹
    print("\nè®­ç»ƒç²¾é€‰ç‰¹å¾æ¨¡å‹...")
    train_data_selected = lgb.Dataset(X_train[selected_features], label=y_train)
    val_data_selected = lgb.Dataset(X_valid[selected_features], label=y_valid, reference=train_data_selected)

    model_selected = lgb.train(
        params,
        train_data_selected,
        num_boost_round=1000,
        valid_sets=[train_data_selected, val_data_selected],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50, verbose=False),
            lgb.log_evaluation(period=100)
        ]
    )

    # é¢„æµ‹
    pred_full = model_full.predict(X_test[full_features])
    pred_selected = model_selected.predict(X_test[selected_features])

    # è®¡ç®—IC
    ic_full = pearsonr(pred_full, y_test)[0]
    ic_selected = pearsonr(pred_selected, y_test)[0]

    # æ‰“å°å¯¹æ¯”
    print("\nğŸ”¬ ç‰¹å¾é€‰æ‹©éªŒè¯:")
    print("-" * 50)
    print(f"{'æ¨¡å‹':<15s} {'ç‰¹å¾æ•°':<10s} {'æµ‹è¯•é›† IC':<10s}")
    print("-" * 50)
    print(f"{'å…¨éƒ¨ç‰¹å¾':<15s} {len(full_features):<10d} {ic_full:<10.4f}")
    print(f"{'ç²¾é€‰ç‰¹å¾':<15s} {len(selected_features):<10d} {ic_selected:<10.4f}")
    print("-" * 50)

    # è¯„ä¼°
    ic_ratio = ic_selected / ic_full if ic_full != 0 else 0
    print(f"\nICä¿æŒç‡: {ic_ratio:.2%}")

    if ic_selected >= ic_full * 0.95:
        print("âœ… ç²¾é€‰ç‰¹å¾è¡¨ç°æ¥è¿‘å…¨ç‰¹å¾ï¼Œå¯ä»¥ç®€åŒ–æ¨¡å‹!")
    elif ic_selected >= ic_full * 0.90:
        print("âš ï¸ ç²¾é€‰ç‰¹å¾æ€§èƒ½ç•¥æœ‰ä¸‹é™ï¼Œä½†ç®€åŒ–æ¨¡å‹å¯èƒ½å€¼å¾—")
    else:
        print("âŒ ç²¾é€‰ç‰¹å¾æ€§èƒ½ä¸‹é™è¾ƒå¤šï¼Œéœ€è¦è°ƒæ•´")

    return {
        'full_features_ic': ic_full,
        'selected_features_ic': ic_selected,
        'n_full': len(full_features),
        'n_selected': len(selected_features),
        'ic_ratio': ic_ratio,
        'model_full': model_full,
        'model_selected': model_selected
    }

# ä½¿ç”¨ç¤ºä¾‹
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'verbose': -1,
}

comparison = validate_feature_selection(
    X_train, y_train, X_valid, y_valid, X_test, y_test,
    full_features=list(FEATURES.keys()),
    selected_features=selected_features,
    params=params
)
```

### 6.2 ç‰¹å¾é€‰æ‹©æ•ˆæœå¯è§†åŒ–

```python
def plot_feature_selection_comparison(comparison):
    """
    å¯è§†åŒ–ç‰¹å¾é€‰æ‹©å¯¹æ¯”ç»“æœ

    å‚æ•°:
        comparison: validate_feature_selection çš„è¿”å›ç»“æœ
    """
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # ç‰¹å¾æ•°é‡å¯¹æ¯”
    models = ['å…¨ç‰¹å¾', 'ç²¾é€‰ç‰¹å¾']
    n_features = [comparison['n_full'], comparison['n_selected']]
    ics = [comparison['full_features_ic'], comparison['selected_features_ic']]

    # ç‰¹å¾æ•° vs IC
    axes[0].scatter(n_features, ics, s=200, alpha=0.6)
    axes[0].plot(n_features, ics, 'r-', linewidth=2, alpha=0.5)

    # æ·»åŠ æ ‡æ³¨
    for i, (n, ic, model) in enumerate(zip(n_features, ics, models)):
        axes[0].annotate(f'{model}\n{n}ç‰¹å¾\nIC={ic:.4f}',
                        (n, ic),
                        textcoords="offset points",
                        xytext=(0, 10),
                        ha='center')

    axes[0].set_xlabel('ç‰¹å¾æ•°é‡')
    axes[0].set_ylabel('æµ‹è¯•é›† IC')
    axes[0].set_title('ç‰¹å¾æ•°é‡ vs æ€§èƒ½')
    axes[0].grid(True, alpha=0.3)

    # IC æŸ±çŠ¶å›¾
    bars = axes[1].bar(models, ics, color=['steelblue', 'coral'], alpha=0.7, edgecolor='black')
    axes[1].set_ylabel('æµ‹è¯•é›† IC')
    axes[1].set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
    axes[1].grid(True, axis='y', alpha=0.3)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, ic in zip(bars, ics):
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width()/2., height,
                     f'{ic:.4f}',
                     ha='center', va='bottom', fontweight='bold')

    # æ·»åŠ ICä¿æŒç‡
    axes[1].axhline(y=comparison['full_features_ic'] * 0.95,
                   color='green', linestyle='--', alpha=0.5,
                   label='95% é˜ˆå€¼')
    axes[1].legend()

    plt.tight_layout()
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
plot_feature_selection_comparison(comparison)
```

### 6.3 æ—¶åºç‰¹å¾é‡è¦æ€§

## 5. æ—¶åºç‰¹å¾é‡è¦æ€§

### 5.1 æ»šåŠ¨çª—å£ç‰¹å¾é‡è¦æ€§

**æ ¸å¿ƒæ€æƒ³**

åœ¨ä¸åŒæ—¶é—´çª—å£å†…è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼Œåˆ†æé‡è¦æ€§çš„ç¨³å®šæ€§ã€‚

**ä»£ç å®ç°**

```python
def rolling_feature_importance(X, y, model_class, params,
                               window_size=252, step_size=21):
    """
    æ»šåŠ¨çª—å£ç‰¹å¾é‡è¦æ€§

    å‚æ•°:
        X: ç‰¹å¾çŸ©é˜µï¼Œshape=[n_samples, n_features]
        y: ç›®æ ‡å˜é‡
        model_class: æ¨¡å‹ç±»
        params: æ¨¡å‹å‚æ•°
        window_size: çª—å£å¤§å°
        step_size: æ­¥é•¿

    è¿”å›:
        importance_history: ç‰¹å¾é‡è¦æ€§å†å²
    """
    n_samples = len(X)
    importance_history = []

    start_idx = window_size
    while start_idx + step_size <= n_samples:
        print(f"Processing window: {start_idx - window_size} - {start_idx}")

        # åˆ’åˆ†æ•°æ®
        X_window = X[start_idx - window_size:start_idx]
        y_window = y[start_idx - window_size:start_idx]

        # è®­ç»ƒæ¨¡å‹
        model = model_class(**params)
        model.fit(X_window, y_window)

        # è®¡ç®—ç‰¹å¾é‡è¦æ€§
        importance = model.feature_importance(importance_type='gain')
        importance_history.append(importance)

        # æ»šåŠ¨çª—å£
        start_idx += step_size

    return np.array(importance_history)

# ä½¿ç”¨ç¤ºä¾‹
importance_history = rolling_feature_importance(
    X, y, LGBMRegressor, params,
    window_size=252,  # 1å¹´
    step_size=21      # 1ä¸ªæœˆ
)

print(f"é‡è¦æ€§å†å²: {importance_history.shape}")
```

### 5.2 ç‰¹å¾é‡è¦æ€§ç¨³å®šæ€§åˆ†æ

```python
def analyze_importance_stability(importance_history, feature_names, top_n=10):
    """
    åˆ†æç‰¹å¾é‡è¦æ€§çš„ç¨³å®šæ€§

    å‚æ•°:
        importance_history: ç‰¹å¾é‡è¦æ€§å†å²ï¼Œshape=[n_windows, n_features]
        feature_names: ç‰¹å¾åç§°
        top_n: åˆ†æå‰nä¸ªç‰¹å¾
    """
    # è®¡ç®—ç»Ÿè®¡é‡
    mean_importance = importance_history.mean(axis=0)
    std_importance = importance_history.std(axis=0)
    cv_importance = std_importance / (mean_importance + 1e-6)  # å˜å¼‚ç³»æ•°

    # æ’åº
    sorted_indices = np.argsort(mean_importance)[::-1][:top_n]

    # æ‰“å°
    print(f"{'ç‰¹å¾':<20} {'å‡å€¼':>10} {'æ ‡å‡†å·®':>10} {'å˜å¼‚ç³»æ•°':>10}")
    print("-" * 60)
    for i, idx in enumerate(sorted_indices):
        print(f"{feature_names[idx]:<20} "
              f"{mean_importance[idx]:>10.4f} "
              f"{std_importance[idx]:>10.4f} "
              f"{cv_importance[idx]:>10.4f}")

    # å¯è§†åŒ–
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))

    # å‡å€¼vsæ ‡å‡†å·®
    axes[0].scatter(mean_importance, std_importance, alpha=0.6)
    for idx in sorted_indices:
        axes[0].annotate(feature_names[idx],
                        (mean_importance[idx], std_importance[idx]))
    axes[0].set_xlabel('Mean Importance')
    axes[0].set_ylabel('Std Importance')
    axes[0].set_title('Importance Stability')
    axes[0].grid(True)

    # æ—¶é—´åºåˆ—
    for idx in sorted_indices:
        axes[1].plot(importance_history[:, idx], label=feature_names[idx])
    axes[1].set_xlabel('Time Window')
    axes[1].set_ylabel('Importance')
    axes[1].set_title(f'Top {top_n} Features Importance Over Time')
    axes[1].legend()
    axes[1].grid(True)

    plt.tight_layout()
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
analyze_importance_stability(importance_history, feature_names, top_n=10)
```

## 6. ç‰¹å¾é€‰æ‹©ç­–ç•¥

### 6.1 åŸºäºé‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©

```python
def select_features_by_importance(model, X, feature_names,
                                  importance_type='gain', threshold=0.01):
    """
    åŸºäºç‰¹å¾é‡è¦æ€§é€‰æ‹©ç‰¹å¾

    å‚æ•°:
        model: LightGBMæ¨¡å‹
        X: ç‰¹å¾çŸ©é˜µ
        feature_names: ç‰¹å¾åç§°
        importance_type: 'split' æˆ– 'gain'
        threshold: é‡è¦æ€§é˜ˆå€¼ï¼ˆæ¯”ä¾‹ï¼‰

    è¿”å›:
        X_selected: é€‰æ‹©åçš„ç‰¹å¾çŸ©é˜µ
        selected_features: é€‰æ‹©çš„ç‰¹å¾åç§°
        selected_indices: é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
    """
    # è·å–ç‰¹å¾é‡è¦æ€§
    importance = model.feature_importance(importance_type=importance_type)

    # å½’ä¸€åŒ–
    importance_normalized = importance / importance.sum()

    # é€‰æ‹©é‡è¦æ€§è¶…è¿‡é˜ˆå€¼çš„ç‰¹å¾
    selected_indices = np.where(importance_normalized >= threshold)[0]

    # æå–æ•°æ®
    X_selected = X[:, selected_indices]
    selected_features = np.array(feature_names)[selected_indices]

    print(f"é€‰æ‹© {len(selected_indices)}/{len(feature_names)} ä¸ªç‰¹å¾")
    print(f"ç´¯è®¡é‡è¦æ€§: {importance_normalized[selected_indices].sum():.2%}")

    return X_selected, selected_features, selected_indices

# ä½¿ç”¨ç¤ºä¾‹
X_selected, selected_features, selected_indices = select_features_by_importance(
    model, X, feature_names, importance_type='gain', threshold=0.01
)
```

### 6.2 é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰

```python
from sklearn.feature_selection import RFE

def recursive_feature_elimination(X, y, estimator, n_features_to_select=None,
                                 step=1, cv=5):
    """
    é€’å½’ç‰¹å¾æ¶ˆé™¤

    å‚æ•°:
        X: ç‰¹å¾çŸ©é˜µ
        y: ç›®æ ‡å˜é‡
        estimator: è¯„ä¼°å™¨
        n_features_to_select: ç›®æ ‡ç‰¹å¾æ•°
        step: æ¯æ¬¡æ¶ˆé™¤çš„ç‰¹å¾æ•°
        cv: äº¤å‰éªŒè¯æŠ˜æ•°

    è¿”å›:
        X_selected: é€‰æ‹©åçš„ç‰¹å¾çŸ©é˜µ
        selected_indices: é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
        rfe: RFEå¯¹è±¡
    """
    # åˆ›å»ºRFE
    rfe = RFE(estimator=estimator,
              n_features_to_select=n_features_to_select,
              step=step,
              importance_getter='auto')

    # æ‹Ÿåˆ
    rfe.fit(X, y)

    # æå–ç»“æœ
    selected_indices = np.where(rfe.support_)[0]
    X_selected = X[:, selected_indices]

    print(f"é€‰æ‹© {len(selected_indices)}/{X.shape[1]} ä¸ªç‰¹å¾")

    return X_selected, selected_indices, rfe

# ä½¿ç”¨ç¤ºä¾‹
estimator = LGBMRegressor(**params)
X_selected, selected_indices, rfe = recursive_feature_elimination(
    X, y, estimator, n_features_to_select=50, step=5
)
```

### 6.3 ç¨³å®šæ€§ç‰¹å¾é€‰æ‹©

```python
def stable_feature_selection(importance_history, feature_names,
                             stability_threshold=0.7, top_n=None):
    """
    ç¨³å®šæ€§ç‰¹å¾é€‰æ‹©

    å‚æ•°:
        importance_history: ç‰¹å¾é‡è¦æ€§å†å²
        feature_names: ç‰¹å¾åç§°
        stability_threshold: ç¨³å®šæ€§é˜ˆå€¼
        top_n: é€‰æ‹©å‰nä¸ªç¨³å®šç‰¹å¾

    è¿”å›:
        selected_features: é€‰æ‹©çš„ç‰¹å¾åç§°
        stability_scores: ç¨³å®šæ€§å¾—åˆ†
    """
    # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ’å
    rankings = []
    for importance in importance_history:
        ranking = np.argsort(importance)[::-1]
        rankings.append(ranking)

    rankings = np.array(rankings)

    # è®¡ç®—ç¨³å®šæ€§å¾—åˆ†ï¼ˆåŸºäºæ’åçš„æ–¹å·®ï¼‰
    stability_scores = []
    for feature_idx in range(len(feature_names)):
        feature_rankings = np.where(rankings == feature_idx)[1]
        stability_score = 1 / (1 + np.var(feature_rankings))
        stability_scores.append(stability_score)

    stability_scores = np.array(stability_scores)

    # æ’åº
    sorted_indices = np.argsort(stability_scores)[::-1]

    # é€‰æ‹©ç¨³å®šç‰¹å¾
    if top_n is None:
        selected_indices = sorted_indices[stability_scores[sorted_indices] >= stability_threshold]
    else:
        selected_indices = sorted_indices[:top_n]

    selected_features = np.array(feature_names)[selected_indices]

    print(f"é€‰æ‹© {len(selected_indices)} ä¸ªç¨³å®šç‰¹å¾")

    return selected_features, stability_scores

# ä½¿ç”¨ç¤ºä¾‹
selected_features, stability_scores = stable_feature_selection(
    importance_history, feature_names,
    stability_threshold=0.7, top_n=20
)
```

## 7. ç‰¹å¾é‡è¦æ€§åˆ†æçš„æœ€ä½³å®è·µ

### 7.1 ç»¼åˆåˆ†ææµç¨‹

```python
class FeatureImportanceAnalyzer:
    """
    ç‰¹å¾é‡è¦æ€§åˆ†æå™¨

    åŠŸèƒ½ï¼š
    1. è®¡ç®—å¤šç§ç‰¹å¾é‡è¦æ€§
    2. å¯è§†åŒ–åˆ†æç»“æœ
    3. æ—¶åºç¨³å®šæ€§åˆ†æ
    4. ç‰¹å¾é€‰æ‹©å»ºè®®
    """

    def __init__(self, model, X, y, feature_names):
        self.model = model
        self.X = X
        self.y = y
        self.feature_names = feature_names

        self.split_importance = None
        self.gain_importance = None
        self.permutation_importance = None
        self.shap_values = None

    def calculate_lightgbm_importance(self):
        """è®¡ç®—LightGBMå†…ç½®é‡è¦æ€§"""
        self.split_importance = self.model.feature_importance(importance_type='split')
        self.gain_importance = self.model.feature_importance(importance_type='gain')

    def calculate_permutation_importance(self, metric='ic', n_repeats=5):
        """è®¡ç®—Permutation Importance"""
        self.permutation_importance = permutation_importance(
            self.model, self.X, self.y,
            metric=metric, n_repeats=n_repeats
        )

    def calculate_shap_values(self):
        """è®¡ç®—SHAPå€¼"""
        explainer = shap.TreeExplainer(self.model)
        self.shap_values = explainer.shap_values(self.X)

    def analyze_all(self):
        """æ‰§è¡Œæ‰€æœ‰åˆ†æ"""
        print("è®¡ç®—LightGBMé‡è¦æ€§...")
        self.calculate_lightgbm_importance()

        print("è®¡ç®—Permutation Importance...")
        self.calculate_permutation_importance()

        print("è®¡ç®—SHAPå€¼...")
        self.calculate_shap_values()

    def plot_summary(self, top_n=20):
        """ç»˜åˆ¶æ±‡æ€»å›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # Splité‡è¦æ€§
        indices = np.argsort(self.split_importance)[::-1][:top_n]
        axes[0, 0].barh(range(len(indices)), self.split_importance[indices][::-1])
        axes[0, 0].set_yticks(range(len(indices)), np.array(self.feature_names)[indices][::-1])
        axes[0, 0].set_title('Split Importance')
        axes[0, 0].invert_yaxis()

        # Gainé‡è¦æ€§
        indices = np.argsort(self.gain_importance)[::-1][:top_n]
        axes[0, 1].barh(range(len(indices)), self.gain_importance[indices][::-1])
        axes[0, 1].set_yticks(range(len(indices)), np.array(self.feature_names)[indices][::-1])
        axes[0, 1].set_title('Gain Importance')
        axes[0, 1].invert_yaxis()

        # Permutation Importance
        if self.permutation_importance is not None:
            mean_imp = self.permutation_importance.mean(axis=1)
            indices = np.argsort(mean_imp)[::-1][:top_n]
            axes[1, 0].barh(range(len(indices)), mean_imp[indices][::-1])
            axes[1, 0].set_yticks(range(len(indices)), np.array(self.feature_names)[indices][::-1])
            axes[1, 0].set_title('Permutation Importance')
            axes[1, 0].invert_yaxis()

        # SHAPé‡è¦æ€§
        if self.shap_values is not None:
            mean_shap = np.abs(self.shap_values).mean(axis=0)
            indices = np.argsort(mean_shap)[::-1][:top_n]
            axes[1, 1].barh(range(len(indices)), mean_shap[indices][::-1])
            axes[1, 1].set_yticks(range(len(indices)), np.array(self.feature_names)[indices][::-1])
            axes[1, 1].set_title('SHAP Importance')
            axes[1, 1].invert_yaxis()

        plt.tight_layout()
        plt.show()

# ä½¿ç”¨ç¤ºä¾‹
analyzer = FeatureImportanceAnalyzer(model, X_val, y_val, feature_names)
analyzer.analyze_all()
analyzer.plot_summary(top_n=15)
```

### 7.2 æ£€æŸ¥æ¸…å•

**ç‰¹å¾é‡è¦æ€§åˆ†ææ£€æŸ¥æ¸…å•**

- [ ] è®¡ç®—è‡³å°‘ä¸¤ç§ä¸åŒçš„ç‰¹å¾é‡è¦æ€§
- [ ] å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§åˆ†å¸ƒ
- [ ] æ£€æŸ¥ç‰¹å¾é‡è¦æ€§çš„ç¨³å®šæ€§
- [ ] åˆ†æç‰¹å¾é—´çš„ç›¸å…³æ€§
- [ ] éªŒè¯ç‰¹å¾é€‰æ‹©çš„åˆç†æ€§
- [ ] è®°å½•åˆ†æè¿‡ç¨‹å’Œç»“è®º

**æ—¶åºç‰¹å¾é‡è¦æ€§åˆ†ææ£€æŸ¥æ¸…å•**

- [ ] ä½¿ç”¨æ»šåŠ¨çª—å£åˆ†æé‡è¦æ€§å˜åŒ–
- [ ] è¯†åˆ«ç¨³å®šå’Œä¸ç¨³å®šç‰¹å¾
- [ ] åˆ†æä¸åŒå¸‚åœºçŠ¶æ€ä¸‹çš„é‡è¦æ€§
- [ ] æ£€æŸ¥é‡è¦æ€§ä¸å¸‚åœºå‘¨æœŸçš„å…³ç³»
- [ ] åˆ¶å®šç‰¹å¾æ›´æ–°ç­–ç•¥

## 8. æ€»ç»“

ç‰¹å¾é‡è¦æ€§åˆ†ææ˜¯é‡åŒ–æ¨¡å‹å¼€å‘ä¸­çš„å…³é”®ç¯èŠ‚ï¼š

1. **åŸºç¡€é‡è¦æ€§**ï¼šLightGBMå†…ç½®çš„Splitå’ŒGainé‡è¦æ€§
2. **Permutation Importance**ï¼šæ›´çœŸå®çš„é‡è¦æ€§è¯„ä¼°æ–¹æ³•
3. **SHAPåˆ†æ**ï¼šæä¾›ä¸ªä½“å’Œå…¨å±€è§£é‡Š
4. **æ—¶åºåˆ†æ**ï¼šåˆ†æé‡è¦æ€§çš„ç¨³å®šæ€§
5. **ç‰¹å¾é€‰æ‹©**ï¼šåŸºäºé‡è¦æ€§çš„ç‰¹å¾ç­›é€‰ç­–ç•¥

æ­£ç¡®çš„ç‰¹å¾é‡è¦æ€§åˆ†æèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬ï¼š
- ç†è§£æ¨¡å‹å†³ç­–é€»è¾‘
- è¯†åˆ«æœ‰æ•ˆå› å­
- æå‡æ¨¡å‹æ€§èƒ½
- æ§åˆ¶æ¨¡å‹é£é™©
