# PyTorchæ¡†æ¶ç³»åˆ— - Tensorä¸æ¨¡å‹æ„å»º

## ğŸ“š ç³»åˆ—æ¦‚è¿°
æœ¬ç³»åˆ—æ–‡æ¡£æ¶µç›–PyTorchçš„æ ¸å¿ƒæ¦‚å¿µã€Tensoræ“ä½œã€è‡ªåŠ¨å¾®åˆ†ã€æ¨¡å‹å®šä¹‰å’Œå¸¸ç”¨å±‚ã€‚

---

## ğŸ“– æ–‡æ¡£åˆ—è¡¨

1. [PyTorchç®€ä»‹](#pytorchç®€ä»‹)
2. [TensoråŸºç¡€](#tensoråŸºç¡€)
3. [Autogradè‡ªåŠ¨å¾®åˆ†](#autogradè‡ªåŠ¨å¾®åˆ†)
4. [nn.Moduleæ¨¡å‹å®šä¹‰](#nnmoduleæ¨¡å‹å®šä¹‰)
5. [å¸¸ç”¨å±‚](#å¸¸ç”¨å±‚)

---

## PyTorchç®€ä»‹

### ä»€ä¹ˆæ˜¯PyTorch
- åŸºäºPythonçš„æ·±åº¦å­¦ä¹ æ¡†æ¶
- Facebookï¼ˆMetaï¼‰å¼€å‘
- å­¦æœ¯ç ”ç©¶é¦–é€‰æ¡†æ¶

### æ ¸å¿ƒç‰¹ç‚¹

#### 1. åŠ¨æ€è®¡ç®—å›¾
- è¿è¡Œæ—¶æ„å»ºè®¡ç®—å›¾
- çµæ´»æ€§é«˜
- é€‚åˆç ”ç©¶

#### 2. GPUåŠ é€Ÿ
- è‡ªåŠ¨åˆ©ç”¨GPU
- æ˜¾è‘—æå‡è®­ç»ƒé€Ÿåº¦
- æ”¯æŒCUDA

#### 3. è‡ªåŠ¨å¾®åˆ†
- è‡ªåŠ¨è®¡ç®—æ¢¯åº¦
- ç®€åŒ–åå‘ä¼ æ’­
- æ”¯æŒå¤æ‚è®¡ç®—å›¾

#### 4. ä¸°å¯Œçš„API
- é¢„å®šä¹‰å±‚å’Œæ¨¡å‹
- ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
- æ•°æ®å¤„ç†å·¥å…·

### ä¸ºä»€ä¹ˆé‡åŒ–æŠ•èµ„ç”¨PyTorch

**1. çµæ´»æ€§å¼º**
- å¯ä»¥è‡ªå®šä¹‰å¤æ‚çš„æ¨¡å‹ç»“æ„
- å®¹æ˜“å®ç°ç ”ç©¶æƒ³æ³•

**2. ç¤¾åŒºæ´»è·ƒ**
- å¤§é‡æ•™ç¨‹å’Œç¤ºä¾‹
- é—®é¢˜å®¹æ˜“è§£å†³

**3. æ˜“äºéƒ¨ç½²**
- æ”¯æŒå¯¼å‡ºä¸ºå¤šç§æ ¼å¼
- ç”Ÿäº§ç¯å¢ƒå‹å¥½

---

## TensoråŸºç¡€

### Tensorå®šä¹‰
- PyTorchçš„æ ¸å¿ƒæ•°æ®ç»“æ„
- ç±»ä¼¼äºNumPyæ•°ç»„
- å¯ä»¥è¿è¡Œåœ¨GPUä¸Š

### åˆ›å»ºTensor

```python
import torch

# ä»Pythonåˆ—è¡¨åˆ›å»º
t1 = torch.tensor([1, 2, 3, 4])

# ä»NumPyåˆ›å»º
import numpy as np
np_array = np.array([[1, 2], [3, 4]])
t2 = torch.from_numpy(np_array)

# åˆ›å»ºç‰¹æ®ŠTensor
zeros = torch.zeros(2, 3)        # å…¨é›¶
ones = torch.ones(2, 3)          # å…¨ä¸€
random = torch.randn(2, 3)       # æ ‡å‡†æ­£æ€åˆ†å¸ƒ

# åˆ›å»ºåºåˆ—
arange = torch.arange(0, 10)      # 0-9
linspace = torch.linspace(0, 10, 5)  # 0åˆ°10ï¼Œ5ä¸ªç‚¹
```

### Tensoræ“ä½œ

#### åŸºæœ¬è¿ç®—

```python
a = torch.tensor([1, 2, 3])
b = torch.tensor([4, 5, 6])

# åŠ æ³•ã€å‡æ³•ã€ä¹˜æ³•ã€é™¤æ³•
c = a + b
d = a - b
e = a * b
f = a / b

# ç‚¹ç§¯
dot = torch.dot(a, b)

# çŸ©é˜µä¹˜æ³•
mat_a = torch.randn(2, 3)
mat_b = torch.randn(3, 4)
mat_c = torch.mm(mat_a, mat_b)  # æˆ– mat_a @ mat_b
```

#### ç»Ÿè®¡è¿ç®—

```python
x = torch.randn(10)

# å‡å€¼ã€æ ‡å‡†å·®ã€æ–¹å·®
mean = x.mean()
std = x.std()
var = x.var()

# æœ€å¤§å€¼ã€æœ€å°å€¼
max_val = x.max()
min_val = x.min()

# æ±‚å’Œ
sum_val = x.sum()
```

#### å½¢çŠ¶æ“ä½œ

```python
x = torch.arange(12)

# reshape
x_reshaped = x.view(3, 4)  # æˆ– x.reshape(3, 4)

# è½¬ç½®
x_transposed = x_reshaped.t()

# squeeze: å»é™¤ç»´åº¦ä¸º1çš„
x_squeezed = torch.randn(1, 10, 1).squeeze()

# unsqueeze: å¢åŠ ç»´åº¦
x_unsqueezed = x.unsqueeze(0)
```

### Tensor vs NumPy

| ç‰¹æ€§ | NumPy | Tensor |
|------|-------|--------|
| **GPUæ”¯æŒ** | âŒ | âœ… |
| **è‡ªåŠ¨å¾®åˆ†** | âŒ | âœ… |
| **æ€§èƒ½** | CPU | CPU/GPU |
| **API** | ç±»ä¼¼ | ç±»ä¼¼ |
| **äº’æ“ä½œ** | æ˜“ | æ˜“ |

#### äº’æ“ä½œ

```python
# NumPy â†’ Tensor
np_array = np.array([1, 2, 3])
tensor = torch.from_numpy(np_array)

# Tensor â†’ NumPy
array = tensor.numpy()
```

---

## Autogradè‡ªåŠ¨å¾®åˆ†

### Autogradæ¦‚è¿°
- PyTorchçš„è‡ªåŠ¨å¾®åˆ†å¼•æ“
- è‡ªåŠ¨è®¡ç®—æ¢¯åº¦
- æ”¯æŒå¤æ‚çš„è®¡ç®—å›¾

### å…³é”®æ¦‚å¿µ

#### 1. requires_grad
- æ ‡è®°éœ€è¦è®¡ç®—æ¢¯åº¦çš„Tensor
- é»˜è®¤ä¸ºFalse
- é€šå¸¸æ˜¯æ¨¡å‹å‚æ•°

#### 2. backward()
- åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
- ä»losså¼€å§‹
- æ²¿è®¡ç®—å›¾ä¼ æ’­æ¢¯åº¦

#### 3. grad
- å­˜å‚¨æ¢¯åº¦å€¼
- åœ¨backward()åå¡«å……
- ç”¨äºå‚æ•°æ›´æ–°

### ç®€å•ç¤ºä¾‹

#### å•å˜é‡æ¢¯åº¦

```python
import torch

# åˆ›å»ºéœ€è¦æ¢¯åº¦çš„Tensor
x = torch.tensor(2.0, requires_grad=True)

# å®šä¹‰å‡½æ•°: y = x^2 + 3x + 1
y = x**2 + 3 * x + 1

# åå‘ä¼ æ’­
y.backward()

# æ¢¯åº¦: dy/dx = 2x + 3 = 2*2 + 3 = 7
print(x.grad)  # tensor(7.)
```

#### å¤šå˜é‡æ¢¯åº¦

```python
x1 = torch.tensor(2.0, requires_grad=True)
x2 = torch.tensor(3.0, requires_grad=True)

# y = x1^2 + x2^2
y = x1**2 + x2**2

y.backward()

print(f"âˆ‚y/âˆ‚x1 = {x1.grad}")  # 4.0
print(f"âˆ‚y/âˆ‚x2 = {x2.grad}")  # 6.0
```

### è®­ç»ƒå¾ªç¯ä¸­çš„æ¢¯åº¦

```python
# æ¨¡å‹å‚æ•°
w = torch.tensor([1.0], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)

# è¾“å…¥å’Œç›®æ ‡
x = torch.tensor([2.0])
y_true = torch.tensor([5.0])

# å‰å‘ä¼ æ’­
y_pred = w * x + b

# è®¡ç®—æŸå¤±
loss = (y_pred - y_true) ** 2

# åå‘ä¼ æ’­
loss.backward()

print(f"âˆ‚loss/âˆ‚w = {w.grad}")  # -4.0
print(f"âˆ‚loss/âˆ‚b = {b.grad}")  # -2.0

# å‚æ•°æ›´æ–°
learning_rate = 0.1
with torch.no_grad():
    w -= learning_rate * w.grad
    b -= learning_rate * b.grad

print(f"w = {w.data}")  # 1.4
print(f"b = {b.data}")  # 0.2

# é‡è¦: æ›´æ–°åæ¸…é›¶æ¢¯åº¦
w.grad.zero_()
b.grad.zero_()
```

### æ¢¯åº¦è®¡ç®—æ³¨æ„äº‹é¡¹

#### 1. æ¸…é›¶æ¢¯åº¦
```python
# æ¯æ¬¡backward()å‰éœ€è¦æ¸…é›¶æ¢¯åº¦
optimizer.zero_grad()
# æˆ–æ‰‹åŠ¨æ¸…é›¶
model.zero_grad()
```

#### 2. ç¦ç”¨æ¢¯åº¦è®¡ç®—
```python
# è¯„ä¼°æ—¶ç¦ç”¨æ¢¯åº¦
with torch.no_grad():
    predictions = model(X)

# æ¨ç†æ¨¡å¼
model.eval()
```

#### 3. æ¢¯åº¦è£å‰ª
```python
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

## nn.Moduleæ¨¡å‹å®šä¹‰

### nn.Moduleæ¦‚è¿°
- PyTorchä¸­æ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å‹çš„åŸºç±»
- æä¾›æ¨¡å‹ç®¡ç†å’Œè‡ªåŠ¨å¾®åˆ†åŠŸèƒ½
- å¿…é¡»å®ç°__init__å’Œforwardæ–¹æ³•

### æ¨¡å‹å®šä¹‰æ¨¡æ¿

```python
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        # å®šä¹‰å±‚

    def forward(self, x):
        # å‰å‘ä¼ æ’­
        return output
```

### çº¿æ€§æ¨¡å‹ç¤ºä¾‹

```python
class SimpleLinearModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        # å®šä¹‰å±‚
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()

    def forward(self, x):
        # å‰å‘ä¼ æ’­
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# åˆ›å»ºæ¨¡å‹
model = SimpleLinearModel(input_size=10, hidden_size=20, output_size=1)

# æŸ¥çœ‹æ¨¡å‹
print(model)

# å‚æ•°æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
print(f"æ€»å‚æ•°: {total_params}")
```

### æŸ¥çœ‹æ¨¡å‹å‚æ•°

```python
# éå†å‚æ•°
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")

# è®¿é—®ç‰¹å®šå±‚
fc1_weights = model.fc1.weight
fc1_bias = model.fc1.bias

print(fc1_weights.shape)
print(fc1_bias.shape)
```

### æ¨¡å‹æ–¹æ³•

```python
# è®­ç»ƒæ¨¡å¼
model.train()

# è¯„ä¼°æ¨¡å¼
model.eval()

# ç§»åŠ¨åˆ°GPU
model.to('cuda')

# ä¿å­˜æ¨¡å‹å‚æ•°
torch.save(model.state_dict(), 'model.pth')

# åŠ è½½æ¨¡å‹å‚æ•°
model.load_state_dict(torch.load('model.pth'))
```

---

## å¸¸ç”¨å±‚

### å…¨è¿æ¥å±‚ï¼ˆnn.Linearï¼‰

#### å…¬å¼
```
y = xW^T + b
```

#### ç¤ºä¾‹

```python
# è¾“å…¥ç»´åº¦10ï¼Œè¾“å‡ºç»´åº¦5
linear = nn.Linear(10, 5)

# è¾“å…¥ (batch_size=3, input_size=10)
x = torch.randn(3, 10)

# è¾“å‡º (batch_size=3, output_size=5)
y = linear(x)

print(y.shape)  # torch.Size([3, 5])
```

### LSTMå±‚ï¼ˆnn.LSTMï¼‰

#### å‚æ•°è¯´æ˜
- `input_size`: è¾“å…¥ç‰¹å¾ç»´åº¦
- `hidden_size`: éšè—çŠ¶æ€ç»´åº¦
- `num_layers`: LSTMå±‚æ•°
- `batch_first`: batchæ˜¯å¦åœ¨ç¬¬ä¸€ç»´
- `bidirectional`: æ˜¯å¦åŒå‘
- `dropout`: Dropoutæ¯”ä¾‹

#### è¾“å…¥æ ¼å¼
- `batch_first=False`: (seq_len, batch, input_size)
- `batch_first=True`: (batch, seq_len, input_size)

#### è¾“å‡ºæ ¼å¼
- `output`: (batch, seq_len, hidden_size)
- `h_n`: (num_layers, batch, hidden_size)
- `c_n`: (num_layers, batch, hidden_size)

#### ç¤ºä¾‹

```python
# åˆ›å»ºLSTM
lstm = nn.LSTM(
    input_size=10,
    hidden_size=20,
    num_layers=2,
    batch_first=True,
    dropout=0.2
)

# è¾“å…¥ (batch=5, seq_len=8, input_size=10)
x = torch.randn(5, 8, 10)

# å‰å‘ä¼ æ’­
output, (h_n, c_n) = lstm(x)

print(f"output: {output.shape}")  # (5, 8, 20)
print(f"h_n: {h_n.shape}")        # (2, 5, 20)
print(f"c_n: {c_n.shape}")        # (2, 5, 20)
```

### æ¿€æ´»å‡½æ•°

```python
import torch.nn as nn

x = torch.randn(5)

# ReLU
relu = nn.ReLU()
y_relu = relu(x)  # max(0, x)

# Sigmoid
sigmoid = nn.Sigmoid()
y_sigmoid = sigmoid(x)  # 1 / (1 + exp(-x))

# Tanh
tanh = nn.Tanh()
y_tanh = tanh(x)  # (exp(x) - exp(-x)) / (exp(x) + exp(-x))

# Leaky ReLU
leaky_relu = nn.LeakyReLU(0.01)
y_leaky = leaky_relu(x)  # max(0.01x, x)
```

### Dropout

#### ä½œç”¨
- é˜²æ­¢è¿‡æ‹Ÿåˆ
- éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒ

#### ç¤ºä¾‹

```python
dropout = nn.Dropout(p=0.5)  # 50%çš„ç¥ç»å…ƒè¢«ç½®é›¶

x = torch.ones(10)
y = dropout(x)

print(x)  # tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
print(y)  # tensor([2., 0., 2., 0., 2., 0., 0., 2., 0., 2.])  # çº¦50%ä¸º0
```

### BatchNorm

#### ä½œç”¨
- åŠ é€Ÿè®­ç»ƒ
- æé«˜ç¨³å®šæ€§

#### 1D BatchNorm

```python
batchnorm1d = nn.BatchNorm1d(num_features=10)

# è¾“å…¥ (batch=5, features=10)
x = torch.randn(5, 10)
y = batchnorm1d(x)
```

#### 2D BatchNormï¼ˆç”¨äºCNNï¼‰

```python
batchnorm2d = nn.BatchNorm2d(num_features=10)

# è¾“å…¥ (batch=5, channels=10, height=20, width=20)
x = torch.randn(5, 10, 20, 20)
y = batchnorm2d(x)
```

### Embeddingå±‚

#### ä½œç”¨
- å°†ç¦»æ•£å€¼æ˜ å°„ä¸ºç¨ å¯†å‘é‡
- ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†

#### ç¤ºä¾‹

```python
# è¯æ±‡è¡¨å¤§å°10000ï¼ŒåµŒå…¥ç»´åº¦100
embedding = nn.Embedding(num_embeddings=10000, embedding_dim=100)

# è¾“å…¥ (batch=5, seq_len=10)
x = torch.randint(0, 10000, (5, 10))

# è¾“å‡º (batch=5, seq_len=10, embedding_dim=100)
y = embedding(x)

print(y.shape)  # torch.Size([5, 10, 100])
```

---

## æ ¸å¿ƒçŸ¥è¯†ç‚¹æ€»ç»“

### PyTorchç®€ä»‹
- âœ… åŠ¨æ€è®¡ç®—å›¾
- âœ… GPUåŠ é€Ÿ
- âœ… è‡ªåŠ¨å¾®åˆ†
- âœ… ä¸°å¯Œçš„API

### TensoråŸºç¡€
- âœ… Tensoråˆ›å»º
- âœ… Tensoræ“ä½œ
- âœ… Tensor vs NumPy
- âœ… äº’æ“ä½œ

### Autograd
- âœ… requires_grad
- âœ… backward()
- âœ… æ¢¯åº¦è®¡ç®—
- âœ… æ¢¯åº¦æ¸…é›¶å’Œè£å‰ª

### nn.Module
- âœ… æ¨¡å‹å®šä¹‰æ¨¡æ¿
- âœ… forwardæ–¹æ³•
- âœ… æ¨¡å‹å‚æ•°ç®¡ç†
- âœ… è®­ç»ƒ/è¯„ä¼°æ¨¡å¼

### å¸¸ç”¨å±‚
- âœ… nn.Linear
- âœ… nn.LSTM
- âœ… æ¿€æ´»å‡½æ•°
- âœ… Dropout
- âœ… BatchNorm

---

## ä¸‹ä¸€æ­¥

ç»§ç»­å­¦ä¹ : [LSTMæ¨¡å‹æ„å»ºç³»åˆ—](../03_LSTMæ¨¡å‹æ„å»ºç³»åˆ—/README.md)
