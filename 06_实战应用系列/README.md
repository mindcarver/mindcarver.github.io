# å®æˆ˜åº”ç”¨ç³»åˆ— - æ¡ˆä¾‹ä¸æœ€ä½³å®è·µ

## ğŸ“š ç³»åˆ—æ¦‚è¿°
æœ¬ç³»åˆ—æ–‡æ¡£æ¶µç›–å®Œæ•´çš„LSTMé¢„æµ‹æµç¨‹ã€è¶…å‚æ•°è°ƒä¼˜ã€æ¨¡å‹ä¿å­˜ä¸åŠ è½½ã€è¯„ä¼°æŒ‡æ ‡å’Œæœ€ä½³å®è·µã€‚

---

## ğŸ“– æ–‡æ¡£åˆ—è¡¨

1. [å®Œæ•´é¢„æµ‹æµç¨‹](#å®Œæ•´é¢„æµ‹æµç¨‹)
2. [è¶…å‚æ•°è°ƒä¼˜](#è¶…å‚æ•°è°ƒä¼˜)
3. [æ¨¡å‹ä¿å­˜ä¸åŠ è½½](#æ¨¡å‹ä¿å­˜ä¸åŠ è½½)
4. [è¯„ä¼°æŒ‡æ ‡](#è¯„ä¼°æŒ‡æ ‡)
5. [LSTM vs LightGBM](#lstm-vs-lightgbm)
6. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## å®Œæ•´é¢„æµ‹æµç¨‹

### æ­¥éª¤1: æ•°æ®å‡†å¤‡

```python
import pandas as pd
import numpy as np

# åŠ è½½æ•°æ®
data = pd.read_csv('stock_prices.csv')

# ç‰¹å¾å·¥ç¨‹
features = ['close', 'volume', 'ma5', 'ma20', 'rsi']
data = data[features].values

# æ ‡å‡†åŒ–
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

# æ„é€ åºåˆ—
def create_sequences(data, seq_len, target_idx=0):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len, target_idx])
    return np.array(X), np.array(y)

seq_len = 20
X, y = create_sequences(data_scaled, seq_len)

# åˆ’åˆ†æ•°æ®
train_size = int(0.7 * len(X))
val_size = int(0.15 * len(X))

X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]
y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]

print(f"Train: {X_train.shape}")
print(f"Val: {X_val.shape}")
print(f"Test: {X_test.shape}")
```

### æ­¥éª¤2: æ¨¡å‹å®šä¹‰

```python
import torch
import torch.nn as nn

class StockPredictionLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_output = lstm_out[:, -1, :]
        last_output = self.dropout(last_output)
        output = self.fc(last_output)
        return output

# åˆ›å»ºæ¨¡å‹
model = StockPredictionLSTM(
    input_size=X_train.shape[2],
    hidden_size=64,
    num_layers=2,
    dropout=0.2
)
```

### æ­¥éª¤3: Datasetå’ŒDataLoader

```python
from torch.utils.data import Dataset, DataLoader

class StockDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# åˆ›å»ºDatasetå’ŒDataLoader
train_dataset = StockDataset(X_train, y_train)
val_dataset = StockDataset(X_val, y_val)
test_dataset = StockDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
```

### æ­¥éª¤4: è®­ç»ƒ

```python
import copy

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒ
num_epochs = 50
best_val_loss = float('inf')
best_model = None

for epoch in range(num_epochs):
    # è®­ç»ƒ
    model.train()
    train_loss = 0.0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        predictions = model(X_batch)
        loss = criterion(predictions.squeeze(), y_batch)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        train_loss += loss.item()

    train_loss /= len(train_loader)

    # éªŒè¯
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            predictions = model(X_batch)
            loss = criterion(predictions.squeeze(), y_batch)
            val_loss += loss.item()

    val_loss /= len(val_loader)

    print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_model = copy.deepcopy(model.state_dict())

# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(best_model)
```

### æ­¥éª¤5: è¯„ä¼°

```python
# æµ‹è¯•
model.eval()
predictions = []
actuals = []

with torch.no_grad():
    for X_batch, y_batch in test_loader:
        pred = model(X_batch)
        predictions.extend(pred.squeeze().numpy())
        actuals.extend(y_batch.numpy())

# åæ ‡å‡†åŒ–
predictions = scaler.inverse_transform(
    np.column_stack([predictions, np.zeros((len(predictions), X_train.shape[2]-1))])
)[:, 0]

actuals = scaler.inverse_transform(
    np.column_stack([actuals, np.zeros((len(actuals), X_train.shape[2]-1))])
)[:, 0]

# è®¡ç®—æŒ‡æ ‡
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

mse = mean_squared_error(actuals, predictions)
mae = mean_absolute_error(actuals, predictions)
r2 = r2_score(actuals, predictions)

print(f"MSE: {mse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"RÂ²: {r2:.4f}")
```

---

## è¶…å‚æ•°è°ƒä¼˜

### ç½‘æ ¼æœç´¢

```python
from itertools import product

# å®šä¹‰å‚æ•°ç½‘æ ¼
param_grid = {
    'hidden_size': [32, 64, 128],
    'num_layers': [1, 2, 3],
    'dropout': [0.1, 0.2, 0.3],
    'learning_rate': [0.0001, 0.001, 0.01],
    'batch_size': [16, 32, 64]
}

# ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
param_combinations = list(product(
    param_grid['hidden_size'],
    param_grid['num_layers'],
    param_grid['dropout'],
    param_grid['learning_rate'],
    param_grid['batch_size']
))

best_params = None
best_val_loss = float('inf')

for hidden_size, num_layers, dropout, lr, batch_size in param_combinations:
    print(f"Testing: hidden_size={hidden_size}, num_layers={num_layers}, dropout={dropout}, lr={lr}, batch_size={batch_size}")

    # åˆ›å»ºæ¨¡å‹
    model = StockPredictionLSTM(
        input_size=X_train.shape[2],
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=dropout
    )

    # åˆ›å»ºDataLoader
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # å®šä¹‰ä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # è®­ç»ƒï¼ˆå¿«é€ŸéªŒè¯ï¼‰
    for epoch in range(10):
        model.train()
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            predictions = model(X_batch)
            loss = criterion(predictions.squeeze(), y_batch)
            loss.backward()
            optimizer.step()

    # éªŒè¯
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            predictions = model(X_batch)
            loss = criterion(predictions.squeeze(), y_batch)
            val_loss += loss.item()

    val_loss /= len(val_loader)

    print(f"  Val Loss: {val_loss:.4f}")

    # æ›´æ–°æœ€ä½³å‚æ•°
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_params = {
            'hidden_size': hidden_size,
            'num_layers': num_layers,
            'dropout': dropout,
            'learning_rate': lr,
            'batch_size': batch_size
        }

print(f"\nBest parameters: {best_params}")
print(f"Best validation loss: {best_val_loss:.4f}")
```

### éšæœºæœç´¢

```python
import random

n_trials = 50

for trial in range(n_trials):
    # éšæœºé€‰æ‹©å‚æ•°
    hidden_size = random.choice([32, 64, 128, 256])
    num_layers = random.choice([1, 2, 3, 4])
    dropout = random.uniform(0.1, 0.5)
    learning_rate = random.choice([0.0001, 0.001, 0.01])
    batch_size = random.choice([16, 32, 64, 128])

    print(f"Trial {trial+1}/{n_trials}")

    # è®­ç»ƒå’ŒéªŒè¯
    # ...
```

---

## æ¨¡å‹ä¿å­˜ä¸åŠ è½½

### ä¿å­˜æ¨¡å‹

```python
# 1. ä¿å­˜æ•´ä¸ªæ¨¡å‹ï¼ˆåŒ…å«ç»“æ„å’Œå‚æ•°ï¼‰
torch.save(model, 'lstm_model.pth')

# 2. åªä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆæ¨èï¼‰
torch.save(model.state_dict(), 'lstm_model_state.pth')

# 3. ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
    'best_val_loss': best_val_loss
}
torch.save(checkpoint, 'checkpoint.pth')
```

### åŠ è½½æ¨¡å‹

```python
# 1. åŠ è½½æ•´ä¸ªæ¨¡å‹
model = torch.load('lstm_model.pth')
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# 2. åŠ è½½æ¨¡å‹å‚æ•°ï¼ˆéœ€è¦å…ˆå®šä¹‰æ¨¡å‹ï¼‰
model = StockPredictionLSTM(
    input_size=X_train.shape[2],
    hidden_size=64,
    num_layers=2,
    dropout=0.2
)
model.load_state_dict(torch.load('lstm_model_state.pth'))
model.eval()

# 3. åŠ è½½æ£€æŸ¥ç‚¹
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
best_val_loss = checkpoint['best_val_loss']
```

### æœ€ä½³å®è·µ

```python
# è®­ç»ƒæ—¶ä¿å­˜æœ€ä½³æ¨¡å‹
best_val_loss = float('inf')

for epoch in range(num_epochs):
    # è®­ç»ƒå’ŒéªŒè¯
    val_loss = validate(model, val_loader)

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pth')
        print(f"Saved best model with val_loss: {val_loss:.4f}")

    # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
    if (epoch + 1) % 10 == 0:
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_val_loss': best_val_loss
        }
        torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}.pth')
```

---

## è¯„ä¼°æŒ‡æ ‡

### å›å½’æŒ‡æ ‡

#### 1. MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰

```python
mse = torch.mean((predictions - targets) ** 2)
print(f"MSE: {mse.item():.4f}")
```

#### 2. MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰

```python
mae = torch.mean(torch.abs(predictions - targets))
print(f"MAE: {mae.item():.4f}")
```

#### 3. RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰

```python
rmse = torch.sqrt(torch.mean((predictions - targets) ** 2))
print(f"RMSE: {rmse.item():.4f}")
```

#### 4. RÂ²ï¼ˆå†³å®šç³»æ•°ï¼‰

```python
ss_res = torch.sum((targets - predictions) ** 2)
ss_tot = torch.sum((targets - torch.mean(targets)) ** 2)
r2 = 1 - ss_res / ss_tot
print(f"RÂ²: {r2.item():.4f}")
```

### é‡åŒ–æŠ•èµ„ä¸“ç”¨æŒ‡æ ‡

#### 1. ICï¼ˆä¿¡æ¯ç³»æ•°ï¼‰

```python
from scipy.stats import spearmanr

ic, _ = spearmanr(predictions.numpy(), targets.numpy())
print(f"IC: {ic:.4f}")
```

#### 2. ICIRï¼ˆä¿¡æ¯ç³»æ•°ä¿¡æ¯æ¯”ç‡ï¼‰

```python
# è®¡ç®—å¤šæœŸIC
ic_values = []
for i in range(n_periods):
    ic, _ = spearmanr(preds[i], targets[i])
    ic_values.append(ic)

icir = np.mean(ic_values) / np.std(ic_values)
print(f"ICIR: {icir:.4f}")
```

#### 3. MAPEï¼ˆå¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®ï¼‰

```python
mape = torch.mean(torch.abs((targets - predictions) / targets)) * 100
print(f"MAPE: {mape.item():.2f}%")
```

### å®Œæ•´è¯„ä¼°

```python
def evaluate_model(model, test_loader, scaler):
    """
    è¯„ä¼°æ¨¡å‹

    å‚æ•°:
        model: æ¨¡å‹
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        scaler: æ ‡å‡†åŒ–å™¨

    è¿”å›:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            pred = model(X_batch)
            predictions.extend(pred.squeeze().numpy())
            actuals.extend(y_batch.numpy())

    predictions = np.array(predictions)
    actuals = np.array(actuals)

    # åæ ‡å‡†åŒ–
    predictions_original = scaler.inverse_transform(
        np.column_stack([predictions, np.zeros((len(predictions), X_train.shape[2]-1))])
    )[:, 0]

    actuals_original = scaler.inverse_transform(
        np.column_stack([actuals, np.zeros((len(actuals), X_train.shape[2]-1))])
    )[:, 0]

    # è®¡ç®—æŒ‡æ ‡
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    from scipy.stats import spearmanr

    results = {
        'MSE': mean_squared_error(actuals_original, predictions_original),
        'MAE': mean_absolute_error(actuals_original, predictions_original),
        'RMSE': np.sqrt(mean_squared_error(actuals_original, predictions_original)),
        'RÂ²': r2_score(actuals_original, predictions_original),
        'IC': spearmanr(predictions_original, actuals_original)[0],
        'MAPE': np.mean(np.abs((actuals_original - predictions_original) / actuals_original)) * 100
    }

    return results, predictions_original, actuals_original

# è¯„ä¼°
results, preds, actuals = evaluate_model(model, test_loader, scaler)

for metric, value in results.items():
    print(f"{metric}: {value:.4f}")
```

---

## LSTM vs LightGBM

### å¯¹æ¯”ç»´åº¦

| ç»´åº¦ | LSTM | LightGBM |
|------|------|----------|
| **æ•°æ®éœ€æ±‚** | å¤§é‡ | ä¸­ç­‰ |
| **è®­ç»ƒæ—¶é—´** | é•¿ | çŸ­ |
| **ç‰¹å¾å·¥ç¨‹** | å°‘ | å¤š |
| **å¯è§£é‡Šæ€§** | ä½ | é«˜ |
| **é•¿æœŸä¾èµ–** | ä¼˜ç§€ | å·® |
| **è¿‡æ‹Ÿåˆé£é™©** | é«˜ | ä¸­ |
| **æ¨ç†é€Ÿåº¦** | ä¸­ | å¿« |
| **GPUæ”¯æŒ** | âœ… | âŒ |
| **å­¦ä¹ æ›²çº¿** | æ…¢ | å¿« |

### æ€§èƒ½å¯¹æ¯”å®éªŒ

```python
# LSTMæ¨¡å‹
lstm_model = StockPredictionLSTM(
    input_size=X_train.shape[2],
    hidden_size=64,
    num_layers=2,
    dropout=0.2
)

# è®­ç»ƒLSTM
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)

for epoch in range(50):
    lstm_model.train()
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        predictions = lstm_model(X_batch)
        loss = criterion(predictions.squeeze(), y_batch)
        loss.backward()
        optimizer.step()

# LightGBMæ¨¡å‹
import lightgbm as lgb

# å±•å¹³æ•°æ®
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

# è®­ç»ƒ
lgb_model = lgb.LGBMRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6
)
lgb_model.fit(X_train_flat, y_train)

# é¢„æµ‹
lstm_preds = lstm_model(torch.FloatTensor(X_test)).squeeze().numpy()
lgb_preds = lgb_model.predict(X_test_flat)

# å¯¹æ¯”
lstm_mse = np.mean((lstm_preds - y_test) ** 2)
lgb_mse = np.mean((lgb_preds - y_test) ** 2)

print(f"LSTM MSE: {lstm_mse:.4f}")
print(f"LightGBM MSE: {lgb_mse:.4f}")
```

### é€‰æ‹©å»ºè®®

**é€‰æ‹©LSTM**:
- æ•°æ®é‡å¤§
- é•¿æœŸä¾èµ–é‡è¦
- ç‰¹å¾å·¥ç¨‹å°‘
- éœ€è¦æ•æ‰å¤æ‚æ¨¡å¼

**é€‰æ‹©LightGBM**:
- æ•°æ®é‡ä¸­ç­‰
- éœ€è¦å¿«é€Ÿè¿­ä»£
- å¯è§£é‡Šæ€§é‡è¦
- è®¡ç®—èµ„æºæœ‰é™

---

## æœ€ä½³å®è·µ

### æ•°æ®å‡†å¤‡

#### 1. æ•°æ®è´¨é‡æ£€æŸ¥
```python
# æ£€æŸ¥ç¼ºå¤±å€¼
print(data.isnull().sum())

# æ£€æŸ¥å¼‚å¸¸å€¼
print(data.describe())

# å¤„ç†ç¼ºå¤±å€¼
data = data.fillna(method='ffill')
```

#### 2. ç‰¹å¾é€‰æ‹©
```python
# é€‰æ‹©ä¸ç›®æ ‡ç›¸å…³çš„ç‰¹å¾
from sklearn.feature_selection import SelectKBest, f_regression

selector = SelectKBest(f_regression, k=10)
X_selected = selector.fit_transform(X, y)
```

#### 3. æ—¶é—´åºåˆ—åˆ’åˆ†
```python
# ä¸¥æ ¼æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†
# é¿å…æœªæ¥å‡½æ•°
# ä¿ç•™æ ·æœ¬å¤–æ•°æ®
```

### æ¨¡å‹è®¾è®¡

#### 1. æ¨¡å‹å¤æ‚åº¦
```python
# ä»ç®€å•æ¨¡å‹å¼€å§‹
# é€æ­¥å¢åŠ å¤æ‚åº¦
# é¿å…è¿‡åº¦å¤æ‚
```

#### 2. è¶…å‚æ•°é€‰æ‹©
```python
# hidden_size: 32-128
# num_layers: 1-3
# dropout: 0.1-0.3
# learning_rate: 0.0001-0.01
```

#### 3. æ­£åˆ™åŒ–
```python
# ä½¿ç”¨Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
# ä½¿ç”¨BatchNormåŠ é€Ÿè®­ç»ƒ
# ä½¿ç”¨L1/L2æ­£åˆ™åŒ–
```

### è®­ç»ƒç­–ç•¥

#### 1. æ—©åœ
```python
# ç›‘æ§éªŒè¯é›†æŸå¤±
# é˜²æ­¢è¿‡æ‹Ÿåˆ
# èŠ‚çœè®­ç»ƒæ—¶é—´
```

#### 2. å­¦ä¹ ç‡è°ƒåº¦
```python
# åˆå§‹å­¦ä¹ ç‡è¾ƒå¤§
# é€æ¸é™ä½å­¦ä¹ ç‡
# ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡
```

#### 3. æ¢¯åº¦è£å‰ª
```python
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
# ç¨³å®šè®­ç»ƒè¿‡ç¨‹
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### è¯„ä¼°ä¸éªŒè¯

#### 1. å¤šæŒ‡æ ‡è¯„ä¼°
```python
# ä¸è¦åªçœ‹ä¸€ä¸ªæŒ‡æ ‡
# ç»¼åˆè€ƒè™‘å¤šä¸ªæŒ‡æ ‡
# å…³æ³¨é£é™©è°ƒæ•´æ”¶ç›Š
```

#### 2. æ ·æœ¬å¤–éªŒè¯
```python
# ä¿ç•™ä¸€éƒ¨åˆ†æ•°æ®ä¸å‚ä¸è®­ç»ƒ
# ä¸¥æ ¼éªŒè¯æ³›åŒ–èƒ½åŠ›
# é¿å…è¿‡æ‹Ÿåˆ
```

#### 3. ç¨³å®šæ€§æµ‹è¯•
```python
# åœ¨ä¸åŒæ—¶é—´æ®µæµ‹è¯•
# æ£€æŸ¥å‚æ•°ç¨³å®šæ€§
# éªŒè¯é²æ£’æ€§
```

### å¸¸è§é—®é¢˜

#### Q1: LSTMè¿‡æ‹Ÿåˆæ€ä¹ˆåŠï¼Ÿ

**A**: å¤šç§æ–¹æ³•ç»“åˆ:
- å¢åŠ Dropoutæ¯”ä¾‹
- å‡å°‘æ¨¡å‹å¤æ‚åº¦
- å¢åŠ è®­ç»ƒæ•°æ®
- ä½¿ç”¨æ›´å¼ºçš„æ­£åˆ™åŒ–

#### Q2: LSTMè®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: ä¼˜åŒ–è®­ç»ƒé€Ÿåº¦:
- ä½¿ç”¨GPU
- å‡å°batch_size
- å‡å°‘æ¨¡å‹å¤æ‚åº¦
- ä½¿ç”¨æ›´å¿«çš„ä¼˜åŒ–å™¨

#### Q3: LSTM vs LightGBMå¦‚ä½•é€‰æ‹©ï¼Ÿ

**A**: æ ¹æ®å®é™…æƒ…å†µ:
- æ•°æ®é‡å¤§ã€é•¿æœŸä¾èµ–é‡è¦ï¼šLSTM
- éœ€è¦å¿«é€Ÿè¿­ä»£ã€å¯è§£é‡Šæ€§é‡è¦ï¼šLightGBM
- ä¸¤è€…éƒ½è¯•ï¼Œé€‰æ‹©æ›´å¥½çš„

#### Q4: å¦‚ä½•ç¡®å®šåºåˆ—é•¿åº¦ï¼Ÿ

**A**: é€šè¿‡å®éªŒç¡®å®š:
- å°è¯•ä¸åŒçš„åºåˆ—é•¿åº¦ï¼ˆ10, 20, 30, 60ï¼‰
- ä½¿ç”¨éªŒè¯é›†é€‰æ‹©æœ€ä¼˜é•¿åº¦
- è€ƒè™‘ä¸šåŠ¡é€»è¾‘ï¼ˆå¦‚ä¸€å‘¨ã€ä¸€æœˆï¼‰

---

## æ ¸å¿ƒçŸ¥è¯†ç‚¹æ€»ç»“

### å®Œæ•´é¢„æµ‹æµç¨‹
- âœ… æ•°æ®å‡†å¤‡
- âœ… æ¨¡å‹å®šä¹‰
- âœ… Datasetå’ŒDataLoader
- âœ… è®­ç»ƒ
- âœ… è¯„ä¼°

### è¶…å‚æ•°è°ƒä¼˜
- âœ… ç½‘æ ¼æœç´¢
- âœ… éšæœºæœç´¢
- âœ… è´å¶æ–¯ä¼˜åŒ–

### æ¨¡å‹ä¿å­˜ä¸åŠ è½½
- âœ… ä¿å­˜æ¨¡å‹
- âœ… åŠ è½½æ¨¡å‹
- âœ… æœ€ä½³å®è·µ

### è¯„ä¼°æŒ‡æ ‡
- âœ… å›å½’æŒ‡æ ‡
- âœ… é‡åŒ–æŠ•èµ„æŒ‡æ ‡
- âœ… å®Œæ•´è¯„ä¼°

### LSTM vs LightGBM
- âœ… å¯¹æ¯”ç»´åº¦
- âœ… æ€§èƒ½å¯¹æ¯”
- âœ… é€‰æ‹©å»ºè®®

### æœ€ä½³å®è·µ
- âœ… æ•°æ®å‡†å¤‡
- âœ… æ¨¡å‹è®¾è®¡
- âœ… è®­ç»ƒç­–ç•¥
- âœ… è¯„ä¼°éªŒè¯

---

## é™„å½•

### ä¸“ä¸šæœ¯è¯­è¡¨

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|------|------|------|
| RNN | Recurrent Neural Network | å¾ªç¯ç¥ç»ç½‘ç»œ |
| LSTM | Long Short-Term Memory | é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ |
| GRU | Gated Recurrent Unit | é—¨æ§å¾ªç¯å•å…ƒ |
| IC | Information Coefficient | ä¿¡æ¯ç³»æ•° |
| ICIR | IC Information Ratio | ä¿¡æ¯ç³»æ•°ä¿¡æ¯æ¯”ç‡ |
| Early Stopping | æ—©åœ | é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ |
| Dropout | ä¸¢å¼ƒ | éšæœºä¸¢å¼ƒç¥ç»å…ƒ |

### æ¨èå­¦ä¹ èµ„æº

**ä¹¦ç±**:
- ã€Šæ·±åº¦å­¦ä¹ ã€‹ï¼ˆGoodfellowç­‰ï¼‰
- ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹
- ã€ŠPythonæ·±åº¦å­¦ä¹ ã€‹

**åœ¨çº¿è¯¾ç¨‹**:
- Coursera: Deep Learning Specialization
- Fast.ai: Practical Deep Learning for Coders

**æ–‡æ¡£å’Œæ•™ç¨‹**:
- PyTorchå®˜æ–¹æ–‡æ¡£ï¼šhttps://pytorch.org/docs/
- PyTorchæ•™ç¨‹ï¼šhttps://pytorch.org/tutorials/

---

**ç³»åˆ—æ–‡æ¡£ç»“æŸ**

ç¥å­¦ä¹ é¡ºåˆ©ï¼ğŸ“
