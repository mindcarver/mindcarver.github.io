# æ—¶åºæ•°æ®å¤„ç†ç³»åˆ— - é¢„å¤„ç†ä¸Dataset

## ğŸ“š ç³»åˆ—æ¦‚è¿°
æœ¬ç³»åˆ—æ–‡æ¡£æ¶µç›–æ—¶åºæ•°æ®çš„æ„é€ ã€æ ‡å‡†åŒ–ã€æ•°æ®åˆ’åˆ†ã€PyTorch Datasetå’ŒDataLoaderã€‚

---

## ğŸ“– æ–‡æ¡£åˆ—è¡¨

1. [æ»‘åŠ¨çª—å£æ–¹æ³•](#æ»‘åŠ¨çª—å£æ–¹æ³•)
2. [æ•°æ®åˆ’åˆ†](#æ•°æ®åˆ’åˆ†)
3. [ç‰¹å¾æ ‡å‡†åŒ–](#ç‰¹å¾æ ‡å‡†åŒ–)
4. [PyTorch Dataset](#pytorch-dataset)
5. [DataLoader](#dataloader)

---

## æ»‘åŠ¨çª—å£æ–¹æ³•

### åŸç†
- ä½¿ç”¨è¿‡å»Nå¤©çš„æ•°æ®é¢„æµ‹ä¸‹ä¸€å¤©
- çª—å£æ»‘åŠ¨ï¼Œç”Ÿæˆå¤šä¸ªæ ·æœ¬
- æ˜¯æ„é€ æ—¶åºè®­ç»ƒæ•°æ®çš„æ ‡å‡†æ–¹æ³•

### ç¤ºä¾‹

#### åŸå§‹æ•°æ®
```
Day 1: [0.1, 0.2, 0.3]
Day 2: [0.2, 0.3, 0.4]
Day 3: [0.3, 0.4, 0.5]
Day 4: [0.4, 0.5, 0.6]
Day 5: [0.5, 0.6, 0.7]
```

#### åºåˆ—é•¿åº¦ = 3

| æ ·æœ¬ | è¾“å…¥ (X) | ç›®æ ‡ (y) |
|------|---------|---------|
| 1 | Day 1-3 | Day 4 |
| 2 | Day 2-4 | Day 5 |

### Pythonå®ç°

```python
import numpy as np

def create_sequences(data, seq_len, target_idx=0):
    """
    æ»‘åŠ¨çª—å£æ„é€ æ—¶åºåºåˆ—

    å‚æ•°:
        data: åŸå§‹æ•°æ® (n_samples, n_features)
        seq_len: åºåˆ—é•¿åº¦
        target_idx: ç›®æ ‡ç‰¹å¾ç´¢å¼•

    è¿”å›:
        X: è¾“å…¥åºåˆ— (n_samples-seq_len, seq_len, n_features)
        y: ç›®æ ‡å€¼ (n_samples-seq_len,)
    """
    X, y = [], []

    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len, target_idx])

    return np.array(X), np.array(y)

# ç¤ºä¾‹
data = np.random.randn(100, 10)  # 100å¤©ï¼Œ10ä¸ªç‰¹å¾
X, y = create_sequences(data, seq_len=20, target_idx=0)

print(X.shape)  # (80, 20, 10)
print(y.shape)  # (80,)
```

### é«˜çº§æ»‘åŠ¨çª—å£

```python
def create_sequences_multi_step(data, seq_len, target_len, target_idx=0):
    """
    å¤šæ­¥é¢„æµ‹æ»‘åŠ¨çª—å£

    å‚æ•°:
        data: åŸå§‹æ•°æ®
        seq_len: è¾“å…¥åºåˆ—é•¿åº¦
        target_len: é¢„æµ‹æ­¥æ•°
        target_idx: ç›®æ ‡ç‰¹å¾ç´¢å¼•

    è¿”å›:
        X: (n_samples-seq_len-target_len, seq_len, n_features)
        y: (n_samples-seq_len-target_len, target_len)
    """
    X, y = [], []

    for i in range(len(data) - seq_len - target_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len:i+seq_len+target_len, target_idx])

    return np.array(X), np.array(y)

# ç¤ºä¾‹ï¼šé¢„æµ‹æœªæ¥5å¤©
X, y = create_sequences_multi_step(data, seq_len=20, target_len=5)
print(X.shape)  # (75, 20, 10)
print(y.shape)  # (75, 5)
```

### æ»‘åŠ¨çª—å£é€‰æ‹©å»ºè®®

| é¢„æµ‹ç›®æ ‡ | seq_len | è¯´æ˜ |
|---------|---------|------|
| **çŸ­æœŸé¢„æµ‹** | 5-10 | æ—¥å†…äº¤æ˜“ |
| **ä¸­æœŸé¢„æµ‹** | 20-60 | å‡ å¤©åˆ°å‡ å‘¨ |
| **é•¿æœŸé¢„æµ‹** | 60-120 | å‡ ä¸ªæœˆ |

---

## æ•°æ®åˆ’åˆ†

### æ—¶é—´åºåˆ—åˆ’åˆ†åŸåˆ™
- æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†
- ä¸èƒ½éšæœºåˆ’åˆ†
- è®­ç»ƒé›† < éªŒè¯é›† < æµ‹è¯•é›†

### åˆ’åˆ†æ–¹æ³•

```python
def time_series_split(data, train_ratio=0.7, val_ratio=0.15):
    """
    æ—¶é—´åºåˆ—æ•°æ®åˆ’åˆ†

    å‚æ•°:
        data: å®Œæ•´æ•°æ®
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹

    è¿”å›:
        train, val, test
    """
    n = len(data)
    train_end = int(n * train_ratio)
    val_end = int(n * (train_ratio + val_ratio))

    train = data[:train_end]
    val = data[train_end:val_end]
    test = data[val_end:]

    return train, val, test

# ç¤ºä¾‹
data = np.random.randn(1000, 10)
train, val, test = time_series_split(data)

print(f"Train: {train.shape}")  # (700, 10)
print(f"Val: {val.shape}")      # (150, 10)
print(f"Test: {test.shape}")     # (150, 10)
```

### æ»šåŠ¨çª—å£éªŒè¯

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

for fold, (train_index, test_index) in enumerate(tscv.split(X)):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    print(f"Fold {fold+1}:")
    print(f"  Train: {X_train.shape}")
    print(f"  Test: {X_test.shape}")
```

### Walk-ForwardéªŒè¯

```python
def walk_forward_validation(data, initial_train_size, step_size):
    """
    Walk-ForwardéªŒè¯

    å‚æ•°:
        data: å®Œæ•´æ•°æ®
        initial_train_size: åˆå§‹è®­ç»ƒé›†å¤§å°
        step_size: æ¯æ¬¡å‰è¿›æ­¥æ•°
    """
    splits = []
    n = len(data)

    train_end = initial_train_size

    while train_end + step_size < n:
        test_start = train_end
        test_end = min(test_start + step_size, n)

        splits.append((
            data[:train_end],
            data[test_start:test_end]
        ))

        train_end += step_size

    return splits

# ç¤ºä¾‹
splits = walk_forward_validation(data, initial_train_size=500, step_size=100)

for i, (train, test) in enumerate(splits):
    print(f"Fold {i+1}: Train {train.shape}, Test {test.shape}")
```

---

## ç‰¹å¾æ ‡å‡†åŒ–

### æ ‡å‡†åŒ–æ–¹æ³•

#### 1. Z-scoreæ ‡å‡†åŒ–ï¼ˆStandardScalerï¼‰

```python
from sklearn.preprocessing import StandardScaler

# æ‹Ÿåˆè®­ç»ƒé›†
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))

# è½¬æ¢éªŒè¯é›†å’Œæµ‹è¯•é›†
X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1]))
X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1]))

# æ¢å¤å½¢çŠ¶
X_train_scaled = X_train_scaled.reshape(X_train.shape)
X_val_scaled = X_val_scaled.reshape(X_val.shape)
X_test_scaled = X_test_scaled.reshape(X_test.shape)
```

#### 2. Min-Maxæ ‡å‡†åŒ–

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))

X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1]))
X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1]))

# æ¢å¤å½¢çŠ¶
X_train_scaled = X_train_scaled.reshape(X_train.shape)
X_val_scaled = X_val_scaled.reshape(X_val.shape)
X_test_scaled = X_test_scaled.reshape(X_test.shape)
```

#### 3. RobustScaler

```python
from sklearn.preprocessing import RobustScaler

# å¯¹å¼‚å¸¸å€¼æ›´é²æ£’
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))

X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1]))
X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1]))

# æ¢å¤å½¢çŠ¶
X_train_scaled = X_train_scaled.reshape(X_train.shape)
X_val_scaled = X_val_scaled.reshape(X_val.shape)
X_test_scaled = X_test_scaled.reshape(X_test.shape)
```

### æ»šåŠ¨æ ‡å‡†åŒ–

```python
def rolling_standardize(data, window=20):
    """
    æ»šåŠ¨çª—å£æ ‡å‡†åŒ–

    å‚æ•°:
        data: åŸå§‹æ•°æ® (n_samples, n_features)
        window: æ»šåŠ¨çª—å£å¤§å°

    è¿”å›:
        scaled: æ ‡å‡†åŒ–åçš„æ•°æ®
    """
    scaled = np.zeros_like(data)

    for i in range(len(data)):
        if i < window:
            # åˆæœŸä½¿ç”¨ç´¯ç§¯æ•°æ®
            mean = data[:i+1].mean(axis=0)
            std = data[:i+1].std(axis=0)
        else:
            # ä½¿ç”¨æ»šåŠ¨çª—å£
            mean = data[i-window:i].mean(axis=0)
            std = data[i-window:i].std(axis=0)

        scaled[i] = (data[i] - mean) / (std + 1e-8)

    return scaled

# ç¤ºä¾‹
X_train_scaled = rolling_standardize(X_train, window=20)
```

### æ ‡å‡†åŒ–æ³¨æ„äº‹é¡¹

#### 1. åªç”¨è®­ç»ƒé›†æ‹Ÿåˆ
```python
# âŒ é”™è¯¯ï¼šç”¨æ‰€æœ‰æ•°æ®æ‹Ÿåˆ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(all_data)

# âœ… æ­£ç¡®ï¼šåªç”¨è®­ç»ƒé›†æ‹Ÿåˆ
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
```

#### 2. åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœ
```python
# é¢„æµ‹
predictions = model(X_test_scaled)

# åæ ‡å‡†åŒ–
predictions_original = scaler.inverse_transform(predictions)
```

---

## PyTorch Dataset

### è‡ªå®šä¹‰Dataset

```python
from torch.utils.data import Dataset
import torch

class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        """
        å‚æ•°:
            X: è¾“å…¥æ•°æ® (n_samples, seq_len, n_features)
            y: ç›®æ ‡å€¼ (n_samples,)
        """
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)

    def __len__(self):
        """è¿”å›æ ·æœ¬æ•°é‡"""
        return len(self.X)

    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬

        è¿”å›:
            X: (seq_len, n_features)
            y: scalar
        """
        return self.X[idx], self.y[idx]
```

### åˆ›å»ºDataset

```python
# åˆ›å»ºDataset
train_dataset = TimeSeriesDataset(X_train, y_train)
val_dataset = TimeSeriesDataset(X_val, y_val)
test_dataset = TimeSeriesDataset(X_test, y_test)

# æŸ¥çœ‹å¤§å°
print(f"Train dataset size: {len(train_dataset)}")
print(f"Val dataset size: {len(val_dataset)}")
print(f"Test dataset size: {len(test_dataset)}")

# è·å–å•ä¸ªæ ·æœ¬
X_sample, y_sample = train_dataset[0]
print(f"Sample X shape: {X_sample.shape}")
print(f"Sample y: {y_sample}")
```

### é«˜çº§Dataset

```python
class AdvancedTimeSeriesDataset(Dataset):
    def __init__(self, X, y, transform=None):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
        self.transform = transform

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        X, y = self.X[idx], self.y[idx]

        # åº”ç”¨å˜æ¢
        if self.transform:
            X = self.transform(X)

        return X, y

# å®šä¹‰å˜æ¢
def add_noise(x, noise_level=0.01):
    """æ·»åŠ å™ªå£°"""
    noise = torch.randn_like(x) * noise_level
    return x + noise

# åˆ›å»ºDataset
train_dataset = AdvancedTimeSeriesDataset(
    X_train,
    y_train,
    transform=lambda x: add_noise(x, 0.01)
)
```

---

## DataLoader

### åˆ›å»ºDataLoader

```python
from torch.utils.data import DataLoader

# åˆ›å»ºDataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,  # è®­ç»ƒé›†æ‰“ä¹±
    num_workers=4,
    pin_memory=True  # åŠ é€ŸGPUä¼ è¾“
)

val_loader = DataLoader(
    val_dataset,
    batch_size=32,
    shuffle=False,  # éªŒè¯é›†ä¸æ‰“ä¹±
    num_workers=4,
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=32,
    shuffle=False,
    num_workers=4,
    pin_memory=True
)
```

### ä½¿ç”¨DataLoader

```python
# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    model.train()

    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):
        # å‰å‘ä¼ æ’­
        predictions = model(X_batch)
        loss = criterion(predictions.squeeze(), y_batch)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # æ‰“å°è¿›åº¦
        if batch_idx % 100 == 0:
            print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}")

    # éªŒè¯
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            predictions = model(X_batch)
            loss = criterion(predictions.squeeze(), y_batch)
            val_loss += loss.item()

    val_loss /= len(val_loader)
    print(f"Epoch {epoch}, Val Loss: {val_loss:.4f}")
```

### DataLoaderå‚æ•°

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| **batch_size** | æ‰¹å¤§å° | 32, 64, 128 |
| **shuffle** | æ˜¯å¦æ‰“ä¹± | è®­ç»ƒé›†=Trueï¼ŒéªŒè¯/æµ‹è¯•=False |
| **num_workers** | åŠ è½½è¿›ç¨‹æ•° | 4-8 |
| **pin_memory** | é”é¡µå†…å­˜ | Trueï¼ˆGPUè®­ç»ƒï¼‰ |
| **drop_last** | ä¸¢å¼ƒä¸å®Œæ•´batch | False |

### åŠ¨æ€æ‰¹å¤§å°

```python
# æ ¹æ®åºåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´
class DynamicBatchSampler:
    def __init__(self, dataset, max_batch_size=32):
        self.dataset = dataset
        self.max_batch_size = max_batch_size

    def __iter__(self):
        batch = []
        for idx in range(len(self.dataset)):
            batch.append(idx)
            if len(batch) >= self.max_batch_size:
                yield batch
                batch = []

        if batch:
            yield batch

# ä½¿ç”¨
train_loader = DataLoader(
    train_dataset,
    batch_sampler=DynamicBatchSampler(train_dataset, max_batch_size=32),
    collate_fn=lambda batch: default_collate([train_dataset[i] for i in batch])
)
```

---

## æ•°æ®å¤„ç†æµç¨‹

### å®Œæ•´æµç¨‹

```python
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler

# 1. åŠ è½½æ•°æ®
data = np.load('stock_data.npy')  # (n_days, n_features)

# 2. æ„é€ åºåˆ—
def create_sequences(data, seq_len, target_idx=0):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len, target_idx])
    return np.array(X), np.array(y)

X, y = create_sequences(data, seq_len=20)

# 3. åˆ’åˆ†æ•°æ®
train_size = int(0.7 * len(X))
val_size = int(0.15 * len(X))

X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]
y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]

# 4. æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

# 5. åˆ›å»ºDataset
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_dataset = TimeSeriesDataset(X_train_scaled, y_train)
val_dataset = TimeSeriesDataset(X_val_scaled, y_val)
test_dataset = TimeSeriesDataset(X_test_scaled, y_test)

# 6. åˆ›å»ºDataLoader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

print(f"Train: {len(train_dataset)} samples")
print(f"Val: {len(val_dataset)} samples")
print(f"Test: {len(test_dataset)} samples")
```

---

## æ ¸å¿ƒçŸ¥è¯†ç‚¹æ€»ç»“

### æ»‘åŠ¨çª—å£
- âœ… åŸºæœ¬åŸç†
- âœ… å•æ­¥é¢„æµ‹
- âœ… å¤šæ­¥é¢„æµ‹
- âœ… åºåˆ—é•¿åº¦é€‰æ‹©

### æ•°æ®åˆ’åˆ†
- âœ… æ—¶é—´åºåˆ—åˆ’åˆ†åŸåˆ™
- âœ… æ»šåŠ¨çª—å£éªŒè¯
- âœ… Walk-ForwardéªŒè¯

### ç‰¹å¾æ ‡å‡†åŒ–
- âœ… Z-scoreæ ‡å‡†åŒ–
- âœ… Min-Maxæ ‡å‡†åŒ–
- âœ… RobustScaler
- âœ… æ»šåŠ¨æ ‡å‡†åŒ–

### Dataset
- âœ… è‡ªå®šä¹‰Dataset
- âœ… é«˜çº§Dataset
- âœ… æ•°æ®å˜æ¢

### DataLoader
- âœ… åˆ›å»ºDataLoader
- âœ… å‚æ•°é…ç½®
- âœ… ä½¿ç”¨ç¤ºä¾‹

---

## ä¸‹ä¸€æ­¥

ç»§ç»­å­¦ä¹ : [æ¨¡å‹è®­ç»ƒä¼˜åŒ–ç³»åˆ—](../05_æ¨¡å‹è®­ç»ƒä¼˜åŒ–ç³»åˆ—/README.md)
